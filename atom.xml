<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://philosopherzb.github.io</id>
    <title>Philosopher</title>
    <updated>2023-03-17T06:36:28.763Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://philosopherzb.github.io"/>
    <link rel="self" href="https://philosopherzb.github.io/atom.xml"/>
    <subtitle>WORLD AS CODE</subtitle>
    <logo>https://philosopherzb.github.io/images/avatar.png</logo>
    <icon>https://philosopherzb.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, Philosopher</rights>
    <entry>
        <title type="html"><![CDATA[i18n-sdk 使用手册]]></title>
        <id>https://philosopherzb.github.io/post/i18n-sdk-shi-yong-shou-ce/</id>
        <link href="https://philosopherzb.github.io/post/i18n-sdk-shi-yong-shou-ce/">
        </link>
        <updated>2022-08-30T06:22:19.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述i18n-sdk 的使用方式。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/boats-1802340_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="源码地址">源码地址</h2>
<p><a href="https://github.com/philosopherZB/demo-all/tree/master/i18n">点击此处跳转github查看具体源码</a></p>
<h2 id="引入依赖">引入依赖</h2>
<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;com.philosopherzb&lt;/groupId&gt;
        &lt;artifactId&gt;i18n-sdk&lt;/artifactId&gt;
         &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<h2 id="使用-可参考i18n-client工程">使用--&gt;(可参考i18n-client工程)</h2>
<h3 id="配置文件在本地">配置文件在本地</h3>
<ul>
<li>在resource目录下创建 messages_en_US.properties, messages_zb_CN.properties</li>
<li>在配置文件中输入中英文配置信息</li>
<li>项目的application.properties配置文件中需要加入:<br>
<code>spring.main.allow-bean-definition-overriding=true</code><br>
<em>其中i18n为resources下的文件目录，messages是第一步中的properties前缀</em><br>
<code>spring.messages.basename=classpath:i18n/messages</code></li>
<li>代码中使用: MessageUtil.get(&quot;key&quot;); MessageUtil.get(&quot;key&quot;, &quot;params&quot;);</li>
<li>测试使用，通过在请求头信息中添加如下信息:<br>
<code>[{&quot;key&quot;:&quot;Accept-Language&quot;,&quot;value&quot;:&quot;en_US&quot;,&quot;description&quot;:&quot;英文&quot;,&quot;type&quot;:&quot;text&quot;,&quot;enabled&quot;:true}]</code><br>
<code>[{&quot;key&quot;:&quot;Accept-Language&quot;,&quot;value&quot;:&quot;zh_CN&quot;,&quot;description&quot;:&quot;中文&quot;,&quot;type&quot;:&quot;text&quot;,&quot;enabled&quot;:true}]</code></li>
</ul>
<h3 id="配置文件在nacos">配置文件在nacos</h3>
<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt;
        &lt;artifactId&gt;nacos-client&lt;/artifactId&gt;
    &lt;/dependency&gt;
</code></pre>
<ul>
<li>如上引入nacos依赖</li>
<li>项目的application.properties配置文件中需要加入:<br>
<code>spring.main.allow-bean-definition-overriding=true</code><br>
<em>mode 表示i18n模式，local为本地，nacos为nacos配置中心，默认为local</em><br>
<code>spring.messages.mode=nacos</code><br>
<em>存储i18n的文件目录，默认为 i18n/</em><br>
<code>spring.messages.baseFolder=i18n/</code><br>
<em>此值与nacos中的配置信息需保持一致，且为必填。若为空则会抛出spring.messages.basename value is empty的异常信息。</em><br>
<code>spring.messages.basename=user-message</code><br>
<em>nacos的地址，为必填。若为空则会抛出spring.messages.serverAddress value is empty的异常信息。</em><br>
<code>spring.messages.serverAddress=127.0.0.1:8848</code><br>
<em>nacos的namespace值，为必填。若为空则会抛出spring.messages.namespace value is empty的异常信息。</em><br>
<code>spring.messages.namespace=e52f0660-c859-4762-b0f4-4f6f17727d59</code></li>
<li>代码中使用: MessageUtil.get(&quot;key&quot;); MessageUtil.get(&quot;key&quot;, &quot;params&quot;);</li>
<li>测试使用，通过在请求头信息中添加如下信息:<br>
<code>[{&quot;key&quot;:&quot;Accept-Language&quot;,&quot;value&quot;:&quot;en_US&quot;,&quot;description&quot;:&quot;英文&quot;,&quot;type&quot;:&quot;text&quot;,&quot;enabled&quot;:true}]</code><br>
<code>[{&quot;key&quot;:&quot;Accept-Language&quot;,&quot;value&quot;:&quot;zh_CN&quot;,&quot;description&quot;:&quot;中文&quot;,&quot;type&quot;:&quot;text&quot;,&quot;enabled&quot;:true}]</code></li>
</ul>
<h3 id="自动捕获javaxvalidationconstraints中的message作为国际化输出-可参考i18n-client工程">自动捕获javax.validation.constraints中的message作为国际化输出--&gt;(可参考i18n-client工程)</h3>
<h4 id="全局异常捕获">全局异常捕获</h4>
<ul>
<li>可查看 i18n-client 工程中的 com.philosopherzb.i18n.client.advice.handler.GlobalExceptionHandler</li>
</ul>
<pre><code>/**
 * @author philosopherZB
 */
@Slf4j
@RestControllerAdvice
public class GlobalExceptionHandler {

    private Function&lt;Exception, String&gt; messageExtractor = this::extractReadableMessage;

    @ExceptionHandler(RuntimeException.class)
    public Result&lt;String&gt; handleSystemException(Exception e) {
        log.error(&quot;service occurs runtime error: &quot;, e);
        return ResultUtils.failResult(BizErrorCode.SYSTEM_BUSY);
    }

    @ExceptionHandler({MethodArgumentNotValidException.class, BindException.class})
    public Result&lt;String&gt; handleArgumentException(Exception e) {
        String msg = this.messageExtractor.apply(e);
        return ResultUtils.failResult(BizErrorCode.PARAM_VALID_EXCEPTION.getCode(), msg);
    }

    @ExceptionHandler({ConstraintViolationException.class})
    public Result&lt;String&gt; handleException(ConstraintViolationException e) {
        log.error(&quot;param exception: &quot;, e);
        Optional&lt;ConstraintViolation&lt;?&gt;&gt; optional = e.getConstraintViolations().stream().findFirst();
        String msg = optional.isPresent() ? optional.get().getMessage() : BizErrorCode.PARAM_VALID_EXCEPTION.getErrorMessage();
        return ResultUtils.failResult(BizErrorCode.PARAM_VALID_EXCEPTION.getCode(), msg);
    }

    private String extractReadableMessage(Exception e) {
        log.error(&quot;param exception: &quot;, e);
        BindingResult bindingResult;
        if (e instanceof BindException) {
            bindingResult = ((BindException) e).getBindingResult();
        } else {
            bindingResult = ((MethodArgumentNotValidException) e).getBindingResult();
        }

        return Objects.requireNonNull(bindingResult.getFieldError()).getDefaultMessage();
    }
}

</code></pre>
<h4 id="通用型validation配置">通用型validation配置</h4>
<ul>
<li>可查看 i18n-client 工程中的 com.philosopherzb.i18n.client.advice.config.CommonValidationConfig 类</li>
<li>此配置器对于 javax.validation.constraints 中的message的格式要求为el形式</li>
<li>eg:  @NotBlank(message = &quot;{PARAM_INVALID}&quot;)</li>
<li>eg:  @NotBlank(message = &quot;prefix-{PARAM_INVALID}-suffix&quot;)</li>
<li>其中 PARAM_INVALID 为国际化配置文件中的key; 在花括号的左右两边可以输入额外的参数</li>
</ul>
<pre><code>/**
 * 此配置器对于 javax.validation.constraints 中的message的格式要求为el形式
 * eg:  @NotBlank(message = &quot;{PARAM_INVALID}&quot;)
 * eg:  @NotBlank(message = &quot;prefix-{PARAM_INVALID}-suffix&quot;)
 * 其中 PARAM_INVALID 为配置文件中的key
 *
 * @author philosopherZB
 */
@Configuration
public class CommonValidationConfig {
    @Bean
    public LocalValidatorFactoryBean localValidatorFactoryBean(MessageSource messageSource) {
        LocalValidatorFactoryBean bean = new LocalValidatorFactoryBean();
        bean.setValidationMessageSource(messageSource);
        return bean;
    }
}
</code></pre>
<h4 id="定制化validation配置">定制化validation配置</h4>
<ul>
<li>
<p>可查看 i18n-client 工程中的 com.philosopherzb.i18n.client.advice.config.CustomValidationConfig 类</p>
</li>
<li>
<p>此配置器对于 javax.validation.constraints 中的message的格式要求为el形式，或者定制化分隔符</p>
</li>
<li>
<p>eg:  @NotBlank(message = &quot;{PARAM_INVALID}&quot;)</p>
</li>
<li>
<p>eg:  @NotBlank(message = &quot;prefix-{PARAM_INVALID}-suffix&quot;)</p>
</li>
<li>
<p>其中 PARAM_INVALID 为国际化配置文件中的key; 在花括号的左右两边可以输入额外的参数</p>
</li>
<li>
<p>eg:  @NotBlank(message = I18nConstant.PARAM_IS_NULL_BY_SEPARATE + &quot;orderSource&quot;)</p>
</li>
<li>
<p>其中 18nConstant.PARAM_IS_NULL_BY_SEPARATE 是一个组合字符串，由 PARAM_IS_NULL_BY + SEPARATE构成</p>
</li>
<li>
<p>PARAM_IS_NULL_BY为配置文件中的key，而orderSource则为对应的字段名</p>
</li>
</ul>
<pre><code>/**
 * 此配置器对于 javax.validation.constraints 中的message的格式要求为el形式，或者分隔符
 * eg:  @NotBlank(message = &quot;{PARAM_INVALID}&quot;)
 * eg:  @NotBlank(message = &quot;prefix-{PARAM_INVALID}-suffix&quot;)
 * 其中 PARAM_INVALID 为配置文件中的key
 * &lt;p&gt;
 * eg:  @NotBlank(message = I18nConstant.PARAM_IS_NULL_BY_SEPARATE + &quot;orderSource&quot;)
 * 其中 18nConstant.PARAM_IS_NULL_BY_SEPARATE 是一个组合字符串，由 PARAM_IS_NULL_BY + SEPARATE构成
 * PARAM_IS_NULL_BY为配置文件中的key，而orderSource则为对应的字段名
 *
 * @author philosopherZB
 */
@Configuration
public class CustomValidationConfig {

    private static final Pattern MESSAGE_PATTERN = Pattern.compile(&quot;(?&lt;=\\{)[^}]*(?=})&quot;);

    @Bean
    @Primary
    public Validator localValidator() {
        return Validation.byDefaultProvider()
                .configure()
                .messageInterpolator(new I18nMessageInterpolator())
                .buildValidatorFactory()
                .getValidator();
    }

    static class I18nMessageInterpolator implements MessageInterpolator {
        @Override
        public String interpolate(String s, Context context) {
            return interpolate(s, context, null);
        }

        @Override
        public String interpolate(String s, Context context, Locale locale) {
            if (MessageUtil.prepared()) {
                if (s.contains(I18nConstant.SEPARATE)) {
                    String[] split = s.split(I18nConstant.SEPARATE);
                    return MessageUtil.get(split[0], split[1]);
                }
                Matcher matcher = MESSAGE_PATTERN.matcher(s);
                if (matcher.find()) {
                    String temp = MessageUtil.get(matcher.group());
                    return s.replaceFirst(&quot;\\{&quot;.concat(matcher.group()).concat(&quot;\\}&quot;), temp);
                }
                return MessageUtil.get(s);
            }
            return s;
        }
    }
}

</code></pre>
<pre><code>/**
 * @author philosopherZB
 */
public class I18nConstant {
    /**
     * 分隔符
     */
    public static final String SEPARATE = &quot;~&quot;;

    /**
     * 空值参数，可以指定参数名
     */
    public static final String PARAM_IS_NULL_BY = &quot;PARAM_IS_NULL_BY&quot;;
    /**
     * 空值参数，有分隔符，可以指定参数名
     */
    public static final String PARAM_IS_NULL_BY_SEPARATE = PARAM_IS_NULL_BY + SEPARATE;
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[health-check-sdk 使用手册]]></title>
        <id>https://philosopherzb.github.io/post/health-check-sdk-shi-yong-shou-ce/</id>
        <link href="https://philosopherzb.github.io/post/health-check-sdk-shi-yong-shou-ce/">
        </link>
        <updated>2022-08-20T06:12:12.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述health-check-sdk 的使用方式。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/lake-192978_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="源码地址">源码地址</h2>
<p><a href="https://github.com/philosopherZB/demo-all/tree/master/health-check">点击此处跳转github查看具体源码</a></p>
<h2 id="查看自带的健康监测">查看自带的健康监测</h2>
<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
    &lt;/dependency&gt;
</code></pre>
<p>如果项目中有引入spring-cloud-stream-binder-xxx之类的jar，那么其中可能已经编写了对应的服务健康监测，可以先引入上述jar查看<br>
在配置文件中加入如下内容:<br>
<em>此配置需在项目中的启动yaml中的配置</em><br>
<em>禁用所有的监控点</em><br>
<code>management.endpoints.enabled-by-default=false</code><br>
<em>启用health监控点</em><br>
<code>management.endpoint.health.enabled=true</code><br>
<em>显示详细内容</em><br>
<code>management.endpoint.health.show-details=always</code><br>
随后访问：http://ip:prot//actuator/health 观察是否已经存在对应的中间件健康监测</p>
<h2 id="定制化的健康监测操作如下所示">定制化的健康监测操作如下所示：</h2>
<h3 id="引入pom">引入pom</h3>
<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;com.dbappsecurity.cloudpl.mss&lt;/groupId&gt;
        &lt;artifactId&gt;health&lt;/artifactId&gt;
        &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<h4 id="如果要监测kafka需额外引入如下pom">如果要监测kafka，需额外引入如下pom</h4>
<pre><code>    &lt;!-- kafka --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
        &lt;version&gt;2.7.0&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<h4 id="如果要监测minio则需要额外引入如下pom">如果要监测minio，则需要额外引入如下pom</h4>
<pre><code>    &lt;!-- minio --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.minio&lt;/groupId&gt;
        &lt;artifactId&gt;minio&lt;/artifactId&gt;
        &lt;version&gt;8.0.3&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<h4 id="如果要监测rocketmq则需要额外引入如下pom">如果要监测rocketmq，则需要额外引入如下pom</h4>
<pre><code>    &lt;!-- rocketmq --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt;
        &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt;
        &lt;version&gt;4.9.1&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<h4 id="如果要监测xxl-job则需要额外引入如下pom">如果要监测xxl-job，则需要额外引入如下pom</h4>
<pre><code>    &lt;!-- xxl-job --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.xuxueli&lt;/groupId&gt;
        &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt;
        &lt;version&gt;2.3.0&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<h3 id="在配置文件中添加如下内容">在配置文件中添加如下内容</h3>
<h4 id="在启动应用程序的配置文件中加入如下内容">在启动应用程序的配置文件中加入如下内容:</h4>
<p><em>此配置需在项目中的启动yaml中的配置</em><br>
<em>禁用所有的监控点</em><br>
<code>management.endpoints.enabled-by-default=false</code><br>
<em>启用health监控点</em><br>
<code>management.endpoint.health.enabled=true</code><br>
<em>显示详细内容</em><br>
<code>management.endpoint.health.show-details=always</code></p>
<h4 id="以下配置可以放在nacos或其他配置中心里">以下配置可以放在nacos或其他配置中心里</h4>
<h5 id="kafka-配置">kafka 配置</h5>
<p><em>启动kafka监测</em><br>
<code>health.check.kafka.enabled=true</code><br>
<em>kafka地址,此参数必填，格式：master:9092,slave1:9092,slave2:9092</em><br>
<code>health.check.kafka.server-address=10.50.79.250:9091</code><br>
<em>kafka监测主题,根据自己项目进行命名。必须存在，否则将无法发送消息至主题或从主题消费消息。</em><br>
<em>格式：{project-name}-health-check-topic</em><br>
<code>health.check.kafka.topic=service-plan-health-check-topic</code></p>
<h5 id="minio-配置">minio 配置</h5>
<p><em>启动minio监测</em><br>
<code>health.check.minio.enabled=true</code><br>
<em>minio 服务地址</em><br>
<code>health.check.minio.end-point=http://10.50.79.250:9000</code><br>
<em>access key</em><br>
<code>health.check.minio.access-key=wfawggehhhwehw</code><br>
<em>secret key</em><br>
<code>health.check.minio.secret-key=1231231231</code><br>
<em>文件名</em><br>
<em>格式：{project-name}-health-check</em><br>
<code>health.check.minio.file-name=service-plan-health-check</code></p>
<h5 id="rocketmq-配置">rocketmq 配置</h5>
<p><em>启动rocketmq监测</em><br>
<code>health.check.rocketmq.enabled=true</code><br>
<em>rocketmq 服务地址, 格式：master:9876;slave1:9876;slave2:9876</em><br>
<code>health.check.rocketmq.server-address=10.50.79.236:9876</code><br>
<em>rocketmq 监测tag, 根据自己项目进行命名</em><br>
<em>格式：{project-name}-health-check-tag</em><br>
<code>health.check.rocketmq.tag=service-plan-health-check-tag</code></p>
<h5 id="xxl-job-配置">xxl-job 配置</h5>
<p><em>启动xxl-job监测，注意：xxl-job还需要在xxl-web界面配置一个bean名为healthCheckJobHandler的执行任务，并启动该任务，</em><br>
<em>其中调度类型可以选择固定速度，值选30（此值应与health.check.xxljob.execute-interval的配置值保持一致）。</em><br>
<code>health.check.xxljob.enabled=true</code></p>
<p><em>注意如果项目中已配置XxlJobSpringExecutor 的bean 则不需要再额外添加以下内容</em><br>
<em>此配置与正常的xxl-job配置一致</em><br>
<code>health.check.xxljob.admin-addresses=</code><br>
<code>health.check.xxljob.access-token=</code><br>
<code>health.check.xxljob.appname=</code><br>
<code>health.check.xxljob.address=</code><br>
<code>health.check.xxljob.ip=</code><br>
<code>health.check.xxljob.port=</code><br>
<code>health.check.xxljob.log-path=</code><br>
<code>health.check.xxljob.log-retention-days=</code></p>
<h2 id="注意事项">注意事项</h2>
<p>源码在health-check-sdk模块中，如果有额外的自定义监测，可以模仿编写</p>
<p>该模块引入的spring-boot-starter-actuator已提供常用的健康监测，如db，redis，es等</p>
<h2 id="样例">样例</h2>
<p>启动时，可以通过接口访问健康状态：http://ip:prot//actuator/health<br>
注意：此接口需要左安全保证，防止应用数据泄露。<br>
正常内容如下：</p>
<pre><code>{    
    &quot;status&quot;: &quot;UP&quot;,
    &quot;components&quot;: {
        &quot;diskSpace&quot;: {
            &quot;status&quot;: &quot;UP&quot;,
            &quot;details&quot;: {
                &quot;total&quot;: 381826846720,
                &quot;free&quot;: 310320566272,
                &quot;threshold&quot;: 10485760,
                &quot;exists&quot;: true
            }
        },
        &quot;kafka&quot;: {
            &quot;status&quot;: &quot;UP&quot;
        },
        &quot;minio&quot;: {
            &quot;status&quot;: &quot;UP&quot;
        },
        &quot;ping&quot;: {
            &quot;status&quot;: &quot;UP&quot;
        },
        &quot;rocketMQ&quot;: {
            &quot;status&quot;: &quot;UP&quot;
        },
        &quot;xxljob&quot;: {
            &quot;status&quot;: &quot;UP&quot;
        }
    }
}
</code></pre>
<p>异常内容如下：</p>
<pre><code> {  
     &quot;status&quot;: &quot;DOWN&quot;,
    &quot;components&quot;: {
        &quot;diskSpace&quot;: {
            &quot;status&quot;: &quot;UP&quot;,
            &quot;details&quot;: {
                &quot;total&quot;: 381826846720,
                &quot;free&quot;: 310320607232,
                &quot;threshold&quot;: 10485760,
                &quot;exists&quot;: true
            }
        },
        &quot;kafka&quot;: {
            &quot;status&quot;: &quot;DOWN&quot;,
            &quot;details&quot;: {
                &quot;topic&quot;: &quot;client-health-check-topic&quot;,
                &quot;serviceAddress&quot;: &quot;127.0.0.1:9092&quot;,
                &quot;errorMsg&quot;: &quot;received msg not equals expected msg&quot;
            }
        },
        &quot;minio&quot;: {
            &quot;status&quot;: &quot;DOWN&quot;,
            &quot;details&quot;: {
                &quot;error&quot;: &quot;com.philosopherzb.health.check.exception.HealthCheckException: java.nio.file.FileAlreadyExistsException: \\temp\\temp.txt&quot;,
                &quot;endpoint&quot;: &quot;https://play.min.io&quot;,
                &quot;message&quot;: &quot;download file failure&quot;
            }
        },
        &quot;ping&quot;: {
            &quot;status&quot;: &quot;UP&quot;
        },
        &quot;rocketMQ&quot;: {
            &quot;status&quot;: &quot;DOWN&quot;,
            &quot;details&quot;: {
                &quot;topic&quot;: &quot;rocketmq-default-health-check-topic&quot;,
                &quot;tag&quot;: &quot;client-health-check-tag&quot;,
                &quot;serviceAddress&quot;: &quot;127.0.0.1:9876&quot;,
                &quot;errorMsg&quot;: &quot;received msg not equals expected msg&quot;
            }
        },
        &quot;xxljob&quot;: {
            &quot;status&quot;: &quot;DOWN&quot;,
            &quot;details&quot;: {
                &quot;errorMsg&quot;: &quot;time offset too big, standard value:  33&quot;
            }
        }
    }
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之日志&事务&扩展]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-ri-zhi-andshi-wu-andkuo-zhan/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-ri-zhi-andshi-wu-andkuo-zhan/">
        </link>
        <updated>2022-08-06T02:07:21.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka的日志存储，事务操作以及扩展的mmp，零拷贝等信息。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/lake-1681485_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="日志">日志</h2>
<h3 id="简介">简介</h3>
<p>Kafka 中的消息是以主题为基本单位进行归类的，各个主题在逻辑上相互独立。每个主题又可以分为一个或多个分区。不考虑多副本的情况，一个分区对应一个日志（Log）；本质上其实是一个副本对应一个日志（log）。</p>
<p>为了防止 Log 过大，Kafka 中引入了日志分段（LogSegment）的概念，所谓日志分段，就是当一个日志文件大小到达一定条件之后，就新建一个新的日志分段，然后在新的日志分段写入数据。</p>
<p>Log 和 LogSegment 也不是纯粹物理意义上的概念，Log 在物理上只以文件夹的形式存储，而每个 LogSegment 对应于磁盘上的一个日志文件（.log）和两个索引文件（.index，.timeindex），以及可能的其他文件（比如以“.txnindex”为后缀的事务索引文件）</p>
<p>日志分段文件中的索引文件主要是用来提高查找消息的效率:</p>
<ul>
<li>偏移量索引文件用来建立消息偏移量（offset）到物理地址之间的映射关系，方便快速定位消息所在的物理文件位置。</li>
<li>时间戳索引文件则根据指定的时间戳（timestamp）来查找对应的偏移量信息。</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230317001.png" alt="img" loading="lazy"></figure>
<h3 id="日志操作">日志操作</h3>
<p>日志实现图</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230317002.png" alt="img" loading="lazy"></figure>
<h4 id="写操作writes">写操作（Writes）</h4>
<p>日志允许在文件的尾部进行串行化追加数据。当这个文件达到可配置的大小（比如 1GB）时，它会滚动到一个新文件。日志有两个重要的配置参数：M：表示操作系统强制将文件刷新到磁盘之前要写入的消息数；S：表示经过多少秒才能强制刷新到磁盘。这种特性保证了在系统崩溃时最多丢失M条消息或S秒数据。</p>
<h4 id="读操作reads">读操作（Reads）</h4>
<p>读取依赖消息的 64-bit逻辑偏移量以及S-byte最大块大小。此操作将返回S-byte缓冲区中所包含的消息迭代器。S旨在大于任何单个消息，但在消息异常大的情况下，可以多次重试读取，每次将缓冲区大小加倍，直到成功读取消息。可以指定最大消息和缓冲区大小以使服务器拒绝大于某个大小的消息，并为客户端提供它需要读取的最大值以获得完整的消息。读取缓冲区很可能以部分消息结尾，这很容易通过分隔大小来检测。</p>
<p>偏移量的实际读取过程需要首先找到存储数据的日志段文件，从而根据全局偏移量值计算文件特定的偏移量，然后从该文件偏移量中读取。搜索的实际处理过程是通过为每个文件维护的内存范围的简单二进制搜索变体来完成的。</p>
<p>该日志提供了获取最近写入消息的能力，以允许客户端从“现在”开始订阅。这在消费者未能在其 SLA 指定的天数内使用其数据的情况下也很有用。在这种情况下，当客户端尝试使用不存在的偏移量时，将返回OutOfRangeException 并且可以根据用例自行重置或失败。</p>
<h4 id="删除操作deletes">删除操作（Deletes）</h4>
<p>数据一次删除一个日志段。日志管理器应用两个指标来识别符合删除条件的段：时间和大小。对于基于时间的策略，会考虑记录时间戳，段文件中的最大时间戳（记录顺序不相关）定义整个段的保留时间。默认情况下会禁用基于大小的保留策略。启用后，日志管理器依然会继续删除最旧的段文件，直到分区的总大小再次在配置的限制内。如果同时启用这两个策略，则将删除符合任一策略段。</p>
<p>为了避免锁定读取，同时仍然允许修改段列表的删除，kafka使用了写时复制(copy-on-write )样式的段列表实现，该实现提供了一致的视图，以允许在日志段的不可变静态快照视图上进行二分查找，即使该日志段正在进行删除操作。</p>
<h3 id="日志保留log-retention">日志保留（Log Retention）</h3>
<p>日志保留指的是按照一定的保留策略直接删除不符合条件的日志分段，也可以简单理解为日志删除操作。可以通过服务端的参数 log.cleanup.policy来进行配置，此值默认为delete（表示删除），还有一个可选值compact（表示压缩）。</p>
<h4 id="基于时间">基于时间</h4>
<p>kafka默认是基于时间对日志进行删除操作的。</p>
<p>日志删除任务会检查当前日志文件中是否有保留时间超过设定的阈值（retention.ms）来寻找可删除的日志分段文件集合（deletableSegments）。</p>
<p>retention.ms 可以通过 broker 端参数 log.retention.hours、log.retention.minutes 和 log.retention.ms 来配置，优先级：log.retention.ms &gt; log.retention.minutes &gt; log.retention.hours 。默认情况下只配置了 log.retention.hours 参数，其值为168，故默认情况下日志分段文件的保留时间为7天。</p>
<p>删除日志分段时，首先会从 Log 对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作。然后将日志分段所对应的所有文件添加上“.deleted”的后缀（当然也包括对应的索引文件）。最后交由一个以“delete-file”命名的延迟任务来删除这些以“.deleted”为后缀的文件，这个任务的延迟执行时间可以通过 file.delete.delay.ms 参数来调配，此参数的默认值为60000，即1分钟。</p>
<h4 id="基于日志大小">基于日志大小</h4>
<p>日志删除任务会检查当前日志的大小是否超过设定的阈值（retention.bytes）来寻找可删除的日志分段的文件集合（deletableSegments）。</p>
<p>retention.bytes 可以通过 broker 端参数 log.retention.bytes 来配置，默认值为-1，表示无穷大(即只有时间限制，没有大小限制)。</p>
<p>注意：log.retention.bytes 配置的是分区（由日志段组成）的最大大小，而不是单个日志分段（确切地说应该为 .log 日志文件）的大小。</p>
<p>单个日志分段的大小(segment.bytes)由 broker 端参数 log.segment.bytes 来限制，默认值为1073741824，即 1GB。此参数控制着日志分段达到多大才会新建一个新的日志分段。</p>
<p>删除操作和基于时间的保留策略的删除操作相同。</p>
<h4 id="基于lso手动删除">基于LSO（手动删除）</h4>
<p>基于LSO（日志起始偏移量）的保留策略的判断依据是某日志分段的下一个日志分段的起始偏移量 baseOffset 是否小于等于 logStartOffset，若是，则可以删除此日志分段（放入deletableSegments中）。</p>
<p>一般情况下，日志文件的起始偏移量 logStartOffset 等于第一个日志分段的 baseOffset，但这并不是绝对的，logStartOffset 的值可以通过 DeleteRecordsRequest 请求(比如使用 KafkaAdminClient 的 deleteRecords()方法、使用 kafka-delete-records.sh 脚本、日志的清理和截断等操作）进行修改。</p>
<p>删除操作和基于时间的保留策略的删除操作相同。</p>
<pre><code>// 使用命令删除记录
// 假设现在存在一个名为‘test-topic'的topic，要求删除其所有记录。
// 首先需要定义一个名为delete-test.config 的文件，文件内容如下：
{
  &quot;partitions&quot;: [
    {
      &quot;topic&quot;: &quot;test-topic&quot;,
      &quot;partition&quot;: 0,
      &quot;offset&quot;: -1 // -1 表示无穷大偏移量，即删除所有记录数据
    }
  ],
  &quot;version&quot;: 1
}
// 然后运行kafka-delete-records.sh
kafka-delete-records.sh --bootstrap-server localhost:9092 --offset-json-file ./delete-test.config

</code></pre>
<h4 id="日志压缩log-compaction">日志压缩（Log Compaction）</h4>
<p>日志压缩（Log Compaction）：Kafka 将始终为单个主题分区的数据日志中的每个消息键至少保留最后一个已知值；即针对每个消息的 key 进行整合，对于有相同 key 的不同 value 值，只保留最后一个版本。这对于解决某些应用程序崩溃、系统故障后恢复或者应用在运行维护过程中重启后重新加载缓存的场景很有效。</p>
<p>采用日志压缩的清理策略时需要将 log.cleanup.policy 设置为“compact”，并且还需要将 log.cleaner.enable （默认值为 true）设定为 true（此值如果设置为false，那么日志将不会被压缩，且一直增长）。除此之外还有两个参数可以控制日志压缩：log.cleaner.min.compaction.lag.ms（防止比该时间更小的更新消息被压缩） 和 log.cleaner.max.compaction.lag.ms（防止某些消息一直进行不恰当的压缩操作）</p>
<p>Kafka 日志的逻辑结构以及每条消息的偏移量，如下所示：</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230317003.png" alt="img" loading="lazy"></figure>
<p>日志头部与传统的kafka日志一样，它具有紧凑的顺序偏移并保留所有的消息。与此同时，日志压缩还提供了已被压缩的日志尾部，就如上图所示。</p>
<p>注意：</p>
<ul>
<li>日志尾部的消息保留了第一次写入时分配的原始偏移量且该值永远不会改变。</li>
<li>所有偏移量在日志中仍然具备有效位置（即使已被压缩）；如上图所示，36,37,38的偏移位置是等效的；即从这些偏移量位置中无论选择了哪一个，都会从返回从38开始的消息集。</li>
</ul>
<p>压缩还允许删除。带有键和空负载的消息将被视为从日志中删除。这样的记录有时被称为墓碑（tombstone）。此删除标记将导致删除具有该键的任何先前消息（与具有该键的任何新消息一样），但删除标记的特殊之处在于它们本身将在一段时间后从日志中清除以释放空间. 删除不再保留的时间点在上图中标记为“删除保留点”。</p>
<p>压缩是通过定期重新复制日志段在后台完成的。清理不会阻塞读取，并且可以限制为使用不超过可配置量的 I/O 吞吐量，以避免影响生产者和消费者。压缩日志段的实际过程如下所示：</p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230317004.png" alt="img" loading="lazy"></figure>
<h2 id="事务">事务</h2>
<h3 id="简介-2">简介</h3>
<ul>
<li>kafka的事务机制是其实现端到端有且仅有一次语义（end-to-end EOS)（Exactly once：每条消息会传递一次且仅有一次）的基础。</li>
<li>transactional producer 和 transactional consumer 两者配合使用，才能实现端到端有且仅有一次的语义（end-to-end EOS)，即事务消息。</li>
<li>kafka的事务机制在底层依赖于幂等生产者，幂等生产者是 kafka 事务的必要不充分条件；事实上，开启 kafka事务时，kafka 会自动开启幂等生产者。</li>
<li>内部topic：__transaction_state：用来存储事务日志消息；__consumer_offsets：作用是保存 Kafka 消费者的位移信息。</li>
</ul>
<h3 id="内部事务支持">内部事务支持</h3>
<p>为支持事务机制，kafka引入了两个新的组件：TransactionCoordinator 和 __transaction_state（内部Topic）</p>
<ul>
<li>__transaction_state 有多个分区，每个分区副本都有一个 leader，该 leade对应哪个 kafka broker，哪个 broker 上的 TransactionCoordinator 就负责对这些分区副本的写操作（所有的事务状态值都会持久化到 __transaction_state 这个内部topic中）。</li>
<li>由于 TransactionCoordinator 是 kafka broker 内部的一个模块，而 __transaction_state 是 kakfa 的一个内部 topic, 所以 kafka可以通过内部的复制协议和选举机制（replication protocol and leader election processes)，来确保 TransactionCoordinator 的可用性和 transaction state 的持久性。</li>
<li>__transaction_state 内部存储的只是事务的最新状态和其相关元数据信息，kafka producer 生产的原始消息，仍然是只存储在kafka producer指定的 topic 中。</li>
<li>实际上，每个 transactional.id 通过 hash 都对应到 了 __transaction_state 的一个分区，所以每个 transactional.id 都有且仅有一个 TransactionCoordinator 负责。</li>
</ul>
<pre><code>// 事务状态
public enum TransactionState {
    ONGOING(&quot;Ongoing&quot;),
    PREPARE_ABORT(&quot;PrepareAbort&quot;),
    PREPARE_COMMIT(&quot;PrepareCommit&quot;),
    COMPLETE_ABORT(&quot;CompleteAbort&quot;),
    COMPLETE_COMMIT(&quot;CompleteCommit&quot;),
    EMPTY(&quot;Empty&quot;),
    PREPARE_EPOCH_FENCE(&quot;PrepareEpochFence&quot;),
   UNKNOWN(&quot;Unknown&quot;);
}  

</code></pre>
<h3 id="使用及流程">使用及流程</h3>
<h4 id="使用配置">使用配置</h4>
<p>producer端需配置acks = all，retries &gt; 1，transactional.id = some unique id；consumer端根据需要配置isolation.level 为 read_committed 或 read_uncommitted(此为默认值)。</p>
<pre><code># producer config
enable.idempotence = true # 默认为true，表示是否开启producer 幂等性
acks = all
retries &gt; 1
transactional.id = some unique id

# consumer config
isolation.level = read_committed # 可选值 read_committed，read_uncommitted(此为默认值)

</code></pre>
<h4 id="流程">流程</h4>
<p>Kafka中的事务可以使应用程序将消费消息、生产消息、提交消费位移当作原子操作来处理，同时成功或失败，即使该生产或消费会跨多个分区。</p>
<pre><code>// 简单发送消息示例代码
try {
    producer.initTransactions();
    producer.beginTransaction();
    producer.send(new ProducerRecord&lt;&gt;(&quot;topic0&quot;, &quot;key0&quot;, &quot;msg0&quot;));
    producer.send(new ProducerRecord&lt;&gt;(&quot;topic1&quot;, &quot;key1&quot;, &quot;msg1&quot;));
    producer.commitTransaction();
} catch (KafkaException e) {
   producer.abortTransaction();
}

</code></pre>
<ul>
<li>生产者必须提供唯一的transactionalId，启动后请求事务协调器获取一个PID，transactionalId与PID一一对应。（使用spring-kafka时，可以通过配置spring.kafka.producer.transaction-id-prefix属性及@Transactional启动事务：详见：<a href="https://docs.spring.io/spring-kafka/docs/current/reference/html/#transactions">点击此处跳转详情页面</a>）</li>
<li>每次发送数据给&lt;Topic, Partition&gt;前，需要先向事务协调器发送AddPartitionsToTxnRequest，事务协调器会将该&lt;Transaction, Topic, Partition&gt;存于__transaction_state内，并将其状态置为Ongoing。</li>
<li>在处理完 AddOffsetsToTxnRequest 之后，生产者还会发送 TxnOffsetCommitRequest 请求给 GroupCoordinator，从而将本次事务中包含的消费位移信息 offsets 存储到主题 __consumer_offsets 中。</li>
<li>一旦上述数据写入操作完成，应用程序必须调用KafkaProducer的commitTransaction方法或者abortTransaction方法以结束当前事务。无论调用 commitTransaction() 方法还是 abortTransaction() 方法，生产者都会向 TransactionCoordinator 发送 EndTxnRequest 请求。</li>
<li>TransactionCoordinator 收到请求后首先会将 PrepareCommit或 PrepareAbort消息写入主题 __transaction_state；然后通过 WriteTxnMarkersRequest 请求将 commit或 abort信息写入用户所使用的普通主题和 __consumer_offsets；最后将 CompleteCommit或 CompleteAbort信息写入内部主题 __transaction_state标明该事务结束。</li>
</ul>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaApis.scala
case ApiKeys.ADD_PARTITIONS_TO_TXN =&gt; handleAddPartitionToTxnRequest(request, requestLocal)
case ApiKeys.ADD_OFFSETS_TO_TXN =&gt; handleAddOffsetsToTxnRequest(request, requestLocal)
case ApiKeys.END_TXN =&gt; handleEndTxnRequest(request, requestLocal)
case ApiKeys.WRITE_TXN_MARKERS =&gt; handleWriteTxnMarkersRequest(request, requestLocal)
case ApiKeys.TXN_OFFSET_COMMIT =&gt; handleTxnOffsetCommitRequest(request, requestLocal)

</code></pre>
<h2 id="扩展">扩展</h2>
<h3 id="延迟队列">延迟队列</h3>
<p>Kafka 在发送延时消息的时候并不是先投递到要发送的真实主题（real_topic）中，而是先投递到一些 Kafka 内部的主题（delay_topic）中，这些内部主题对用户不可见，然后通过一个自定义的服务拉取这些内部主题中的消息，并将满足条件的消息再投递到要发送的真实的主题中，消费者所订阅的还是真实的主题。</p>
<p>如果采用这种方案，那么一般是按照不同的延时等级来划分的，比如设定5s、10s、30s、1min、2min、5min、10min、20min、30min、45min、1hour、2hour这些按延时时间递增的延时等级，延时的消息按照延时时间投递到不同等级的主题中，投递到同一主题中的消息的延时时间会被强转为与此主题延时等级一致的延时时间，这样延时误差控制在两个延时等级的时间差范围之内（比如延时时间为17s的消息投递到30s的延时主题中，之后按照延时时间为30s进行计算，延时误差为13s）。虽然有一定的延时误差，但是误差可控，并且这样只需增加少许的主题就能实现延时队列的功能。</p>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230317005.png" alt="" loading="lazy"></figure>
<p>发送到内部主题（delaytopic）中的消息会被一个独立的 DelayService 进程消费，这个 DelayService 进程和 Kafka broker 进程以一对一的配比进行同机部署（参考下图），以保证服务的可用性。</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230317006.png" alt="" loading="lazy"></figure>
<p>针对不同延时级别的主题，在 DelayService 的内部都会有单独的线程来进行消息的拉取，以及单独的 DelayQueue（这里用的是 JUC 中 DelayQueue）进行消息的暂存。与此同时，在 DelayService 内部还会有专门的消息发送线程来获取 DelayQueue 的消息并转发到真实的主题中。从消费、暂存再到转发，线程之间都是一一对应的关系。如下图所示，DelayService 的设计应当尽量保持简单，避免锁机制产生的隐患。</p>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230317007.png" alt="img" loading="lazy"></figure>
<p>为了保障内部 DelayQueue 不会因为未处理的消息过多而导致内存的占用过大，DelayService 会对主题中的每个分区进行计数，当达到一定的阈值之后，就会暂停拉取该分区中的消息。</p>
<p>因为一个主题中一般不止一个分区，分区之间的消息并不会按照投递时间进行排序，DelayQueue的作用是将消息按照再次投递时间进行有序排序，这样下游的消息发送线程就能够按照先后顺序获取最先满足投递条件的消息。</p>
<h3 id="重试队列与死信队列">重试队列与死信队列</h3>
<p>Kafka 中的死信可以看作消费者不能处理的receive消息，也可以看作消费者不想处理的receive消息，还可以看作不符合处理要求的消息。比如消息内包含的消息内容无法被消费者解析，为了确保消息的可靠性而不被随意丢弃，故将其投递到死信队列中，这里的死信就可以看作消费者不能处理的消息。再比如超过既定的重试次数之后将消息投入死信队列，这里就可以将死信看作不符合处理要求的消息。</p>
<p>重试队列其实可以看作一种回退队列，具体指消费端消费消息失败时，为了防止消息无故丢失而重新将消息回滚到 broker 中。与回退队列不同的是，重试队列一般分成多个重试等级，每个重试等级一般也会设置重新投递延时，重试次数越多投递延时就越大。</p>
<p>一般情况下，重试队列可以通过延迟队列来实现，同时与死信队列配合使用。在实际操作过程中，重试越多次重新投递的时间就越久，并且需要设置一个上限，超过投递次数就进入死信队列。</p>
<h3 id="页缓存">页缓存</h3>
<p>页缓存是操作系统实现的一种主要的磁盘缓存，以此用来减少对磁盘 I/O 的操作。具体来说，就是把磁盘中的数据缓存到内存中，把对磁盘的访问变为对内存的访问。</p>
<p>当一个进程准备读取磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页（page）是否在页缓存（pagecache）中，如果存在（命中）则直接返回数据，从而避免了对物理磁盘的 I/O 操作；如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。</p>
<p>同样，如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性。</p>
<p>使用页缓存时，操作系统读的时候会预读，根据局部性原理当读取的时候会把相邻的磁盘块读入页缓存中。在写入的时候会延迟写，写入的也是页缓存，这样可以将一些小的写入操作合并成大的写入（类似于批量写入），然后再刷盘。</p>
<p>当然这样的写入操作存在数据丢失的风险，例如机器突然断电，那些还未刷盘的脏页就丢失了。不过可以调用 fsync 强制刷盘，但是这样对于性能的损耗较大。</p>
<p>因此一般建议通过多副本机制来保证消息的可靠，而不是同步刷盘。（例如：kafka的分区多副本）</p>
<h3 id="文件内存映射mmap">文件内存映射（mmap）</h3>
<p>Memory Mapped Files：简称 mmap，也有叫 MMFile 的，使用 mmap 的目的是将内核中读缓冲区（read buffer）的地址与用户空间的缓冲区（user buffer）进行映射。从而实现内核缓冲区与应用程序内存的共享，省去了将数据从内核读缓冲区（read buffer）拷贝到用户缓冲区（user buffer）的过程。它的工作原理是直接利用操作系统的 Page 来实现文件到物理内存的直接映射。完成映射之后用户对物理内存的操作会被同步到硬盘上。</p>
<p>使用这种方式可以获取很大的 I/O 提升，省去了用户空间到内核空间复制的开销。</p>
<p>mmap 也有一个很明显的缺陷——不可靠，写到 mmap 中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用 flush 的时候才把数据真正的写到硬盘。Kafka 提供了一个参数：producer.type 来控制是不是主动 flush；如果 Kafka 写入到 mmap 之后就立即 flush 然后再返回 Producer 叫同步(sync)；写入 mmap 之后立即返回 Producer 不调用 flush 就叫异步(async)，默认是 sync。</p>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230317008.png" alt="img" loading="lazy"></figure>
<h3 id="零拷贝zero-copy">零拷贝（Zero-copy）</h3>
<p>零拷贝（Zero-copy）技术指在计算机执行操作时，CPU 不需要先将数据从一个内存区域复制到另一个内存区域，从而可以减少上下文切换以及 CPU 的拷贝时间。</p>
<p>它的作用是在数据报从网络设备到用户程序空间传递的过程中，减少数据拷贝次数，减少系统调用，实现 CPU 的零参与，彻底消除 CPU 在这方面的负载。</p>
<p>对 Linux（Linux 2.4+）操作系统而言，零拷贝技术依赖于底层的 sendfile()方法实现（数据通过 DMA 拷贝到内核态 Buffer 后，直接通过 DMA 拷贝到 NIC Buffer，无需 CPU 拷贝）。对应于Java 语言 FileChannal.transferTo() 方法的底层实现就是 sendfile() 方法。</p>
<p>DMA（Direct Memory Access）：直接存储器访问。DMA 是一种无需 CPU 的参与，让外设和系统内存之间进行双向数据传输的硬件机制。使用 DMA 可以使系统 CPU 从实际的 I/O 数据传输过程中摆脱出来，从而大大提高系统的吞吐率。</p>
<p>以磁盘文件通过网络发送为例，如果未使用零拷贝技术，那么总共需要经过如下四次上下文切换：</p>
<ol>
<li>首先通过系统调用将文件数据读入到内核态 Buffer（DMA 拷贝）</li>
<li>然后应用程序将内核态 Buffer 数据读入到用户态 Buffer（CPU 拷贝）</li>
<li>接着用户程序通过 Socket 发送数据时将用户态 Buffer 数据拷贝到内核态 Buffer（CPU 拷贝）</li>
<li>最后通过 DMA 拷贝将数据拷贝到 NIC Buffer（NIC：Network Interface Card，一般称为网卡，也叫网络适配器，是电脑与局域网相互连接的设备）</li>
</ol>
<figure data-type="image" tabindex="10"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230317009.png" alt="img" loading="lazy"></figure>
<p>如果使用了零拷贝技术（Linux2.4 版本的 sendfile + 带 「分散-收集（Scatter-gather）」的DMA），那么只需要经过如下两次上下文切换，且没有冗余</p>
<p>Kafka 在这里采用的方案是通过 NIO 的 transferTo/transferFrom 调用操作系统的 sendfile 实现零拷贝。总共发生 2 次内核数据拷贝、2 次上下文切换和一次系统调用，消除了 CPU 数据拷贝。</p>
<ol>
<li>首先通过系统调用sendfile 将文件数据读入到内核态 Buffer（DMA 拷贝）</li>
<li>操作结果响应回用户态，并将内核态 Buffer 数据通过 DMA拷贝至 NIC Buffer</li>
</ol>
<figure data-type="image" tabindex="11"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230317010.png" alt="img" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之消费者重平衡]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-xiao-fei-zhe-chong-ping-heng/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-xiao-fei-zhe-chong-ping-heng/">
        </link>
        <updated>2022-07-30T01:45:22.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka 消费者重平衡机制和源码。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/sunset-5800386_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="重平衡">重平衡</h2>
<h3 id="简介">简介</h3>
<p>重平衡(Rebalance)是指在一个消费组内分区的所属权从一个消费者转移到另一消费者的行为，它为消费组具备高可用性和伸缩性提供了保障，使我们可以既方便又安全地添加或删除消费组内的消费者。</p>
<p>注意：当消费者手动分配分区时，重平衡机制将不会生效；重平衡过程可能会导致消息重复消费。</p>
<p>重平衡时机：新增消费者，移除消费者（消费者crash），新增主题分区。</p>
<p>关于消费者crash的重平衡：在此场景中，消费组中的每个consumer都会定期向coordinator发送心跳请求来表明其还存活；如果某个consumer无法及时发送心跳，那么coordinator便会认为其已经crash，将开始新一轮的rebalance。</p>
<p>通常来说，kafka提供了三个参数来控制consumer是否存活：session.timeout.ms &amp; heartbeat.interval.ms &amp; max.poll.interval.ms</p>
<ul>
<li>session.timeout.ms：默认10s，表示如果10s内未曾收到consumer的心跳，则认为该consumer已经crash。需要注意的是，此值配置应在group.min.session.timeout.ms(默认6s) 和 group.max.session.timeout.ms(默认30min)之间。</li>
<li>heartbeat.interval.ms：默认3s，表示consumer向coordinator发送心跳的间隔，一般此值不应该大于session.timeout.ms的三分之一，可以通过调低此值来控制重平衡的预期时间。例如将其调整为2s，那么在默认的session.timeout.ms配置下，将会发送5次心跳（原3s只会发送3次心跳）。</li>
<li>max.poll.interval.ms：默认5min，表示消费者两次poll数据之间的最大延迟间隔。如果在间隔时间内未进行poll操作，则认为该消费者crash，消费组将会进行重平衡。在此过程中，如果消费者具备非空group.instance.id，那么消费组并不会立刻发起重平衡，而是停止该消费者的心跳发送动作，等待session.timeout.ms超时再发起重平衡。这也体现了已关闭的静态消费者行为。针对线上某些数据处理时间过长，可以适当调整此参数，防止非必要的重平衡出现。</li>
</ul>
<h3 id="消费组状态">消费组状态</h3>
<p>Kafka 为消费者组定义了 5 种状态，它们分别是：Empty、Dead、PreparingRebalance、CompletingRebalance 和 Stable。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala
private[group] sealed trait GroupState {
  val validPreviousStates: Set[GroupState]
}

/**
 * 准备重平衡
 *
 * action: 使用 REBALANCE_IN_PROGRESS 响应心跳
 *        使用 REBALANCE_IN_PROGRESS 响应同步组
 *        移除离开组请求的成员
 *        保留来自新成员或现有成员的加入群组请求，直到所有预期成员都加入
 *        允许前一代的offset提交
 *        允许获取offset
 * transition: 某些成员在超时之前加入 =&gt; CompletingRebalance
 *             所有成员离开群组 =&gt; Empty
 *             组被分区迁移所移除 =&gt; Dead
 */
private[group] case object PreparingRebalance extends GroupState {
  val validPreviousStates: Set[GroupState] = Set(Stable, CompletingRebalance, Empty)
}

/**
 * 组开始等待leader进行状态分配
 *
 * action: 使用 REBALANCE_IN_PROGRESS 响应心跳
 *         使用 REBALANCE_IN_PROGRESS 响应offset提交
 *         保留followers的同步组请求，直到状态过渡为 Stable 
 *         允许获取offset
 * transition: 将从leader接收到的状态分配同步到组 =&gt; Stable
 *             新成员或有更新元数据的现有成员加入组 =&gt; PreparingRebalance
 *             已有成员离开组 =&gt; PreparingRebalance
 *             检测到成员故障 =&gt; PreparingRebalance
 *             组被分区迁移所移除 =&gt; Dead
 */
private[group] case object CompletingRebalance extends GroupState {
  val validPreviousStates: Set[GroupState] = Set(PreparingRebalance)
}

/**
 * 重平衡完成，消费组处于稳定状态
 *
 * action: 正常响应成员的心跳请求
 *         将具备当前分配策略的任何成员同步到组
 *         将元数据与当前组元数据相匹配的 follower 加入进来
 *         允许当前代的offset提交
 *         允许获取offset
 * transition: 通过心跳检测到成员故障 =&gt; PreparingRebalance
 *             已有成员离开群组 =&gt; PreparingRebalance
 *             leader 收到加入组的请求 =&gt; PreparingRebalance
 *             使用新元数据的 follower 加入组 =&gt; PreparingRebalance
 *             组被分区迁移所移除 =&gt; Dead
 */
private[group] case object Stable extends GroupState {
  val validPreviousStates: Set[GroupState] = Set(CompletingRebalance)
}

/**
 * 组中没有成员，且其元数据正在被删除
 *
 * action: 使用 UNKNOWN_MEMBER_ID 响应加入组请求
 *         使用 UNKNOWN_MEMBER_ID 响应同步组请求
 *         使用 UNKNOWN_MEMBER_ID 响应心跳请求
 *         使用 UNKNOWN_MEMBER_ID 响应成员离开组请求
 *         使用 UNKNOWN_MEMBER_ID 响应offset 提交
 *         允许获取offset
 * transition: Dead 是清除元数据之前的最终状态，因此其没有可过渡的其余状态
 */
private[group] case object Dead extends GroupState {
  val validPreviousStates: Set[GroupState] = Set(Stable, PreparingRebalance, CompletingRebalance, Empty, Dead)
}

/**
  * 组中没有成员，但仍会存活直到所有的offset到期.
  * 此状态还表示仅将 Kafka 用于offset提交且没有成员的组
  *
  * action: 对于新成员加入组请求正常响应
  *          使用 UNKNOWN_MEMBER_ID 响应同步组请求
  *          使用 UNKNOWN_MEMBER_ID 响应心跳请求
  *          使用 UNKNOWN_MEMBER_ID 响应成员离开组请求
  *          使用 UNKNOWN_MEMBER_ID 响应offset 提交
  *          允许获取offset
  * transition: 在周期性过期任务中最后一个offsets被移除 =&gt; Dead
  *             新成员加入组 =&gt; PreparingRebalance
  *             组被分区迁移所移除 =&gt; Dead
  *             组到期被移除 =&gt; Dead
  */
private[group] case object Empty extends GroupState {
  val validPreviousStates: Set[GroupState] = Set(PreparingRebalance)
}

</code></pre>
<h3 id="重平衡过程">重平衡过程</h3>
<p>Kafka 重平衡期间主要分为如下几个阶段：</p>
<ol>
<li>寻找协调器（FIND_COORDINATOR）：消费者需要确定它所属的消费组对应的 GroupCoordinator 所在的 broker，并创建与该 broker 相互通信的网络连接。如果消费者已经保存了与消费组对应的 GroupCoordinator 节点的信息，并且与它之间的网络连接是正常的，那么就可以进入第二阶段。否则，就需要向集群中的某个节点发送 FindCoordinatorRequest 请求来查找对应的 GroupCoordinator，这里的“某个节点”并非是集群中的任意节点，而是负载最小的节点。</li>
<li>请求加入组（JOIN_GROUP）：在成功找到消费组所对应的 GroupCoordinator 之后就进入加入消费组的阶段，在此阶段的消费者会向 GroupCoordinator 发送 JoinGroupRequest 请求，并处理响应。如果消费组内还没有 leader，那么第一个加入消费组的消费者即为消费组的 leader。如果某一时刻 leader 消费者由于某些原因退出了消费组，那么会重新选举一个新的 leader。</li>
<li>同步组（SYNC_GROUP）：leader 消费者根据在第二阶段中选举出来的分区分配策略来实施具体的分区分配，在此之后需要将分配的方案同步给各个消费者，通过 GroupCoordinator 这个“中间人”来负责转发同步分配方案的。</li>
<li>发送心跳（HEARTBEAT）：进入这个阶段之后，消费组中的所有消费者就会处于正常工作状态。在正式消费之前，消费者还需要确定拉取消息的起始位置。假设之前已经将最后的消费位移提交到了 GroupCoordinator，并且 GroupCoordinator 将其保存到了 Kafka 内部的 __consumer_offsets 主题中，此时消费者可以通过 OffsetFetchRequest 请求获取上次提交的消费位移并从此处继续消费。消费者通过向 GroupCoordinator 发送心跳来维持它们与消费组的从属关系，以及它们对分区的所有权关系。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区中的消息。心跳线程是一个独立的线程，可以在轮询消息的空档发送心跳。如果消费者停止发送心跳的时间足够长，则整个会话就被判定为过期，GroupCoordinator 也会认为这个消费者已经死亡，就会触发一次再均衡行为。</li>
</ol>
<p>补充：关于选举分区分配策略：</p>
<ol>
<li>收集各个消费者支持的所有分配策略，组成候选集 candidates。</li>
<li>每个消费者从候选集 candidates 中找出第一个自身支持的策略，为这个策略投上一票。</li>
<li>计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。</li>
</ol>
<p>Kafka 重平衡期间主要流程图如下所示：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316009.png" alt="" loading="lazy"></figure>
<p>Kafka 服务端处理重平衡请求入口函数如下所示：</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaApis.scala
case ApiKeys.FIND_COORDINATOR =&gt; handleFindCoordinatorRequest(request)
case ApiKeys.JOIN_GROUP =&gt; handleJoinGroupRequest(request, requestLocal)
case ApiKeys.HEARTBEAT =&gt; handleHeartbeatRequest(request)
case ApiKeys.LEAVE_GROUP =&gt; handleLeaveGroupRequest(request)
case ApiKeys.SYNC_GROUP =&gt; handleSyncGroupRequest(request, requestLocal)

</code></pre>
<h4 id="寻找组协调器find_coordinator">寻找组协调器（FIND_COORDINATOR）</h4>
<p>在 Kafka 中，客户端的每个消费者都会对应一个消费者协调器（ConsumerCoordinator）， 而服务端的每个broker也会有个组协调器（GroupCoordinator）。</p>
<h5 id="消费者端的寻找协调器函数源码">消费者端的寻找协调器函数源码</h5>
<pre><code>// 文件地址：clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java
protected synchronized boolean ensureCoordinatorReady(final Timer timer) {
    // 如果已经找到了coordinator，则直接返回
    if (!coordinatorUnknown())
        return true;

    do {
        if (fatalFindCoordinatorException != null) {
            final RuntimeException fatalException = fatalFindCoordinatorException;
            fatalFindCoordinatorException = null;
            throw fatalException;
        }
        // 核心函数，发送请求向服务端(broker)获取coordinator
        final RequestFuture&lt;Void&gt; future = lookupCoordinator();
        client.poll(future, timer);

        if (!future.isDone()) {
            // ran out of time
            break;
        }

        RuntimeException fatalException = null;

        if (future.failed()) {
            // 引入重试机制
            if (future.isRetriable()) {
                log.debug(&quot;Coordinator discovery failed, refreshing metadata&quot;, future.exception());
                client.awaitMetadataUpdate(timer);
            } else {
                fatalException = future.exception();
                log.info(&quot;FindCoordinator request hit fatal exception&quot;, fatalException);
            }
        } else if (coordinator != null &amp;&amp; client.isUnavailable(coordinator)) {
            // we found the coordinator, but the connection has failed, so mark
            // it dead and backoff before retrying discovery
            // coordinator已找到，但连接断开，此时将其标记为dead
            markCoordinatorUnknown(&quot;coordinator unavailable&quot;);
            timer.sleep(rebalanceConfig.retryBackoffMs);
        }

        clearFindCoordinatorFuture();
        if (fatalException != null)
            throw fatalException;
    } while (coordinatorUnknown() &amp;&amp; timer.notExpired()); // 未超时且未获取到coordinator，便一直轮询

    return !coordinatorUnknown();
}

// 查找coordinator
protected synchronized RequestFuture&lt;Void&gt; lookupCoordinator() {
    if (findCoordinatorFuture == null) {
        // find a node to ask about the coordinator
        Node node = this.client.leastLoadedNode();
        if (node == null) {
            log.debug(&quot;No broker available to send FindCoordinator request&quot;);
            return RequestFuture.noBrokersAvailable();
        } else {
            // 发送请求至broker获取coordinator
            findCoordinatorFuture = sendFindCoordinatorRequest(node);
        }
    }
    return findCoordinatorFuture;
}
// 发送请求至broker获取消费组的当前coordinator，此函数会选择一台负载最小的broker构建请求。
private RequestFuture&lt;Void&gt; sendFindCoordinatorRequest(Node node) {
    // initiate the group metadata request
    log.debug(&quot;Sending FindCoordinator request to broker {}&quot;, node);
    FindCoordinatorRequestData data = new FindCoordinatorRequestData()
            .setKeyType(CoordinatorType.GROUP.id())
            .setKey(this.rebalanceConfig.groupId);
    FindCoordinatorRequest.Builder requestBuilder = new FindCoordinatorRequest.Builder(data);
    return client.send(node, requestBuilder)
            .compose(new FindCoordinatorResponseHandler());
}

</code></pre>
<h5 id="服务端的寻找协调器函数源码">服务端的寻找协调器函数源码</h5>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaApis.scala
case ApiKeys.FIND_COORDINATOR =&gt; handleFindCoordinatorRequest(request)

// 寻找coordinator的核心处理函数
private def getCoordinator(request: RequestChannel.Request, keyType: Byte, key: String): (Errors, Node) = {
  // 验证请求的合法性
  if (keyType == CoordinatorType.GROUP.id &amp;&amp;
      !authHelper.authorize(request.context, DESCRIBE, GROUP, key))
    (Errors.GROUP_AUTHORIZATION_FAILED, Node.noNode)
  else if (keyType == CoordinatorType.TRANSACTION.id &amp;&amp;
      !authHelper.authorize(request.context, DESCRIBE, TRANSACTIONAL_ID, key))
    (Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED, Node.noNode)
  else {
    // 获取分区及内部topic name
    val (partition, internalTopicName) = CoordinatorType.forId(keyType) match {
      case CoordinatorType.GROUP =&gt;
        // 根据消费组的名称，取hashcode，接着与kafka内部topic(__consumer_offsets)的分区个数取模，随后返回该分区所在的物理broker作为消费组的分组协调器
        // 文件地址：core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala
        // 核心函数：def partitionFor(groupId: String): Int = Utils.abs(groupId.hashCode) % groupMetadataTopicPartitionCount
        (groupCoordinator.partitionFor(key), GROUP_METADATA_TOPIC_NAME)

      case CoordinatorType.TRANSACTION =&gt;
        // 根据消费组的名称，取hashcode，接着与kafka内部topic(__consumer_offsets)的分区个数取模，随后返回该分区所在的物理broker作为消费组的分组协调器
        // 文件地址：core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala
        // 核心函数：def partitionFor(transactionalId: String): Int = Utils.abs(transactionalId.hashCode) % transactionTopicPartitionCount
        (txnCoordinator.partitionFor(key), TRANSACTION_STATE_TOPIC_NAME)
    }

    // 通过给定的topics及listener返回topic元数据
    val topicMetadata = metadataCache.getTopicMetadata(Set(internalTopicName), request.context.listenerName)

    if (topicMetadata.headOption.isEmpty) {
      val controllerMutationQuota = quotas.controllerMutation.newPermissiveQuotaFor(request)
      autoTopicCreationManager.createTopics(Seq(internalTopicName).toSet, controllerMutationQuota, None)
      (Errors.COORDINATOR_NOT_AVAILABLE, Node.noNode)
    } else {
      if (topicMetadata.head.errorCode != Errors.NONE.code) {
        (Errors.COORDINATOR_NOT_AVAILABLE, Node.noNode)
      } else {
        // 构建响应体  
        val coordinatorEndpoint = topicMetadata.head.partitions.asScala
          .find(_.partitionIndex == partition)
          .filter(_.leaderId != MetadataResponse.NO_LEADER_ID)
          .flatMap(metadata =&gt; metadataCache.
              getAliveBrokerNode(metadata.leaderId, request.context.listenerName))

        coordinatorEndpoint match {
          case Some(endpoint) =&gt;
            (Errors.NONE, endpoint)
          case _ =&gt;
            (Errors.COORDINATOR_NOT_AVAILABLE, Node.noNode)
        }
      }
    }
  }
}

</code></pre>
<h4 id="加入组join_group">加入组（JOIN_GROUP）</h4>
<p>消费者获取到coordinator之后，便准备加入组。</p>
<h5 id="消费者端的加入组函数源码">消费者端的加入组函数源码</h5>
<pre><code>// 文件地址：clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java
// 在加入组之前需执行如下函数
protected boolean onJoinPrepare(int generation, String memberId) {
    log.debug(&quot;Executing onJoinPrepare with generation {} and memberId {}&quot;, generation, memberId);
    boolean onJoinPrepareAsyncCommitCompleted = false;
    // 如果启用了自动提交(auto-commit)，那么在重平衡之前需要先提交offset
    RequestFuture&lt;Void&gt; future = maybeAutoCommitOffsetsAsync();
    // 返回如下结果：
    // 1. future 为空, 这意味着没有发送提交请求，所以它仍然被认为是完成的
    // 2. offset 提交完成
    // 3. offset 因不可重试异常而提交失败
    if (future == null)
        onJoinPrepareAsyncCommitCompleted = true;
    else if (future.succeeded())
        onJoinPrepareAsyncCommitCompleted = true;
    else if (future.failed() &amp;&amp; !future.isRetriable()) {
        log.error(&quot;Asynchronous auto-commit of offsets failed: {}&quot;, future.exception().getMessage());
        onJoinPrepareAsyncCommitCompleted = true;
    }


    // 当发生错误时，心跳线程可能会重新生成memberId;
    // 在这种情况下，之前所拥有的所有分区都将丢失，此时需要触发回调并清理分区分配。
    // 对应的，如果未发生错误，那么便可依据协议撤销分区分配并正常退出；
    // 在这种情况下，应该只在触发撤销回调之后再更改分配，以便用户仍然可以访问先前拥有的分区以提交偏移量等。
    Exception exception = null;
    final SortedSet&lt;TopicPartition&gt; revokedPartitions = new TreeSet&lt;&gt;(COMPARATOR);
    if (generation == Generation.NO_GENERATION.generationId ||
        memberId.equals(Generation.NO_GENERATION.memberId)) {
        revokedPartitions.addAll(subscriptions.assignedPartitions());

        if (!revokedPartitions.isEmpty()) {
            log.info(&quot;Giving away all assigned partitions as lost since generation/memberID has been reset,&quot; +
                &quot;indicating that consumer is in old state or no longer part of the group&quot;);
            exception = invokePartitionsLost(revokedPartitions);

            subscriptions.assignFromSubscribed(Collections.emptySet());
        }
    } else {
        switch (protocol) {
            case EAGER:
                // 撤销所有分区
                revokedPartitions.addAll(subscriptions.assignedPartitions());
                exception = invokePartitionsRevoked(revokedPartitions);

                subscriptions.assignFromSubscribed(Collections.emptySet());

                break;

            case COOPERATIVE:
                // 只撤销那些不再订阅的分区。此处也说明了CooperativeStickyAssignor分区分配器并不会终止消费。
                Set&lt;TopicPartition&gt; ownedPartitions = new HashSet&lt;&gt;(subscriptions.assignedPartitions());
                revokedPartitions.addAll(ownedPartitions.stream()
                    .filter(tp -&gt; !subscriptions.subscription().contains(tp.topic()))
                    .collect(Collectors.toSet()));

                if (!revokedPartitions.isEmpty()) {
                    exception = invokePartitionsRevoked(revokedPartitions);

                    ownedPartitions.removeAll(revokedPartitions);
                    subscriptions.assignFromSubscribed(ownedPartitions);
                }

                break;
        }
    }

    isLeader = false;
    subscriptions.resetGroupSubscription();

    if (exception != null) {
        throw new KafkaException(&quot;User rebalance callback throws an error&quot;, exception);
    }
    return onJoinPrepareAsyncCommitCompleted;
}

</code></pre>
<pre><code>// 文件地址：clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java
boolean ensureActiveGroup(final Timer timer) {
    // always ensure that the coordinator is ready because we may have been disconnected
    // when sending heartbeats and does not necessarily require us to rejoin the group.
    // 确保消费者已找到coordinator，对应第一步寻找协调器源码
    if (!ensureCoordinatorReady(timer)) {
        return false;
    }
     // 构建心跳线程: 当消费者加入到消费组后处于MemberState.STABLE后需要定时向协调器上报心跳，表示存活，否则将从消费组中移除。
    startHeartbeatThreadIfNeeded();
    // 加入群组
    return joinGroupIfNeeded(timer);
}

/**
 * 不启动心跳线程下加入组
 * 如果此函数返回true，则说明消费者状态稳定，且心跳已开启
 * 否则将存在如下几种状态：
 *  * UNJOINED: 在重试之前超时或者得到了错误的响应结果, 心跳关闭
 *  * PREPARING_REBALANCE: 超时前尚未收到加入组响应, 心跳关闭
 *  * COMPLETING_REBALANCE: 超时前尚未收到同步组响应, 心跳关闭
 *
 */
boolean joinGroupIfNeeded(final Timer timer) {
    while (rejoinNeededOrPending()) {
        if (!ensureCoordinatorReady(timer)) {
            return false;
        }
    
        // 依据flag判断是否需要调用onJoinPrepare函数。
        // 此判断块中有针对性的设置了flag，以防止在未完成的重平衡之前重复调用。
        // 这必须在循环的每次迭代中调用，因为需要重平衡的事件（例如订阅集改变所导致的元数据刷新）可能会在另一个重平衡期间发生。
        if (needsJoinPrepare) {
            // 需要在调用 onJoinPrepare 之前设置标志，因为用户回调可能会抛出异常，在这种情况下进行重试时不应该重复调用 onJoinPrepare。
            needsJoinPrepare = false;
            // 在onJoinPrepare 等待提交offset时，返回false
            if (!onJoinPrepare(generation.generationId, generation.memberId)) {
                needsJoinPrepare = true;
                //should not initiateJoinGroup if needsJoinPrepare still is true
                return false;
            }
        }

        // 开始进行加入组操作
        final RequestFuture&lt;ByteBuffer&gt; future = initiateJoinGroup();
        client.poll(future, timer);
        if (!future.isDone()) {
            // we ran out of time
            return false;
        }

        if (future.succeeded()) {
            Generation generationSnapshot;
            MemberState stateSnapshot;

            // 生成数据的同时可以会被心跳线程所清除
            // 对于 {@code onJoinComplete} 不能直接加同步锁, 因为其处理逻辑过长且不应该阻塞心跳线程
            // See {@link PlaintextConsumerTest#testMaxPollIntervalMsDelayInAssignment}
            synchronized (AbstractCoordinator.this) {
                generationSnapshot = this.generation;
                stateSnapshot = this.state;
            }

            if (!hasGenerationReset(generationSnapshot) &amp;&amp; stateSnapshot == MemberState.STABLE) {
                // 复制缓冲区以防 `onJoinComplete` 未完成并需要重试。
                ByteBuffer memberAssignment = future.value().duplicate();
                // 在加入组完成后执行相关的逻辑操作（一般为等待分配分区策略）
                onJoinComplete(generationSnapshot.generationId, generationSnapshot.memberId, generationSnapshot.protocolName, memberAssignment);

                // 一般来说，一旦future完成，便需要执行resetJoinGroupFuture，然而在此处只有完成回调返回后才能重置加入组future。
                // 这确保了如果回调被唤醒，在下一次 joinGroupIfNeeded 时将会进行重试操作。
                // 因此，有必要在其他条件下显式地触发 resetJoinGroupFuture。
                resetJoinGroupFuture();
                needsJoinPrepare = true;
            } else {
                final String reason = String.format(&quot;rebalance failed since the generation/state was &quot; +
                        &quot;modified by heartbeat thread to %s/%s before the rebalance callback triggered&quot;,
                        generationSnapshot, stateSnapshot);

                resetStateAndRejoin(reason, true);
                resetJoinGroupFuture();
            }
        } else {
            final RuntimeException exception = future.exception();
            // 异常时，重置加入组请求
            resetJoinGroupFuture();
            synchronized (AbstractCoordinator.this) {
                rejoinReason = String.format(&quot;rebalance failed due to '%s' (%s)&quot;, exception.getMessage(), exception.getClass().getSimpleName());
                rejoinNeeded = true;
            }

            if (exception instanceof UnknownMemberIdException ||
                exception instanceof IllegalGenerationException ||
                exception instanceof RebalanceInProgressException ||
                exception instanceof MemberIdRequiredException)
                continue;
            else if (!future.isRetriable())
                throw exception;

            timer.sleep(rebalanceConfig.retryBackoffMs);
        }
    }
    return true;
}

// 初始化加入组
private synchronized RequestFuture&lt;ByteBuffer&gt; initiateJoinGroup() {
    // 存储 joinFuture ，防止在重平衡完成之前重复的调用加入组请求
    if (joinFuture == null) {
        state = MemberState.PREPARING_REBALANCE;
        // 异常情况下连续发起加入组请求，并不会去更新重平衡开始时间
        if (lastRebalanceStartMs == -1L)
            lastRebalanceStartMs = time.milliseconds();
        // 发送加入组请求  
        joinFuture = sendJoinGroupRequest();
        joinFuture.addListener(new RequestFutureListener&lt;ByteBuffer&gt;() {
            @Override
            public void onSuccess(ByteBuffer value) {
                // do nothing since all the handler logic are in SyncGroupResponseHandler already
            }

            @Override
            public void onFailure(RuntimeException e) {
                // 失败处理：如果在唤醒之后加入请求完成，那么将会被忽略并重新发起加入请求
                // 这可以在加入或同步失败的情况下触发
                synchronized (AbstractCoordinator.this) {
                    sensors.failedRebalanceSensor.record();
                }
            }
        });
    }
    return joinFuture;
}

/**
 * 加入组并返回新一代的分区分配策略.简而言之此函数同时处理了加入组及同步组的操作。
 * 如果leader已被协调选举出来，那么将会委托给{@link #onLeaderElected(String, String, List, boolean)}函数进行相关逻辑处理
 *
 * @return A request future which wraps the assignment returned from the group leader
 */
RequestFuture&lt;ByteBuffer&gt; sendJoinGroupRequest() {
    if (coordinatorUnknown())
        return RequestFuture.coordinatorNotAvailable();

    // s给协调者发送加入组请求
    log.info(&quot;(Re-)joining group&quot;);
    // 构建请求参数
    JoinGroupRequest.Builder requestBuilder = new JoinGroupRequest.Builder(
            new JoinGroupRequestData()
                    .setGroupId(rebalanceConfig.groupId)
                    .setSessionTimeoutMs(this.rebalanceConfig.sessionTimeoutMs)
                     // memberId 消费组成员id,第一次为null，后续服务端会为该消费者分配一个唯一的id,构成为客户端id + uuid
                    .setMemberId(this.generation.memberId)
                    .setGroupInstanceId(this.rebalanceConfig.groupInstanceId.orElse(null))
                    .setProtocolType(protocolType())
                    .setProtocols(metadata())
                    .setRebalanceTimeoutMs(this.rebalanceConfig.rebalanceTimeoutMs)
                    .setReason(this.rejoinReason)
    );

    log.debug(&quot;Sending JoinGroup ({}) to coordinator {}&quot;, requestBuilder, this.coordinator);

    // 重平衡超时时间将会覆盖请求超时时间，因为这可能也是请求在协调器上阻塞的最大时间 
    // 针对部分延迟场景会额外再增加5s
    int joinGroupTimeoutMs = Math.max(
        client.defaultRequestTimeoutMs(),
        Math.max(
            rebalanceConfig.rebalanceTimeoutMs + JOIN_GROUP_TIMEOUT_LAPSE,
            rebalanceConfig.rebalanceTimeoutMs) // 此处防止溢出，因为重平衡超时时间可能是MAX_VALUE
        );
    // 接收到响应结果后，将会执行JoinGroupResponseHandler
    // 队列的负载算法是由Leader节点来分配，将分配结果通过向组协调器发送SYNC_GROUP请求，
    // 然后组协调器从Leader节点获取分配算法后，再返回给所有的消费者，从而开始进行消费。
    return client.send(coordinator, requestBuilder, joinGroupTimeoutMs)
            .compose(new JoinGroupResponseHandler(generation));
}

</code></pre>
<h5 id="服务端的加入组函数源码">服务端的加入组函数源码</h5>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaApis.scala
case ApiKeys.JOIN_GROUP =&gt; handleJoinGroupRequest(request, requestLocal)

def handleJoinGroupRequest(request: RequestChannel.Request, requestLocal: RequestLocal): Unit = {
  val joinGroupRequest = request.body[JoinGroupRequest]

  // 响应加入组请求
  def sendResponseCallback(joinResult: JoinGroupResult): Unit = {
    def createResponse(requestThrottleMs: Int): AbstractResponse = {
      val protocolName = if (request.context.apiVersion() &gt;= 7)
        joinResult.protocolName.orNull
      else
        joinResult.protocolName.getOrElse(GroupCoordinator.NoProtocol)
      // 构建响应体 
      val responseBody = new JoinGroupResponse(
        new JoinGroupResponseData()
          .setThrottleTimeMs(requestThrottleMs)
          .setErrorCode(joinResult.error.code)
          .setGenerationId(joinResult.generationId)
          .setProtocolType(joinResult.protocolType.orNull)
          .setProtocolName(protocolName)
          .setLeader(joinResult.leaderId)
          .setSkipAssignment(joinResult.skipAssignment)
          .setMemberId(joinResult.memberId)
          .setMembers(joinResult.members.asJava)
      )

      trace(&quot;Sending join group response %s for correlation id %d to client %s.&quot;
        .format(responseBody, request.header.correlationId, request.header.clientId))
      responseBody
    }
    requestHelper.sendResponseMaybeThrottle(request, createResponse)
  }

  if (joinGroupRequest.data.groupInstanceId != null &amp;&amp; config.interBrokerProtocolVersion &lt; KAFKA_2_3_IV0) {
    // 只有版本&gt;=2.3时才会启用静态membership；因为在确定broker都支持（静态 membership）之前，贸然使用此逻辑并不安全。
    // 例如：如果静态组被较老的协调器加载，那它将会丢弃group.instance.id字段，从而导致其意外的变为“动态”，进而出现错误状态。
    sendResponseCallback(JoinGroupResult(JoinGroupRequest.UNKNOWN_MEMBER_ID, Errors.UNSUPPORTED_VERSION))
  } else if (!authHelper.authorize(request.context, READ, GROUP, joinGroupRequest.data.groupId)) {
    sendResponseCallback(JoinGroupResult(JoinGroupRequest.UNKNOWN_MEMBER_ID, Errors.GROUP_AUTHORIZATION_FAILED))
  } else {
    // 得到 groupInstanceId 
    val groupInstanceId = Option(joinGroupRequest.data.groupInstanceId)

    // 如果 joinGroupRequest 版本 &gt;= 4 并且 groupInstanceId 配置为未知，则仅返回 MEMBER_ID_REQUIRED 错误。
    val requireKnownMemberId = joinGroupRequest.version &gt;= 4 &amp;&amp; groupInstanceId.isEmpty

    // 让协调器处理join group
    val protocols = joinGroupRequest.data.protocols.valuesList.asScala.map { protocol =&gt;
      (protocol.name, protocol.metadata)
    }.toList

    val supportSkippingAssignment = joinGroupRequest.version &gt;= 9
    // 实际由groupCoordinator进行逻辑处理
    groupCoordinator.handleJoinGroup(
      joinGroupRequest.data.groupId,
      joinGroupRequest.data.memberId,
      groupInstanceId,
      requireKnownMemberId,
      supportSkippingAssignment,
      request.header.clientId,
      request.context.clientAddress.toString,
      joinGroupRequest.data.rebalanceTimeoutMs,
      joinGroupRequest.data.sessionTimeoutMs,
      joinGroupRequest.data.protocolType,
      protocols,
      sendResponseCallback,
      Option(joinGroupRequest.data.reason),
      requestLocal)
  }
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala
def handleJoinGroup(groupId: String,
                    memberId: String,
                    groupInstanceId: Option[String],
                    requireKnownMemberId: Boolean,
                    supportSkippingAssignment: Boolean,
                    clientId: String,
                    clientHost: String,
                    rebalanceTimeoutMs: Int,
                    sessionTimeoutMs: Int,
                    protocolType: String,
                    protocols: List[(String, Array[Byte])],
                    responseCallback: JoinCallback,
                    reason: Option[String] = None,
                    requestLocal: RequestLocal = RequestLocal.NoCaching): Unit = {
  // 校验group状态                  
  validateGroupStatus(groupId, ApiKeys.JOIN_GROUP).foreach { error =&gt;
    responseCallback(JoinGroupResult(memberId, error))
    return
  }

  // 判断会话超时时间
  if (sessionTimeoutMs &lt; groupConfig.groupMinSessionTimeoutMs ||
    sessionTimeoutMs &gt; groupConfig.groupMaxSessionTimeoutMs) {
    responseCallback(JoinGroupResult(memberId, Errors.INVALID_SESSION_TIMEOUT))
  } else {
    // 是否为未知的memberId  
    val isUnknownMember = memberId == JoinGroupRequest.UNKNOWN_MEMBER_ID
    // 如果组不存在且memberId 是UNKNOW，那么就创建组；
    // 如果指定了memberId，但组不存在，那么请求将会被拒绝，并返回 UNKNOWN_MEMBER_ID
    groupManager.getOrMaybeCreateGroup(groupId, isUnknownMember) match {
      case None =&gt;
        responseCallback(JoinGroupResult(memberId, Errors.UNKNOWN_MEMBER_ID))
      case Some(group) =&gt;
        group.inLock {
          val joinReason = reason.getOrElse(&quot;not provided&quot;)
          if (!acceptJoiningMember(group, memberId)) {
            group.remove(memberId)
            responseCallback(JoinGroupResult(JoinGroupRequest.UNKNOWN_MEMBER_ID, Errors.GROUP_MAX_SIZE_REACHED))
          } else if (isUnknownMember) {
            // 创建组并加入
            doNewMemberJoinGroup(
              group,
              groupInstanceId,
              requireKnownMemberId,
              supportSkippingAssignment,
              clientId,
              clientHost,
              rebalanceTimeoutMs,
              sessionTimeoutMs,
              protocolType,
              protocols,
              responseCallback,
              requestLocal,
              joinReason
            )
          } else {
            // 直接加入当前组
            doCurrentMemberJoinGroup(
              group,
              memberId,
              groupInstanceId,
              clientId,
              clientHost,
              rebalanceTimeoutMs,
              sessionTimeoutMs,
              protocolType,
              protocols,
              responseCallback,
              joinReason
            )
          }

          // 开始执行完成加入组操作
          if (group.is(PreparingRebalance)) {
            rebalancePurgatory.checkAndComplete(GroupJoinKey(group.groupId))
          }
        }
    }
  }
}
// 创建组并加入
private def doNewMemberJoinGroup(
  group: GroupMetadata, // 消费组的元信息，并未持久化，存储在内存中，一个消费组当前消费者的信息。
  groupInstanceId: Option[String],
  requireKnownMemberId: Boolean,// 是否一定需要知道客户端id,如果客户端请求版本为4,在加入消费组时需要明确知道对方的memberId。
  supportSkippingAssignment: Boolean,
  clientId: String,// 客户端ID,消息组的memberId生成规则为 clientId + uuid
  clientHost: String,// 消费端端ip地址
  rebalanceTimeoutMs: Int,// 重平衡超时时间，取自消费端参数max.poll.interval.ms，默认为5分钟。
  sessionTimeoutMs: Int,// 会话超时时间，默认为10s
  protocolType: String,// 协议类型，默认为consumer
  protocols: List[(String, Array[Byte])],// 客户端支持的队列负载算法。
  responseCallback: JoinCallback,
  requestLocal: RequestLocal,
  reason: String
): Unit = {
  group.inLock {
    if (group.is(Dead)) {
      // 如果组状态被标记为为Dead，那这意味着有某个其他线程从协调器元数据中将该组删除了；
      // 该组很可能已迁移到其他协调器，或者该组处于瞬态不稳定阶段。
      // 此时就需要让消费者重新寻找协调器进行加入操作。
      responseCallback(JoinGroupResult(JoinGroupRequest.UNKNOWN_MEMBER_ID, Errors.COORDINATOR_NOT_AVAILABLE))
    } else if (!group.supportsProtocols(protocolType, MemberMetadata.plainProtocolSet(protocols))) {
      responseCallback(JoinGroupResult(JoinGroupRequest.UNKNOWN_MEMBER_ID, Errors.INCONSISTENT_GROUP_PROTOCOL))
    } else {
      val newMemberId = group.generateMemberId(clientId, groupInstanceId)
      groupInstanceId match {
        // 存在instanceId，说明支持静态member加入组操作  
        case Some(instanceId) =&gt;
          doStaticNewMemberJoinGroup(
            group,
            instanceId,
            newMemberId,
            clientId,
            clientHost,
            supportSkippingAssignment,
            rebalanceTimeoutMs,
            sessionTimeoutMs,
            protocolType,
            protocols,
            responseCallback,
            requestLocal,
            reason
          )
        // 动态加入  
        case None =&gt;
          doDynamicNewMemberJoinGroup(
            group,
            requireKnownMemberId,
            newMemberId,
            clientId,
            clientHost,
            rebalanceTimeoutMs,
            sessionTimeoutMs,
            protocolType,
            protocols,
            responseCallback,
            reason
          )
      }
    }
  }
}
// 动态加入
private def doDynamicNewMemberJoinGroup(
  group: GroupMetadata,// 消费组的元信息，并未持久化，存储在内存中，一个消费组当前消费者的信息。
  requireKnownMemberId: Boolean,// 是否一定需要知道客户端id,如果客户端请求版本为4,在加入消费组时需要明确知道对方的memberId。
  newMemberId: String,// 新的成员id
  clientId: String,// 客户端ID,消息组的memberId生成规则为 clientId + uuid
  clientHost: String,// 消费端端ip地址
  rebalanceTimeoutMs: Int,// 重平衡超时时间，取自消费端参数max.poll.interval.ms，默认为5分钟。
  sessionTimeoutMs: Int,// 会话超时时间，默认为10s
  protocolType: String,// 协议类型，默认为consumer
  protocols: List[(String, Array[Byte])],// 客户端支持的队列负载算法。
  responseCallback: JoinCallback,
  reason: String
): Unit = {
  if (requireKnownMemberId) {
    // 如果需要成员 ID，那么将在待处理成员列表中注册该成员，并返回响应以调用具有分配成员 ID 的另一个加入组请求。
    info(s&quot;Dynamic member with unknown member id joins group ${group.groupId} in &quot; +
      s&quot;${group.currentState} state. Created a new member id $newMemberId and request the &quot; +
      s&quot;member to rejoin with this id.&quot;)
    group.addPendingMember(newMemberId)
    addPendingMemberExpiration(group, newMemberId, sessionTimeoutMs)
    responseCallback(JoinGroupResult(newMemberId, Errors.MEMBER_ID_REQUIRED))
  } else {
    info(s&quot;Dynamic Member with unknown member id joins group ${group.groupId} in &quot; +
      s&quot;${group.currentState} state. Created a new member id $newMemberId for this member &quot; +
      s&quot;and add to the group.&quot;)
    // 加入组并进行重平衡操作  
    addMemberAndRebalance(rebalanceTimeoutMs, sessionTimeoutMs, newMemberId, None,
      clientId, clientHost, protocolType, protocols, group, responseCallback, reason)
  }
}

// 核心重平衡处理函数
private def addMemberAndRebalance(rebalanceTimeoutMs: Int,
                                  sessionTimeoutMs: Int,
                                  memberId: String,
                                  groupInstanceId: Option[String],
                                  clientId: String,
                                  clientHost: String,
                                  protocolType: String,
                                  protocols: List[(String, Array[Byte])],
                                  group: GroupMetadata,
                                  callback: JoinCallback,
                                  reason: String): Unit = {
  // 为每个消费者都创建一个MemberMetadata对象                           
  val member = new MemberMetadata(memberId, groupInstanceId, clientId, clientHost,
    rebalanceTimeoutMs, sessionTimeoutMs, protocolType, protocols)

  member.isNew = true

  // 如果组已经是PreparingRebalance状态，那么将newMemberAdded  标签设置为true，表示新成员可加入
  if (group.is(PreparingRebalance) &amp;&amp; group.generationId == 0)
    group.newMemberAdded = true

  // 加入组中
  group.add(member, callback)

  // 对于新成员的joinGroup操作设置超时时间，让其自己进行重试（如果新成员依旧存在的话），防止反复的重平衡操作导致大量成员失效
  completeAndScheduleNextExpiration(group, member, NewMemberJoinTimeoutMs)
  // 准备重平衡 
  maybePrepareRebalance(group, s&quot;Adding new member $memberId with group instance id $groupInstanceId; client reason: $reason&quot;)
}

private def maybePrepareRebalance(group: GroupMetadata, reason: String): Unit = {
  group.inLock {
    // 主要判断组状态是否为  PreparingRebalance
    if (group.canRebalance)
      prepareRebalance(group, reason)
  }
}

// 准备重平衡
private[group] def prepareRebalance(group: GroupMetadata, reason: String): Unit = {
  // 如果有任何成员正在等待同步，那便取消其请求并让他们重新加入
  if (group.is(CompletingRebalance))
    // 将所有消费者已按分配算法分配到的队列信息置空
    // 将空的分配结果返回给消费者，并且错误码为REBALANCE_IN_PROGRESS，客户端收到该错会重新加入消费组。
    resetAndPropagateAssignmentError(group, Errors.REBALANCE_IN_PROGRESS)

  // 如果同步到期了仍处理待办状态，那么便移除该组
  removeSyncExpiration(group)

  val delayedRebalance = if (group.is(Empty))
    // 如果组为空，则创建InitialDelayedJoin
    new InitialDelayedJoin(this,
      rebalancePurgatory,
      group,
      //即：group.initial.rebalance.delay.ms，用于设置消费组进入到PreparingRebalance真正执行其业务逻辑的延迟时间，其主要目的是等待更多的消费者进入。
      groupConfig.groupInitialRebalanceDelayMs,
      groupConfig.groupInitialRebalanceDelayMs,
      max(group.rebalanceTimeoutMs - groupConfig.groupInitialRebalanceDelayMs, 0))
  else
    // 否则创建DelayedJoin
    new DelayedJoin(this, group, group.rebalanceTimeoutMs)

  // 过渡组状态为PreparingRebalance
  group.transitionTo(PreparingRebalance)

  info(s&quot;Preparing to rebalance group ${group.groupId} in state ${group.currentState} with old generation &quot; +
    s&quot;${group.generationId} (${Topic.GROUP_METADATA_TOPIC_NAME}-${partitionFor(group.groupId)}) (reason: $reason)&quot;)

  // 尝试完成重平衡操作，如果未完成则创建watch
  val groupKey = GroupJoinKey(group.groupId)
  // rebalancePurgatory文件地址：core/src/main/scala/kafka/server/DelayedOperation.scala
  // 此函数最终仍然是调用了InitialDelayedJoin或者DelayedJoin的tryComplete方法。
  // InitialDelayedJoin或者DelayedJoin的文件地址：core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala
  rebalancePurgatory.tryCompleteElseWatch(delayedRebalance, Seq(groupKey))
}

// 最终实现函数：尝试完成重平衡
def tryCompleteJoin(group: GroupMetadata, forceComplete: () =&gt; Boolean): Boolean = {
  group.inLock {
    // 必须所有成员都已成功加入组才能返回true
    // def hasAllMembersJoined: Boolean = members.size == numMembersAwaitingJoin &amp;&amp; pendingMembers.isEmpty  
    if (group.hasAllMembersJoined)
      // 文件地址：core/src/main/scala/kafka/server/DelayedOperation.scala
      // 此函数成功将会调用onComplete()方法
      forceComplete()
    else false
  }
}
// 进入此函数说明重平衡完成，开始分发分区分配策略
def onCompleteJoin(group: GroupMetadata): Unit = {
  group.inLock {
    ...... 
    if (group.is(Dead)) {
      info(s&quot;Group ${group.groupId} is dead, skipping rebalance stage&quot;)
    } else if (!group.maybeElectNewJoinedLeader() &amp;&amp; group.allMembers.nonEmpty) {
      // If all members are not rejoining, we will postpone the completion
      // of rebalance preparing stage, and send out another delayed operation
      // until session timeout removes all the non-responsive members.
      error(s&quot;Group ${group.groupId} could not complete rebalance because no members rejoined&quot;)
      rebalancePurgatory.tryCompleteElseWatch(
        new DelayedJoin(this, group, group.rebalanceTimeoutMs),
        Seq(GroupJoinKey(group.groupId)))
    } else {
      // 当前的 generationId += 1，并将状态变更为transitionTo(CompletingRebalance)，member为空时则变更为transitionTo(Empty)
      group.initNextGeneration()
      if (group.is(Empty)) {
        info(s&quot;Group ${group.groupId} with generation ${group.generationId} is now empty &quot; +
          s&quot;(${Topic.GROUP_METADATA_TOPIC_NAME}-${partitionFor(group.groupId)})&quot;)

        groupManager.storeGroup(group, Map.empty, error =&gt; {
          if (error != Errors.NONE) {
            // we failed to write the empty group metadata. If the broker fails before another rebalance,
            // the previous generation written to the log will become active again (and most likely timeout).
            // This should be safe since there are no active members in an empty generation, so we just warn.
            warn(s&quot;Failed to write empty metadata for group ${group.groupId}: ${error.message}&quot;)
          }
        }, RequestLocal.NoCaching)
      } else {
        info(s&quot;Stabilized group ${group.groupId} generation ${group.generationId} &quot; +
          s&quot;(${Topic.GROUP_METADATA_TOPIC_NAME}-${partitionFor(group.groupId)}) with ${group.size} members&quot;)

        // 重平衡后触发所有成员的等待加入组响应回调
        for (member &lt;- group.allMemberMetadata) {
          val joinResult = JoinGroupResult(
            members = if (group.isLeader(member.memberId)) {
              group.currentMemberMetadata
            } else {
              List.empty
            },
            memberId = member.memberId,
            generationId = group.generationId,
            protocolType = group.protocolType,//分区分配策略
            protocolName = group.protocolName,//分区分配策略
            leaderId = group.leaderOrNull,
            skipAssignment = false,
            error = Errors.NONE)

          group.maybeInvokeJoinCallback(member, joinResult)
          completeAndScheduleNextHeartbeatExpiration(group, member)
          member.isNew = false

          group.addPendingSyncMember(member.memberId)
        }

        schedulePendingSync(group)
      }
    }
  }
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala
def add(member: MemberMetadata, callback: JoinCallback = null): Unit = {
  member.groupInstanceId.foreach { instanceId =&gt;
    if (staticMembers.contains(instanceId))
      throw new IllegalStateException(s&quot;Static member with groupInstanceId=$instanceId &quot; +
        s&quot;cannot be added to group $groupId since it is already a member&quot;)
    staticMembers.put(instanceId, member.memberId)
  }

  if (members.isEmpty)
    this.protocolType = Some(member.protocolType)

  assert(this.protocolType.orNull == member.protocolType)
  assert(supportsProtocols(member.protocolType, MemberMetadata.plainProtocolSet(member.supportedProtocols)))

  // 这里会触发一次消费组选主,选主逻辑：该消费组的第一个加入的消费者成为该消费组中的Leader
  // leader的主要职责如下：
  // 1.为每一个消费者创建一个DelayedHeartbeat对象，用于检测会话超时，组协调器如果检测会话超时，会将该消费者移除组，会重新触发重平衡，消费者为了避免被组协调器移除消费组，需要按间隔发送心跳包。
  // 2.根据当前消费组的状态是否需要进行重平衡。
  if (leaderId.isEmpty)
    leaderId = Some(member.memberId)

  members.put(member.memberId, member)
  incSupportedProtocols(member)
  member.awaitingJoinCallback = callback

  if (member.isAwaitingJoin)
    numMembersAwaitingJoin += 1

  pendingMembers.remove(member.memberId)
}

</code></pre>
<h4 id="同步组sync_group">同步组（SYNC_GROUP）</h4>
<p>消费者获得加入组响应结果后，将会发送同步组请求至服务端。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaApis.scala
case ApiKeys.SYNC_GROUP =&gt; handleSyncGroupRequest(request, requestLocal)

def handleSyncGroupRequest(request: RequestChannel.Request, requestLocal: RequestLocal): Unit = {
  val syncGroupRequest = request.body[SyncGroupRequest]

  def sendResponseCallback(syncGroupResult: SyncGroupResult): Unit = {
    requestHelper.sendResponseMaybeThrottle(request, requestThrottleMs =&gt;
      new SyncGroupResponse(
        new SyncGroupResponseData()
          .setErrorCode(syncGroupResult.error.code)
          .setProtocolType(syncGroupResult.protocolType.orNull)
          .setProtocolName(syncGroupResult.protocolName.orNull)
          .setAssignment(syncGroupResult.memberAssignment)
          .setThrottleTimeMs(requestThrottleMs)
      ))
  }

  if (syncGroupRequest.data.groupInstanceId != null &amp;&amp; config.interBrokerProtocolVersion &lt; KAFKA_2_3_IV0) {
    // 只有版本&gt;=2.3时才会启用静态membership；因为在确定broker都支持（静态 membership）之前，贸然使用此逻辑并不安全。
    // 例如：如果静态组被较老的协调器加载，那它将会丢弃group.instance.id字段，从而导致其意外的变为“动态”，进而出现错误状态。
    sendResponseCallback(SyncGroupResult(Errors.UNSUPPORTED_VERSION))
  } else if (!syncGroupRequest.areMandatoryProtocolTypeAndNamePresent()) {
    // 从版本 5 开始，ProtocolType 和 ProtocolName 字段是强制性的。
    sendResponseCallback(SyncGroupResult(Errors.INCONSISTENT_GROUP_PROTOCOL))
  } else if (!authHelper.authorize(request.context, READ, GROUP, syncGroupRequest.data.groupId)) {
    sendResponseCallback(SyncGroupResult(Errors.GROUP_AUTHORIZATION_FAILED))
  } else {
    val assignmentMap = immutable.Map.newBuilder[String, Array[Byte]]
    syncGroupRequest.data.assignments.forEach { assignment =&gt;
      assignmentMap += (assignment.memberId -&gt; assignment.assignment)
    }

    // 核心处理同步组函数
    groupCoordinator.handleSyncGroup(
      syncGroupRequest.data.groupId,
      syncGroupRequest.data.generationId,
      syncGroupRequest.data.memberId,
      Option(syncGroupRequest.data.protocolType),
      Option(syncGroupRequest.data.protocolName),
      Option(syncGroupRequest.data.groupInstanceId),
      assignmentMap.result(),
      sendResponseCallback,
      requestLocal
    )
  }
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala
def handleSyncGroup(groupId: String,
                    generation: Int,
                    memberId: String,
                    protocolType: Option[String],
                    protocolName: Option[String],
                    groupInstanceId: Option[String],
                    groupAssignment: Map[String, Array[Byte]],
                    responseCallback: SyncCallback,
                    requestLocal: RequestLocal = RequestLocal.NoCaching): Unit = {
  validateGroupStatus(groupId, ApiKeys.SYNC_GROUP) match {
    case Some(error) if error == Errors.COORDINATOR_LOAD_IN_PROGRESS =&gt;
      // 协调器正在加载，这意味着服务端已经失去了活跃重平衡的状态，并且该组将需要从 JoinGroup 重新开始。 
      // 通过返回正在进行的重平衡，消费者将尝试重新加入，而无需重新发现协调器。 
      // 请注意，不能直接返回 COORDINATOR_LOAD_IN_PROGRESS，因为旧客户端不希望出现错误。
      responseCallback(SyncGroupResult(Errors.REBALANCE_IN_PROGRESS))

    case Some(error) =&gt; responseCallback(SyncGroupResult(error))

    case None =&gt;
      groupManager.getGroup(groupId) match {
        // 不存在，直接返回错误  
        case None =&gt; responseCallback(SyncGroupResult(Errors.UNKNOWN_MEMBER_ID))
        // 执行同步组操作
        case Some(group) =&gt; doSyncGroup(group, generation, memberId, protocolType, protocolName,
          groupInstanceId, groupAssignment, requestLocal, responseCallback)
      }
  }
}
// 执行同步组操作，主要是将分区分配策略发放下去
private def doSyncGroup(group: GroupMetadata,
                        generationId: Int,
                        memberId: String,
                        protocolType: Option[String],
                        protocolName: Option[String],
                        groupInstanceId: Option[String],
                        groupAssignment: Map[String, Array[Byte]],
                        requestLocal: RequestLocal,
                        responseCallback: SyncCallback): Unit = {
  group.inLock {
    val validationErrorOpt = validateSyncGroup(
      group,
      generationId,
      memberId,
      protocolType,
      protocolName,
      groupInstanceId
    )

    validationErrorOpt match {
      case Some(error) =&gt; responseCallback(SyncGroupResult(error))

      case None =&gt; group.currentState match {
        // 组状态为Empty ，那么返回 UNKNOWN_MEMBER_ID
        case Empty =&gt;
          responseCallback(SyncGroupResult(Errors.UNKNOWN_MEMBER_ID))
      
        // 组状态为PreparingRebalance ，那么返回 REBALANCE_IN_PROGRESS
        case PreparingRebalance =&gt;
          responseCallback(SyncGroupResult(Errors.REBALANCE_IN_PROGRESS))
      
        // 组状态为CompletingRebalance 
        case CompletingRebalance =&gt;
          group.get(memberId).awaitingSyncCallback = responseCallback
          removePendingSyncMember(group, memberId)

          // 如果这是leader，将尝试保持状态并转为到 Stable。（此处对应消费者处理joinGroup请求响应结果）
          if (group.isLeader(memberId)) {
            info(s&quot;Assignment received from leader $memberId for group ${group.groupId} for generation ${group.generationId}. &quot; +
              s&quot;The group has ${group.size} members, ${group.allStaticMembers.size} of which are static.&quot;)

            // fill any missing members with an empty assignment
            val missing = group.allMembers.diff(groupAssignment.keySet)
            val assignment = groupAssignment ++ missing.map(_ -&gt; Array.empty[Byte]).toMap

            if (missing.nonEmpty) {
              warn(s&quot;Setting empty assignments for members $missing of ${group.groupId} for generation ${group.generationId}&quot;)
            }

            groupManager.storeGroup(group, assignment, (error: Errors) =&gt; {
              group.inLock {
                // 另一个成员可能在等待此回调时加入了该组，因此必须确保组状态仍处于 CompletingRebalance 并且在它被调用时处于同一代。 
                // 如果已经转换到另一个状态，那么什么都不做
                if (group.is(CompletingRebalance) &amp;&amp; generationId == group.generationId) {
                  if (error != Errors.NONE) {
                    resetAndPropagateAssignmentError(group, error)
                    maybePrepareRebalance(group, s&quot;Error when storing group assignment during SyncGroup (member: $memberId)&quot;)
                  } else {
                    setAndPropagateAssignment(group, assignment)
                    group.transitionTo(Stable)
                  }
                }
              }
            }, requestLocal)
            groupCompletedRebalanceSensor.record()
          }

        // 组状态为Stable 直接返回分区分配策略
        case Stable =&gt;
          removePendingSyncMember(group, memberId)

          val memberMetadata = group.get(memberId)
          responseCallback(SyncGroupResult(group.protocolType, group.protocolName, memberMetadata.assignment, Errors.NONE))
          completeAndScheduleNextHeartbeatExpiration(group, group.get(memberId))

        // 组状态为Dead 将抛出异常
        case Dead =&gt;
          throw new IllegalStateException(s&quot;Reached unexpected condition for Dead group ${group.groupId}&quot;)
      }
    }
  }
}

</code></pre>
<h4 id="发送心跳heartbeat">发送心跳（HEARTBEAT）</h4>
<p>消费者协调器会引入心跳机制，即定时向组协调器发送心跳包，在指定时间内未收到客户端的心跳包，表示会话过期，过期时间通过参数session.timeout.ms设置，默认为10s，心跳间隔参数heartbeat.interval.ms，默认3s。</p>
<h5 id="消费者端启动心跳线程">消费者端启动心跳线程</h5>
<pre><code>// 文件地址：clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java
// 此段代码在加入组请求时触发执行
boolean ensureActiveGroup(final Timer timer) {
    // always ensure that the coordinator is ready because we may have been disconnected
    // when sending heartbeats and does not necessarily require us to rejoin the group.
    if (!ensureCoordinatorReady(timer)) {
        return false;
    }
    // 构建心跳线程: 当消费者加入到消费组后处于MemberState.STABLE后需要定时向协调器上报心跳，表示存活，否则将从消费组中移除。
    startHeartbeatThreadIfNeeded();
    return joinGroupIfNeeded(timer);
}

</code></pre>
<pre><code>// 文件地址：clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java
// HeartbeatThread 线程执行函数核心逻辑处理
// 如果协调器未知那么便寻找协调器
if (coordinatorUnknown()) {
    if (findCoordinatorFuture != null) {
        // clear the future so that after the backoff, if the hb still sees coordinator unknown in
        // the next iteration it will try to re-discover the coordinator in case the main thread cannot
        clearFindCoordinatorFuture();

        // backoff properly
        AbstractCoordinator.this.wait(rebalanceConfig.retryBackoffMs);
    } else {
        lookupCoordinator();
    }
} else if (heartbeat.sessionTimeoutExpired(now)) {
    // session超时时间内未受到心跳，此时标记协调器不可知，开始重新寻找协调器
    markCoordinatorUnknown(&quot;session timed out without receiving a &quot;
            + &quot;heartbeat response&quot;);
} else if (heartbeat.pollTimeoutExpired(now)) {
    // 如果此次心跳发送时间距离上一次心跳发送时间超过了pollTimeout，客户端将发送LEAVE_GROUP，离开消费组，
    // 并在下一个poll方法调用时重新进入加入消费组的操作，会再次触发重平衡。
    log.warn(&quot;consumer poll timeout has expired. This means the time between subsequent calls to poll() &quot; +
        &quot;was longer than the configured max.poll.interval.ms, which typically implies that &quot; +
        &quot;the poll loop is spending too much time processing messages. You can address this &quot; +
        &quot;either by increasing max.poll.interval.ms or by reducing the maximum size of batches &quot; +
        &quot;returned in poll() with max.poll.records.&quot;);

    maybeLeaveGroup(&quot;consumer poll timeout has expired.&quot;);
} else if (!heartbeat.shouldHeartbeat(now)) {
    // 等待重试延迟后再次轮询，以防心跳失败或协调器断开连接
    AbstractCoordinator.this.wait(rebalanceConfig.retryBackoffMs);
} else {
    // 正常发送心跳
    heartbeat.sentHeartbeat(now);
    final RequestFuture&lt;Void&gt; heartbeatFuture = sendHeartbeatRequest();
    heartbeatFuture.addListener(new RequestFutureListener&lt;Void&gt;() {
        @Override
        public void onSuccess(Void value) {
            synchronized (AbstractCoordinator.this) {
                heartbeat.receiveHeartbeat();
            }
        }

        @Override
        public void onFailure(RuntimeException e) {
            synchronized (AbstractCoordinator.this) {
                if (e instanceof RebalanceInProgressException) {
                    // 在组重平衡时继续心跳是有效的。 
                    // 这确保了协调器将成员保留在组中的时间与重平衡超时的持续时间一样长。
                    // 然而，如果此时停止发送心跳，那么会话超时可能会在消费者重新加入之前到期。
                    heartbeat.receiveHeartbeat();
                } else if (e instanceof FencedInstanceIdException) {
                    log.error(&quot;Caught fenced group.instance.id {} error in heartbeat thread&quot;, rebalanceConfig.groupInstanceId);
                    heartbeatThread.failed.set(e);
                } else {
                    heartbeat.failHeartbeat();
                    // wake up the thread if it's sleeping to reschedule the heartbeat
                    AbstractCoordinator.this.notify();
                }
            }
        }
    });
}

</code></pre>
<h5 id="服务端处理心跳响应">服务端处理心跳响应</h5>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaApis.scala
case ApiKeys.HEARTBEAT =&gt; handleHeartbeatRequest(request)

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala
// 此函数整体比较简单，根据组的状态返回给客户端不同的心跳。
def handleHeartbeat(groupId: String,
                    memberId: String,
                    groupInstanceId: Option[String],
                    generationId: Int,
                    responseCallback: Errors =&gt; Unit): Unit = {
  validateGroupStatus(groupId, ApiKeys.HEARTBEAT).foreach { error =&gt;
    if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS)
      // the group is still loading, so respond just blindly
      responseCallback(Errors.NONE)
    else
      responseCallback(error)
    return
  }

  val err = groupManager.getGroup(groupId) match {
    case None =&gt;
      Errors.UNKNOWN_MEMBER_ID

    case Some(group) =&gt; group.inLock {
      val validationErrorOpt = validateHeartbeat(
        group,
        generationId,
        memberId,
        groupInstanceId
      )

      if (validationErrorOpt.isDefined) {
        validationErrorOpt.get
      } else {
        group.currentState match {
          case Empty =&gt;
            Errors.UNKNOWN_MEMBER_ID

          case CompletingRebalance =&gt;
            // consumers may start sending heartbeat after join-group response, in which case
            // we should treat them as normal hb request and reset the timer
            val member = group.get(memberId)
            completeAndScheduleNextHeartbeatExpiration(group, member)
            Errors.NONE

          case PreparingRebalance =&gt;
              val member = group.get(memberId)
              completeAndScheduleNextHeartbeatExpiration(group, member)
              Errors.REBALANCE_IN_PROGRESS

          case Stable =&gt;
              val member = group.get(memberId)
              completeAndScheduleNextHeartbeatExpiration(group, member)
              Errors.NONE

          case Dead =&gt;
            throw new IllegalStateException(s&quot;Reached unexpected condition for Dead group $groupId&quot;)
        }
      }
    }
  }
  responseCallback(err)
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之消费者分配器]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-xiao-fei-zhe-fen-pei-qi/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-xiao-fei-zhe-fen-pei-qi/">
        </link>
        <updated>2022-07-23T08:00:58.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka 消费者，消费组以及分区分配器。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/forest-3287976_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="消费者">消费者</h2>
<h3 id="简介">简介</h3>
<p>kafka consumer 订阅感兴趣的主题，同时向其所在的主分区发送fetch请求，获取需要进行消费的消息；需要注意的是，此过程中，consumer的每个请求都需要在partition中指定offset，从而消费从offset处开始的message。因此，consumer对offset的控制便尤为重要，可以依此来进行回退重消费操作。</p>
<h3 id="消费组">消费组</h3>
<p>kafka中，每个consumer都会对应一个消费组（Consumer Group）；不同消费组可以消费同一个Topic中同一个分区的消息，但在一个消费组下，一个Topic中的一个或多个分区消息只能被一个消费者所消费。也就是说一个分区如果已跟一个消费者匹配，那么便不能再跟同一个消费组中的消费者继续匹配。</p>
<p>在kafka集群中，一般会以GroupId（字符串）区分不同的消费组。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316006.png" alt="img" loading="lazy"></figure>
<p>kafka中，消费者跟topic中的分区匹配模式，可以提供横向扩展功能，例如额外增加消费者来提高消费速率；不过需要注意的是，一味地增加消费者并不会带来高额收益。因为当分区数固定的前提下，如果消费组中的消费者大于分区数将会导致部分消费者处于空闲状态。如下图所示，C7便处于空闲状态。</p>
<p>当然，这种情况是可以通过继承kafka的AbstractPartitionAssignor类（或者直接实现ConsumerPartitionAssignor接口）来规避的，只要将其中的分配逻辑修改为同一消费组中的消费者可以任意消费topic的所有分区即可。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316007.png" alt="img" loading="lazy"></figure>
<h3 id="分区分配器">分区分配器</h3>
<p>分配器（Assignor）是ConsumerPartitionAssignor接口（也可直接继承AbstractPartitionAssignor类）的实现类。leader（消费者之一）使用分配器来根据消费者的订阅将分区分配给对应的Consumer实例。</p>
<p>分配器是消费者逻辑处理的一部分，这种设计的好处是可以动态的更改分配算法，而无需修改kafka broker。</p>
<p>assignor 获取有关消费者订阅的数据并为每个消费者返回具体的分区，具体流程如下：</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316008.png" alt="img" loading="lazy"></figure>
<p>kafka（2.4版本及之后）拥有四种可直接使用的分配器（可以直接通过属性 partition.assignment.strategy 来进行分配器配置）：</p>
<ul>
<li>RangeAssignor：<a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/RangeAssignor.html">https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/RangeAssignor.html</a></li>
<li>RoundRobinAssignor：<a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/RoundRobinAssignor.html">https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/RoundRobinAssignor.html</a></li>
<li>StickyAssignor：<a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/StickyAssignor.html">https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/StickyAssignor.html</a></li>
<li>CooperativeStickyAssignor：<a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/CooperativeStickyAssignor.html">https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/CooperativeStickyAssignor.html</a></li>
</ul>
<pre><code>// 简单示例
Properties props = new Properties();
props.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, StickyAssignor.class.getName());
KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);

</code></pre>
<p>同时，需要注意的是：属于同一ConsumerGroup的所有消费者必须声明一个共同的分配器。如果一个consumer尝试加入一个group，且其分配器与组内的其他成员不一致，那么将会出现如下异常：</p>
<pre><code>org.apache.kafka.common.errors.InconsistentGroupProtocolException: The group member’s supported protocols are incompatible with those of existing members or first group member tried to join with empty protocol type or empty protocol list.

</code></pre>
<p>此属性（partition.assignment.strategy）接受以逗号分隔的分配器列表。例如，它允许使用者通过指定新分配器来更新一组消费者，同时暂时保留前一个分配器，作为再平衡协议的一部分，broker协调者将选择所有成员都支持的协议。</p>
<p>kafka本身使用的默认分配器为：RangeAssignor及CooperativeStickyAssignor。</p>
<pre><code>// 文件地址：org.apache.kafka.clients.consumer.ConsumerConfig
.define(PARTITION_ASSIGNMENT_STRATEGY_CONFIG,
        Type.LIST,
        Arrays.asList(RangeAssignor.class, CooperativeStickyAssignor.class),
        new ConfigDef.NonNullValidator(),
        Importance.MEDIUM,
        PARTITION_ASSIGNMENT_STRATEGY_DOC)

</code></pre>
<h4 id="rangeassignor">RangeAssignor</h4>
<p>RangeAssignor 是基于每个topic进行工作的。针对每个Topic，kafka会依据数字顺序将可用分区进行排序(p0,p1,p2...)，同时按照字典顺序排序消费者(c0,c1,c2...)。之后，将分区数除以消费者总数，以确定分配给每个消费者的分区数。如果无法整除，那么额外的分区将会分配给排序靠前的消费者。</p>
<p>数学模型：设主题分区数为x，消费者数为y，设分区数除以消费者数的商为n、余数为m。那么在一个消费组中，前m个消费者将分配到n+1个分区，剩余的消费者分配n个分区。</p>
<p>例如：存在两个消费者C0，C1，两个主题t0，t1，且每个主题拥有三个分区，那么分区结果为：t0p0，t0p1，t0p2，t1p0，t1p1，t1p2。最终分配结果如下所示：</p>
<pre><code>// 依据上述数学模型，此处x=3，y=2，算出n=1，m=1，最终结果如下所示：
C0: [t0p0, t0p1, t1p0, t1p1]
C1: [t0p2, t1p2]

</code></pre>
<p>使用这种分配器的优势在于每个消费者都可以为每个主题获得一个分区，这对于某些负载均衡场景很有利。当然劣势也很明显，比如当遇到某个消费组订阅了多个Topic，那么可能会导致前几个消费者负载过重；又或者消费者数多于分区数时，将会有部分消费者处于空闲状态；且针对消费者进行扩缩容时，分配可能会发生较大的变化。</p>
<pre><code>// 源码内容如下
public class RangeAssignor extends AbstractPartitionAssignor {
    public static final String RANGE_ASSIGNOR_NAME = &quot;range&quot;;

    @Override
    public String name() {
        return RANGE_ASSIGNOR_NAME;
    }

    private Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic(Map&lt;String, Subscription&gt; consumerMetadata) {
        Map&lt;String, List&lt;MemberInfo&gt;&gt; topicToConsumers = new HashMap&lt;&gt;();
        for (Map.Entry&lt;String, Subscription&gt; subscriptionEntry : consumerMetadata.entrySet()) {
            String consumerId = subscriptionEntry.getKey();
            MemberInfo memberInfo = new MemberInfo(consumerId, subscriptionEntry.getValue().groupInstanceId());
            for (String topic : subscriptionEntry.getValue().topics()) {
                // 以topic为key，为每个topic归总所有的消费者信息
                put(topicToConsumers, topic, memberInfo);
            }
        }
        return topicToConsumers;
    }

    @Override
    public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,
                                                    Map&lt;String, Subscription&gt; subscriptions) {
        // 获取每个topic的消费者信息，形如：t0:[c0,c1], t1:[c0,c1]                                         
        Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions);

        // 将主题分区与订阅该主题的消费者绑定
        Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;();
        for (String memberId : subscriptions.keySet())
            // memberId 与消费者关联
            assignment.put(memberId, new ArrayList&lt;&gt;());

        // 对每个主题分区及消费者信息组成的map进行遍历，由此可以看出 RangeAssignor 分配器是针对每个主题进行分配操作的。
        for (Map.Entry&lt;String, List&lt;MemberInfo&gt;&gt; topicEntry : consumersPerTopic.entrySet()) {
            String topic = topicEntry.getKey();
            List&lt;MemberInfo&gt; consumersForTopic = topicEntry.getValue();

            // 获取当前topic的分区数
            Integer numPartitionsForTopic = partitionsPerTopic.get(topic);
            if (numPartitionsForTopic == null)
                continue;

            // 将消费者按字典排序 
            Collections.sort(consumersForTopic);
            // 主题分区数除以消费者数得到分配给每个消费者的分区数
            int numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();
            // 主题分区数对消费者数取余得到额外的消费者分区数
            int consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();

            // 将主题分区汇总为一个列表 
            List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);
            // 此处逻辑可参考上述数学模型
            for (int i = 0, n = consumersForTopic.size(); i &lt; n; i++) {
                // 截取主题分区列表的起始位置
                int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);
                // 截取主题分区列表的长度，如果有consumersWithExtraPartition不为0，那么排序靠前的消费者将会额外分配一个分区
                int length = numPartitionsPerConsumer + (i + 1 &gt; consumersWithExtraPartition ? 0 : 1);
                // 给消费者分配分区数
                assignment.get(consumersForTopic.get(i).memberId).addAll(partitions.subList(start, start + length));
            }
        }
        return assignment;
    }
}

</code></pre>
<h4 id="roundrobinassignor">RoundRobinAssignor</h4>
<p>RoundRobinAssignor 会将消费组内的所有消费者及其订阅的所有主题的分区按照字典序排序，然后通过轮询方式逐个将分区依次分配给每个消费者。步骤如下：</p>
<ol>
<li>消费者按照字典排序，例如C0, C1, C2... ...，并构造环形迭代器。</li>
<li>topic名称按照字典排序，并得到每个topic的所有分区，从而得到所有分区集合。</li>
<li>遍历第2步所有分区集合，同时轮询消费者。</li>
<li>如果轮询到的消费者订阅的topic不包括当前遍历的分区所属topic，则跳过；否则分配给当前消费者，并继续第3步。</li>
</ol>
<p>按照上述步骤来说，当同一个消费组内所有的消费者的订阅信息都是相同的，那么使用 RoundRobinAssignor 分配器进行分配操作时，结果会相对均匀。</p>
<p>但当同一个消费组内的消费者订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能导致分区分配得不均匀。</p>
<p>例1：存在两个消费者C0，C1，两个主题t0，t1，且每个主题拥有三个分区，那么分区结果为：t0p0，t0p1，t0p2，t1p0，t1p1，t1p2。最终分配结果如下所示：</p>
<pre><code>C0: [t0p0, t0p2, t1p1]
C1: [t0p1, t1p0, t1p2]
</code></pre>
<p>例2：存在3个消费者C0、C1 和 C2，三个主题t0、t1、t2，且t0分区数为1，t1分区数为2，t2分区数为3，那么分区结果为：t0p0, t1p0，t1p1, t2p0, t2p1, t2p2。最终分配结果将如下所示：</p>
<pre><code>C0: [t0p0]
C1: [t1p0]
C2: [t1p1, t2p0, t2p1, t2p2]
</code></pre>
<p>这种分配器最大化的利用了消费者的数量优势，某种情况下可以达到统一分配的效果；但这种分配器也有一个主要的缺点，那就是在消费者数量发生变化时（重平衡），会导致分区也重分配。这可能会由于分区更改其所有者而导致额外的处理（额外的性能影响）。</p>
<pre><code>public class RoundRobinAssignor extends AbstractPartitionAssignor {

    @Override
    public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,
                                                    Map&lt;String, Subscription&gt; subscriptions) {
        Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;();
        List&lt;MemberInfo&gt; memberInfoList = new ArrayList&lt;&gt;();
        // 取到所有的消费者信息
        for (Map.Entry&lt;String, Subscription&gt; memberSubscription : subscriptions.entrySet()) {
            assignment.put(memberSubscription.getKey(), new ArrayList&lt;&gt;());
            memberInfoList.add(new MemberInfo(memberSubscription.getKey(),
                                              memberSubscription.getValue().groupInstanceId()));
        }

        // 排序消费者列表，同时将其转化为环形迭代器（主要用于循环取值）
        CircularIterator&lt;MemberInfo&gt; assigner = new CircularIterator&lt;&gt;(Utils.sorted(memberInfoList));

        // 遍历排序后的分区列表
        for (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) {
            final String topic = partition.topic();
            // 遍历环形迭代器获取消费者信息，并判断其是否订阅相关的topic，如果没有则跳过，否则分配分区给该消费者
            while (!subscriptions.get(assigner.peek().memberId).topics().contains(topic))
                assigner.next();
            // 分配分区给该消费者
            assignment.get(assigner.next().memberId).add(partition);
        }
        return assignment;
    }

    // 将所有的分区汇总并进行排序操作
    private List&lt;TopicPartition&gt; allPartitionsSorted(Map&lt;String, Integer&gt; partitionsPerTopic,
                                                     Map&lt;String, Subscription&gt; subscriptions) {
        SortedSet&lt;String&gt; topics = new TreeSet&lt;&gt;();
        for (Subscription subscription : subscriptions.values())
            topics.addAll(subscription.topics());

        List&lt;TopicPartition&gt; allPartitions = new ArrayList&lt;&gt;();
        for (String topic : topics) {
            Integer numPartitionsForTopic = partitionsPerTopic.get(topic);
            if (numPartitionsForTopic != null)
                // 分区中包含topic信息，用于匹配对应的消费者
                allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic));
        }
        return allPartitions;
    }

    @Override
    public String name() {
        return &quot;roundrobin&quot;;
    }

}

</code></pre>
<h4 id="stickyassignor">StickyAssignor</h4>
<p>StickyAssignor 分配器除了让分区分配得更加均匀外，在重平衡期间还会尽可能的减少分区移动（如果两者发生冲突，第一种特性优先于第二种特性）。相对于RoundRobinAssignor 重平衡期间需要重分配分区的操作而言，此特性减少了资源消耗，提升了消费者性能。</p>
<p>相对于前两种分配器，StickyAssignor 实现更为复杂，简单例子如下：</p>
<p>例1：订阅的主题拥有相同分区的场景：假设有3个Consumer（C0、C1和C2），他们都订阅了4个主题（t0、t1、t2、t3），每个主题有2个分区。也就是说，整个消费者组订阅了 t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1 这 8 个 分区。最终分发结果如下：</p>
<pre><code>C0：t0p0、t1p1、t3p0
C1：t0p1、t2p0、t3p1
C2：t1p0、t2p1

</code></pre>
<p>乍一看，似乎与采用 RoundRobinAssignor 分配策略分配的结果是一样的，但真的是这样吗？我们假设消费者 C1 退出了消费者组，那么消费者组会执行再平衡操作，再分配消费区。如果是 RoundRobinAssignor 分配策略，那么分配结果如下：</p>
<pre><code>// 分区发生了移动
C0：t0p0、t1p0、t2p0、t3p0
C2：t0p1、t1p1、t2p1、t3p1
</code></pre>
<p>正如分配结果所示，RoundRobinAssignor 的分配策略将基于消费者 C0 和 C2 重新轮询分配。如果你使用的是 StickyAssignor 分配策略，那么分配结果是：</p>
<pre><code>// 上次分区结果将会被保留，以尽量减少分区移动
C0：t0p0、t1p1、t3p0、t2p0
C2：t1p0、t2p1、t0p1、t3p1
</code></pre>
<p>例2：订阅的主题拥有不同分区的场景：同样有3个Consumer（C0、C1和C2），集群中有3个主题（t0、t1和t2），这3个主题分别有1、2、3个分区。也就是说，集群中有 t0p0、t1p0、t1p1、t2p0、t2p1、t2p2 这 6 个 分区。消费者 C0 订阅了主题 t0，消费者 C1 订阅了主题 t0 和 t1，消费者 C2 订阅了主题 t0、t1 和 t2。RoundRobinAssignor 分配器结果如下：</p>
<pre><code>// 显然结果不并不是最优解
C0：t0p0
C1：t1p0
C2：t1p1、t2p0、t2p1、t2p2

</code></pre>
<p>StickyAssignor 分配器结果如下：</p>
<pre><code>// 可以看出此种分配方式更充分地利用了消费者。
C0：t0p0
C1：t1p0、t1p1
C2：t2p0、t2p1、t2p2

</code></pre>
<p>此时，如果C0从消费组中移除，那么依照 RoundRobinAssignor 分配器的重平衡结果如下：</p>
<pre><code>// 分区重新开始轮询分配
C1：t0p0、t1p1
C2：t1p0、t2p0、t2p1、t2p2

</code></pre>
<p>StickyAssignor 分配器的重平衡结果如下：</p>
<pre><code>// 原有分区结果被保留
C1：t1p0、t1p1、t0p0
C2：t2p0、t2p1、t2p2

</code></pre>
<p>这种分配器最小化了消费者之间的分区移动，节省了额外的开销处理，且分配结果会尽量平衡。不足之处在于经过多次迭代后，某个消费者可能会拥有其所订阅主题的所有分区，即使最终结果仍然平衡。这将导致某个消费者必须完成所有的消费工作（负载不再均衡）。</p>
<pre><code>public class StickyAssignor extends AbstractStickyAssignor {

    // these schemas are used for preserving consumer's previously assigned partitions
    // list and sending it as user data to the leader during a rebalance
    static final String TOPIC_PARTITIONS_KEY_NAME = &quot;previous_assignment&quot;;
    static final String TOPIC_KEY_NAME = &quot;topic&quot;;
    static final String PARTITIONS_KEY_NAME = &quot;partitions&quot;;
    private static final String GENERATION_KEY_NAME = &quot;generation&quot;;

    static final Schema TOPIC_ASSIGNMENT = new Schema(
        new Field(TOPIC_KEY_NAME, Type.STRING),
        new Field(PARTITIONS_KEY_NAME, new ArrayOf(Type.INT32)));
    static final Schema STICKY_ASSIGNOR_USER_DATA_V0 = new Schema(
        new Field(TOPIC_PARTITIONS_KEY_NAME, new ArrayOf(TOPIC_ASSIGNMENT)));
    private static final Schema STICKY_ASSIGNOR_USER_DATA_V1 = new Schema(
        new Field(TOPIC_PARTITIONS_KEY_NAME, new ArrayOf(TOPIC_ASSIGNMENT)),
        new Field(GENERATION_KEY_NAME, Type.INT32));

    private List&lt;TopicPartition&gt; memberAssignment = null;
    private int generation = DEFAULT_GENERATION; // consumer group generation

    @Override
    public String name() {
        return &quot;sticky&quot;;
    }

    @Override
    public void onAssignment(Assignment assignment, ConsumerGroupMetadata metadata) {
        memberAssignment = assignment.partitions();
        this.generation = metadata.generationId();
    }

    @Override
    public ByteBuffer subscriptionUserData(Set&lt;String&gt; topics) {
        if (memberAssignment == null)
            return null;

        return serializeTopicPartitionAssignment(new MemberData(memberAssignment, Optional.of(generation)));
    }

    @Override
    protected MemberData memberData(Subscription subscription) {
        ByteBuffer userData = subscription.userData();
        if (userData == null || !userData.hasRemaining()) {
            return new MemberData(Collections.emptyList(), Optional.empty());
        }
        return deserializeTopicPartitionAssignment(userData);
    }

    // visible for testing
    static ByteBuffer serializeTopicPartitionAssignment(MemberData memberData) {
        Struct struct = new Struct(STICKY_ASSIGNOR_USER_DATA_V1);
        List&lt;Struct&gt; topicAssignments = new ArrayList&lt;&gt;();
        // 获取每个主题的分区并分组
        for (Map.Entry&lt;String, List&lt;Integer&gt;&gt; topicEntry : CollectionUtils.groupPartitionsByTopic(memberData.partitions).entrySet()) {
            Struct topicAssignment = new Struct(TOPIC_ASSIGNMENT);
            topicAssignment.set(TOPIC_KEY_NAME, topicEntry.getKey());
            topicAssignment.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());
            topicAssignments.add(topicAssignment);
        }
        struct.set(TOPIC_PARTITIONS_KEY_NAME, topicAssignments.toArray());
        if (memberData.generation.isPresent())
            struct.set(GENERATION_KEY_NAME, memberData.generation.get());
        ByteBuffer buffer = ByteBuffer.allocate(STICKY_ASSIGNOR_USER_DATA_V1.sizeOf(struct));
        STICKY_ASSIGNOR_USER_DATA_V1.write(buffer, struct);
        buffer.flip();
        return buffer;
    }

    private static MemberData deserializeTopicPartitionAssignment(ByteBuffer buffer) {
        Struct struct;
        ByteBuffer copy = buffer.duplicate();
        try {
            struct = STICKY_ASSIGNOR_USER_DATA_V1.read(buffer);
        } catch (Exception e1) {
            try {
                // fall back to older schema
                struct = STICKY_ASSIGNOR_USER_DATA_V0.read(copy);
            } catch (Exception e2) {
                // ignore the consumer's previous assignment if it cannot be parsed
                return new MemberData(Collections.emptyList(), Optional.of(DEFAULT_GENERATION));
            }
        }

        List&lt;TopicPartition&gt; partitions = new ArrayList&lt;&gt;();
        for (Object structObj : struct.getArray(TOPIC_PARTITIONS_KEY_NAME)) {
            Struct assignment = (Struct) structObj;
            String topic = assignment.getString(TOPIC_KEY_NAME);
            for (Object partitionObj : assignment.getArray(PARTITIONS_KEY_NAME)) {
                Integer partition = (Integer) partitionObj;
                partitions.add(new TopicPartition(topic, partition));
            }
        }
        // make sure this is backward compatible
        Optional&lt;Integer&gt; generation = struct.hasField(GENERATION_KEY_NAME) ? Optional.of(struct.getInt(GENERATION_KEY_NAME)) : Optional.empty();
        return new MemberData(partitions, generation);
    }
}

</code></pre>
<h4 id="cooperativestickyassignor">CooperativeStickyAssignor</h4>
<p>CooperativeStickyAssignor 逻辑基本与 StickyAssignor 一致，不过在重平衡期间此分配器采用了一种全新的协议：RebalanceProtocol.COOPERATIVE：在这个协议中，每个消费者不会在加入重新平衡之前撤销所有分区，而是尽可能地保留它们并且仅在被告知可以撤销之后才会这样做。</p>
<p>其优劣势与StickyAssignor基本一致，但重平衡更加轻量化，并且不会阻止消费者从分区中消费。</p>
<pre><code>public class CooperativeStickyAssignor extends AbstractStickyAssignor {
    public static final String COOPERATIVE_STICKY_ASSIGNOR_NAME = &quot;cooperative-sticky&quot;;

    // these schemas are used for preserving useful metadata for the assignment, such as the last stable generation
    private static final String GENERATION_KEY_NAME = &quot;generation&quot;;
    private static final Schema COOPERATIVE_STICKY_ASSIGNOR_USER_DATA_V0 = new Schema(
        new Field(GENERATION_KEY_NAME, Type.INT32));

    private int generation = DEFAULT_GENERATION; // consumer group generation

    @Override
    public String name() {
        return COOPERATIVE_STICKY_ASSIGNOR_NAME;
    }

    // 使用 COOPERATIVE 协议进行重平衡操作
    @Override
    public List&lt;RebalanceProtocol&gt; supportedProtocols() {
        return Arrays.asList(RebalanceProtocol.COOPERATIVE, RebalanceProtocol.EAGER);
    }

    @Override
    public void onAssignment(Assignment assignment, ConsumerGroupMetadata metadata) {
        this.generation = metadata.generationId();
    }

    @Override
    public ByteBuffer subscriptionUserData(Set&lt;String&gt; topics) {
        Struct struct = new Struct(COOPERATIVE_STICKY_ASSIGNOR_USER_DATA_V0);

        struct.set(GENERATION_KEY_NAME, generation);
        ByteBuffer buffer = ByteBuffer.allocate(COOPERATIVE_STICKY_ASSIGNOR_USER_DATA_V0.sizeOf(struct));
        COOPERATIVE_STICKY_ASSIGNOR_USER_DATA_V0.write(buffer, struct);
        buffer.flip();
        return buffer;
    }

    @Override
    protected MemberData memberData(Subscription subscription) {
        ByteBuffer buffer = subscription.userData();
        Optional&lt;Integer&gt; encodedGeneration;
        if (buffer == null) {
            encodedGeneration = Optional.empty();
        } else {
            try {
                Struct struct = COOPERATIVE_STICKY_ASSIGNOR_USER_DATA_V0.read(buffer);
                encodedGeneration = Optional.of(struct.getInt(GENERATION_KEY_NAME));
            } catch (Exception e) {
                encodedGeneration = Optional.of(DEFAULT_GENERATION);
            }
        }
        return new MemberData(subscription.ownedPartitions(), encodedGeneration);
    }

    @Override
    public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,
                                                    Map&lt;String, Subscription&gt; subscriptions) {
        Map&lt;String, List&lt;TopicPartition&gt;&gt; assignments = super.assign(partitionsPerTopic, subscriptions);

        Map&lt;TopicPartition, String&gt; partitionsTransferringOwnership = super.partitionsTransferringOwnership == null ?
            computePartitionsTransferringOwnership(subscriptions, assignments) :
            super.partitionsTransferringOwnership;

        // 调整主题分区并返回
        adjustAssignment(assignments, partitionsTransferringOwnership);
        return assignments;
    }

    // Following the cooperative rebalancing protocol requires removing partitions that must first be revoked from the assignment
    // 协作重平衡协议的前提是分区已经从分配中废除。
    private void adjustAssignment(Map&lt;String, List&lt;TopicPartition&gt;&gt; assignments,
                                  Map&lt;TopicPartition, String&gt; partitionsTransferringOwnership) {
        for (Map.Entry&lt;TopicPartition, String&gt; partitionEntry : partitionsTransferringOwnership.entrySet()) {
            assignments.get(partitionEntry.getValue()).remove(partitionEntry.getKey());
        }
    }

    private Map&lt;TopicPartition, String&gt; computePartitionsTransferringOwnership(Map&lt;String, Subscription&gt; subscriptions,
                                                                               Map&lt;String, List&lt;TopicPartition&gt;&gt; assignments) {
        Map&lt;TopicPartition, String&gt; allAddedPartitions = new HashMap&lt;&gt;();
        Set&lt;TopicPartition&gt; allRevokedPartitions = new HashSet&lt;&gt;();

        for (final Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt; entry : assignments.entrySet()) {
            String consumer = entry.getKey();

            List&lt;TopicPartition&gt; ownedPartitions = subscriptions.get(consumer).ownedPartitions();
            List&lt;TopicPartition&gt; assignedPartitions = entry.getValue();

            Set&lt;TopicPartition&gt; ownedPartitionsSet = new HashSet&lt;&gt;(ownedPartitions);
            for (TopicPartition tp : assignedPartitions) {
                if (!ownedPartitionsSet.contains(tp))
                    allAddedPartitions.put(tp, consumer);
            }

            Set&lt;TopicPartition&gt; assignedPartitionsSet = new HashSet&lt;&gt;(assignedPartitions);
            for (TopicPartition tp : ownedPartitions) {
                if (!assignedPartitionsSet.contains(tp))
                    allRevokedPartitions.add(tp);
            }
        }

        allAddedPartitions.keySet().retainAll(allRevokedPartitions);
        return allAddedPartitions;
    }
}

</code></pre>
<h3 id="定制化分区分配器">定制化分区分配器</h3>
<p>除了kafka提供的几种分配器之外，还可以通过继承kafka的AbstractPartitionAssignor类或者直接实现ConsumerPartitionAssignor接口来自定义分区分配器。</p>
<pre><code>public interface ConsumerPartitionAssignor {
     // 所有消费者都将调用此函数，此函数会将订阅的主题发送给broker coordinator
    default ByteBuffer subscriptionUserData(Set&lt;String&gt; topics) {
        return null;
    }
    // 消费组leader收到消费者的订阅信息，并开始进行分区分配
    GroupAssignment assign(Cluster metadata, GroupSubscription groupSubscription);

    // 当消费者收到leader的分配消息时会执行此回调函数
    default void onAssignment(Assignment assignment, ConsumerGroupMetadata metadata) {
    }
  
    // 重平衡协议
    default List&lt;RebalanceProtocol&gt; supportedProtocols() {
        return Collections.singletonList(RebalanceProtocol.EAGER);
    }

    // 分配器版本号
    default short version() {
        return (short) 0;
    }
  
    // 分配器的唯一名称，非必传 (e.g. &quot;range&quot; or &quot;roundrobin&quot; or &quot;sticky&quot;). 
    String name();
}  

</code></pre>
<h4 id="failoverassignor">FailoverAssignor</h4>
<p>FailoverAssignor 主要针对一些需要故障转移的场景，其中的基本逻辑是：让多个消费者加入同一个组。但是，所有分区一次都分配给一个消费者。如果该消费者shutdown，则所有的分区将分配给下一个可用的消费者。通常，分区会被分配给第一个可用的消费者。此例子将会对消费者进行排序。</p>
<pre><code>import org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor;
import org.apache.kafka.common.TopicPartition;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * @author philosopherZB
 */
public class FailoverAssignor extends AbstractPartitionAssignor {

    @Override
    public String name() {
        return &quot;failover&quot;;
    }

    @Override
    public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) {
        Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions);

        Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;();
        for (String memberId : subscriptions.keySet()) {
            assignment.put(memberId, new ArrayList&lt;&gt;());
        }

        for (Map.Entry&lt;String, List&lt;MemberInfo&gt;&gt; topicEntry : consumersPerTopic.entrySet()) {
            String topic = topicEntry.getKey();
            List&lt;MemberInfo&gt; consumersForTopic = topicEntry.getValue();

            Integer numPartitionsForTopic = partitionsPerTopic.get(topic);
            if (numPartitionsForTopic == null) {
                continue;
            }

            // 按字典排序
            Collections.sort(consumersForTopic);

            List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);
            // 没有消费者则跳过
            if (consumersForTopic.isEmpty()) {
                continue;
            }
            // 取排序列表中的第一个消费者
            String consumerMemberId = consumersForTopic.stream().findFirst().get().memberId;
            assignment.get(consumerMemberId).addAll(partitions);
        }
        return assignment;
    }

    private Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic(Map&lt;String, Subscription&gt; consumerMetadata) {
        Map&lt;String, List&lt;MemberInfo&gt;&gt; topicToConsumers = new HashMap&lt;&gt;();
        for (Map.Entry&lt;String, Subscription&gt; subscriptionEntry : consumerMetadata.entrySet()) {
            String consumerId = subscriptionEntry.getKey();
            MemberInfo memberInfo = new MemberInfo(consumerId, subscriptionEntry.getValue().groupInstanceId());
            for (String topic : subscriptionEntry.getValue().topics()) {
                put(topicToConsumers, topic, memberInfo);
            }
        }
        return topicToConsumers;
    }
}

</code></pre>
<pre><code>// 使用
Properties props = new Properties();
...
props.put(
    ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG,   
    FailoverAssignor.class.getName()
);

KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);

</code></pre>
<h4 id="randomassignor">RandomAssignor</h4>
<p>RandomAssignor 主要针对的是消费者数大于分区数的场景。采用随机分配可以让所有的消费者都参与消费动作，而不是处于空闲状态。</p>
<pre><code>import org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor;
import org.apache.kafka.common.TopicPartition;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ThreadLocalRandom;

/**
 * @author philosopherZB
 */
public class RandomAssignor extends AbstractPartitionAssignor {

    @Override
    public String name() {
        return &quot;random&quot;;
    }

    @Override
    public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) {
        Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions);

        Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;();
        for (String memberId : subscriptions.keySet()) {
            assignment.put(memberId, new ArrayList&lt;&gt;());
        }

        for (Map.Entry&lt;String, List&lt;MemberInfo&gt;&gt; topicEntry : consumersPerTopic.entrySet()) {
            String topic = topicEntry.getKey();
            List&lt;MemberInfo&gt; consumersForTopic = topicEntry.getValue();
            int consumerSize = consumersForTopic.size();

            Integer numPartitionsForTopic = partitionsPerTopic.get(topic);
            if (numPartitionsForTopic == null) {
                continue;
            }

            // 可不排序
            Collections.sort(consumersForTopic);

            List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);
            partitions.forEach(partition -&gt; {
                String randomConsumer = consumersForTopic.get(ThreadLocalRandom.current().nextInt(consumerSize)).memberId;
                assignment.get(randomConsumer).add(partition);
            });
        }
        return assignment;
    }

    private Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic(Map&lt;String, Subscription&gt; consumerMetadata) {
        Map&lt;String, List&lt;MemberInfo&gt;&gt; topicToConsumers = new HashMap&lt;&gt;();
        for (Map.Entry&lt;String, Subscription&gt; subscriptionEntry : consumerMetadata.entrySet()) {
            String consumerId = subscriptionEntry.getKey();
            MemberInfo memberInfo = new MemberInfo(consumerId, subscriptionEntry.getValue().groupInstanceId());
            for (String topic : subscriptionEntry.getValue().topics()) {
                put(topicToConsumers, topic, memberInfo);
            }
        }
        return topicToConsumers;
    }
}

</code></pre>
<pre><code>// 使用
Properties props = new Properties();
...
props.put(
    ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG,   
    RandomAssignor.class.getName()
);

KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之Broker控制器]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-kong-zhi-qi/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-kong-zhi-qi/">
        </link>
        <updated>2022-07-09T07:18:09.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka broker控制器。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/neuschwanstein-701732_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="broker控制器">Broker控制器</h2>
<h3 id="简介">简介</h3>
<p>在 Kafka 集群中会有一个或多个 broker，其中有一个 broker 会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态以及执行管理任务，例如主题管理，重分配分区，broker管理，数据管理等。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316005.png" alt="img" loading="lazy"></figure>
<h3 id="控制器选举与failover">控制器选举与Failover</h3>
<p>Broker 在启动时，会尝试去 ZooKeeper 中创建 /controller 节点。第一个创建/controller节点成功的broker将会成为控制器。根据源码来看具体流程如下：</p>
<p>获取 zk 的 /cotroller 节点中的 controllerId，如果该节点不存在（比如集群刚创建时），那么controllerId 将会返回-1；</p>
<ol>
<li>
<p>如果 controller id 不为-1，即 controller 已经存在，直接结束流程；</p>
</li>
<li>
<p>如果 controller id 为-1，证明 controller 还不存在，这时候当前 broker 开始在 zk 注册 controller同时递增epoch值；</p>
</li>
<li>
<p>如果注册成功，那么当前 broker 就成为了 controller，这时候开始调用 onControllerFailover() 方法，正式初始化 controller（注意：controller 节点是临时节点，如果当前 controller 与 zk 的 session 断开，那么 controller 的临时节点会消失，会触发 controller 的重新选举）；</p>
</li>
<li>
<p>如果注册失败（刚好 controller 被其他 broker 创建了、抛出异常等），那么直接返回。</p>
</li>
<li>
<p>Zookeeper leader elector 在选举当前broker作为新控制器时调用onControllerFailover函数。</p>
<p>它在成为控制器状态更改时执行以下操作 -</p>
<ol>
<li>初始化控制器的上下文对象，该对象包含当前主题的缓存对象、存活的broker及所有已存在分区的leaders</li>
<li>启动控制器的channel管理器，建立与其他 broker 的连接的,负责与其他 broker 之间的通信;</li>
<li>启动replicaStateMachine，副本状态机,管理副本的状态</li>
<li>启动partitionStateMachine，分区状态机,管理分区的状态</li>
</ol>
<p>如果在 Controller 服务初始化的过程中，出现了任何不可预期的 异常/错误，它将会退出当前的进程，这确保了可以再次触发 controller 的选举。</p>
</li>
</ol>
<p>关于epoch值：其是存放在ZooKeeper中 /controller_epoch 节点上的一个整型值。controller_epoch 用于记录控制器发生变更的次数，即记录当前的控制器是第几代控制器，直译可以称之为“控制器的纪元”。</p>
<p>controller_epoch 的初始值为1，即集群中第一个控制器的纪元为1，当控制器发生变更时，每选出一个新的控制器就将该字段值加1。Kafka 通过 controller_epoch 来保证控制器的唯一性，进而保证相关操作的一致性。</p>
<p>每个和控制器交互的请求都会携带 controller_epoch 这个字段，如果请求的 controller_epoch 值小于内存中的 controller_epoch 值，则认为这个请求是向已经过期的控制器所发送的请求，那么这个请求会被认定为无效的请求。</p>
<p>如果请求的 controller_epoch 值大于内存中的 controller_epoch 值，那么说明已经有新的控制器当选了。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaServer.scala
override def startup(): Unit = {
    info(&quot;starting&quot;)
    if (isShuttingDown.get)
      throw new IllegalStateException(&quot;Kafka server is still shutting down, cannot re-start!&quot;)

    if (startupComplete.get)
      return

   val canStartup = isStartingUp.compareAndSet(false, true)
   if (canStartup) {
       /* start kafka controller --- 启动 kafka 控制器 */
       _kafkaController = new KafkaController(config, zkClient, time, metrics, brokerInfo, brokerEpoch, tokenManager, brokerFeatures, featureCache, threadNamePrefix)
       kafkaController.startup()
    }
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/controller/KafkaController.scala
/**
 * Invoked when the controller module of a Kafka server is started up. This does not assume that the current broker
 * is the controller. It merely registers the session expiration listener and starts the controller leader
 * elector
 * kafka服务的控制器模块启动时调用此函数。调用此函数的broker并不一定是最终的controller（取决于是否选举成功）。
 * 此函数仅仅只是注册一个会话过期监听器并开始进行controller选举。
 */
def startup() = {
  zkClient.registerStateChangeHandler(new StateChangeHandler {
    override val name: String = StateChangeHandlers.ControllerHandler
    override def afterInitializingSession(): Unit = {
      eventManager.put(RegisterBrokerAndReelect)
    }
    override def beforeInitializingSession(): Unit = {
      val queuedEvent = eventManager.clearAndPut(Expire)

      // Block initialization of the new session until the expiration event is being handled,
      // which ensures that all pending events have been processed before creating the new session
      // 在过期事件被处理掉之前，不允许初始化新会话。这确保了所有的待处理事件在新会话之前能够得到处理。
      queuedEvent.awaitProcessing()
    }
  })
  // 事件状态为启动
  // 文件地址：core/src/main/scala/kafka/controller/ControllerEventManager.scala
  // ControllerEventManager 中的 ControllerEventProcessor 会被 KafkaController 继承，同时重写了其中的process函数
  // process 便是线程具体执行的逻辑函数
  eventManager.put(Startup)
  // 启动事件线程开始进行controller选举
  eventManager.start()
}

// 重写 ControllerEventProcessor 的process函数
override def process(event: ControllerEvent): Unit = {
  try {
    event match {
      ......
      case BrokerChange =&gt;
        // broker 发生改变时，执行此函数。主要用于监听zk节点变化。
        processBrokerChange()
      case Startup =&gt;
        // 执行启动操作
        processStartup()
    }
  } catch {
    case e: ControllerMovedException =&gt;
      info(s&quot;Controller moved to another broker when processing $event.&quot;, e)
      maybeResign()
    case e: Throwable =&gt;
      error(s&quot;Error processing event $event&quot;, e)
  } finally {
    updateMetrics()
  }
}

// 执行启动函数
private def processStartup(): Unit = {
  //  注册zk节点改变处理器，与此同时也会注册一个ExistsRequest观察者
  zkClient.registerZNodeChangeHandlerAndCheckExistence(controllerChangeHandler)
  // 选举controller
  elect()
}

// 文件地址：core/src/main/scala/kafka/zk/KafkaZkClient.scala
def registerZNodeChangeHandlerAndCheckExistence(zNodeChangeHandler: ZNodeChangeHandler): Boolean = {
  zooKeeperClient.registerZNodeChangeHandler(zNodeChangeHandler)
  val existsResponse = retryRequestUntilConnected(ExistsRequest(zNodeChangeHandler.path))
  existsResponse.resultCode match {
    case Code.OK =&gt; true
    case Code.NONODE =&gt; false
    case _ =&gt; throw existsResponse.resultException.get
  }
}

// 选举函数
private def elect(): Unit = {
  // 获取活跃的controllerId  
  activeControllerId = zkClient.getControllerId.getOrElse(-1)
  /*
   * We can get here during the initial startup and the handleDeleted ZK callback. Because of the potential race condition,
   * it's possible that the controller has already been elected when we get here. This check will prevent the following
   * createEphemeralPath method from getting into an infinite loop if this broker is already the controller.
   */
  // 如果存在controller，那么直接返回，不进行选举操作 
  if (activeControllerId != -1) {
    debug(s&quot;Broker $activeControllerId has been elected as the controller, so stopping the election process.&quot;)
    return
  }

  try {
    // 注册controller节点并递增epoch值  
    val (epoch, epochZkVersion) = zkClient.registerControllerAndIncrementControllerEpoch(config.brokerId)
    controllerContext.epoch = epoch
    controllerContext.epochZkVersion = epochZkVersion
    activeControllerId = config.brokerId

    info(s&quot;${config.brokerId} successfully elected as the controller. Epoch incremented to ${controllerContext.epoch} &quot; +
      s&quot;and epoch zk version is now ${controllerContext.epochZkVersion}&quot;)
     // 执行controller快速恢复
    // Zookeeper leader elector 在选举当前broker作为新控制器时调用此callback。
    // 它在成为控制器状态更改时执行以下操作 -
    // 1. 初始化控制器的上下文对象，该对象包含当前主题的缓存对象、存活的broker及所有已存在分区的leaders
    // 2. 启动控制器的channel管理器，建立与其他 broker 的连接的,负责与其他 broker 之间的通信;
    // 3. 启动replicaStateMachine，副本状态机,管理副本的状态
    // 4. 启动partitionStateMachine，分区状态机,管理分区的状态
    // 如果在 Controller 服务初始化的过程中，出现了任何不可预期的 异常/错误，它将会退出当前的进程，这确保了可以再次触发 controller 的选举
    onControllerFailover()
  } catch {
    case e: ControllerMovedException =&gt;
      maybeResign()

      if (activeControllerId != -1)
        debug(s&quot;Broker $activeControllerId was elected as controller instead of broker ${config.brokerId}&quot;, e)
      else
        warn(&quot;A controller has been elected but just resigned, this will result in another round of election&quot;, e)
    case t: Throwable =&gt;
      error(s&quot;Error while electing or becoming controller on broker ${config.brokerId}. &quot; +
        s&quot;Trigger controller movement immediately&quot;, t)
      triggerControllerMove()
  }
}

// 快速恢复
/**
 * This callback is invoked by the zookeeper leader elector on electing the current broker as the new controller.
 * It does the following things on the become-controller state change -
 * 1. Initializes the controller's context object that holds cache objects for current topics, live brokers and
 *    leaders for all existing partitions.
 * 2. Starts the controller's channel manager
 * 3. Starts the replica state machine
 * 4. Starts the partition state machine
 * If it encounters any unexpected exception/error while becoming controller, it resigns as the current controller.
 * This ensures another controller election will be triggered and there will always be an actively serving controller
 */
private def onControllerFailover(): Unit = {
  maybeSetupFeatureVersioning()

  info(&quot;Registering handlers&quot;)

  // before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks
  val childChangeHandlers = Seq(brokerChangeHandler, topicChangeHandler, topicDeletionHandler, logDirEventNotificationHandler,
    isrChangeNotificationHandler)
  childChangeHandlers.foreach(zkClient.registerZNodeChildChangeHandler)

  val nodeChangeHandlers = Seq(preferredReplicaElectionHandler, partitionReassignmentHandler)
  nodeChangeHandlers.foreach(zkClient.registerZNodeChangeHandlerAndCheckExistence)

  info(&quot;Deleting log dir event notifications&quot;)
  zkClient.deleteLogDirEventNotifications(controllerContext.epochZkVersion)
  info(&quot;Deleting isr change notifications&quot;)
  zkClient.deleteIsrChangeNotifications(controllerContext.epochZkVersion)
  info(&quot;Initializing controller context&quot;)
  initializeControllerContext()
  info(&quot;Fetching topic deletions in progress&quot;)
  val (topicsToBeDeleted, topicsIneligibleForDeletion) = fetchTopicDeletionsInProgress()
  info(&quot;Initializing topic deletion manager&quot;)
  topicDeletionManager.init(topicsToBeDeleted, topicsIneligibleForDeletion)

  // We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines
  // are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before
  // they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and
  // partitionStateMachine.startup().
  info(&quot;Sending update metadata request&quot;)
  sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, Set.empty)

  replicaStateMachine.startup()
  partitionStateMachine.startup()

  info(s&quot;Ready to serve as the new controller with epoch $epoch&quot;)

  initializePartitionReassignments()
  topicDeletionManager.tryTopicDeletion()
  val pendingPreferredReplicaElections = fetchPendingPreferredReplicaElections()
  onReplicaElection(pendingPreferredReplicaElections, ElectionType.PREFERRED, ZkTriggered)
  info(&quot;Starting the controller scheduler&quot;)
  kafkaScheduler.startup()
  if (config.autoLeaderRebalanceEnable) {
    scheduleAutoLeaderRebalanceTask(delay = 5, unit = TimeUnit.SECONDS)
  }

  if (config.tokenAuthEnabled) {
    info(&quot;starting the token expiry check scheduler&quot;)
    tokenCleanScheduler.startup()
    tokenCleanScheduler.schedule(name = &quot;delete-expired-tokens&quot;,
      fun = () =&gt; tokenManager.expireTokens(),
      period = config.delegationTokenExpiryCheckIntervalMs,
      unit = TimeUnit.MILLISECONDS)
  }
}

</code></pre>
<h3 id="控制器之分区选举">控制器之分区选举</h3>
<p>controller负责分区的重分配。目前主要有四种选举机制。</p>
<pre><code>sealed trait PartitionLeaderElectionStrategy
// leader 掉线或新分区出现时触发
final case class OfflinePartitionLeaderElectionStrategy(allowUnclean: Boolean) extends PartitionLeaderElectionStrategy
// 分区的副本重新分配数据同步完成后触发的
final case object ReassignPartitionLeaderElectionStrategy extends PartitionLeaderElectionStrategy
// 最优 leader 选举，手动触发或自动 leader 均衡调度时触发
final case object PreferredReplicaPartitionLeaderElectionStrategy extends PartitionLeaderElectionStrategy
// controller 发送 ShutDown 请求主动关闭服务时触发
final case object ControlledShutdownPartitionLeaderElectionStrategy extends PartitionLeaderElectionStrategy

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/controller/PartitionStateMachine.scala
/**
 * Invoked on successful controller election.
 * controller 选举成功后调用此函数
 */
def startup(): Unit = {
  info(&quot;Initializing partition state&quot;)
  initializePartitionState()
  info(&quot;Triggering online partition state changes&quot;)
  // 分区函数入口
  triggerOnlinePartitionStateChange()
  debug(s&quot;Started partition state machine with initial state -&gt; ${controllerContext.partitionStates}&quot;)
}

// 选举分区leader
private def doElectLeaderForPartitions(
  partitions: Seq[TopicPartition],
  partitionLeaderElectionStrategy: PartitionLeaderElectionStrategy
): (Map[TopicPartition, Either[Exception, LeaderAndIsr]], Seq[TopicPartition]) = {
    val (partitionsWithoutLeaders, partitionsWithLeaders) = partitionLeaderElectionStrategy match {
      case OfflinePartitionLeaderElectionStrategy(allowUnclean) =&gt;
        val partitionsWithUncleanLeaderElectionState = collectUncleanLeaderElectionState(
          validLeaderAndIsrs,
          allowUnclean
        )
        // 离线/新分区选举
        leaderForOffline(controllerContext, partitionsWithUncleanLeaderElectionState).partition(_.leaderAndIsr.isEmpty)
      case ReassignPartitionLeaderElectionStrategy =&gt;
        // 重分配选举
        leaderForReassign(controllerContext, validLeaderAndIsrs).partition(_.leaderAndIsr.isEmpty)
      case PreferredReplicaPartitionLeaderElectionStrategy =&gt;
        // 最优选举
        leaderForPreferredReplica(controllerContext, validLeaderAndIsrs).partition(_.leaderAndIsr.isEmpty)
      case ControlledShutdownPartitionLeaderElectionStrategy =&gt;
        // shutdown 时选举
        leaderForControlledShutdown(controllerContext, validLeaderAndIsrs).partition(_.leaderAndIsr.isEmpty)
    }
}  

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/controller/Election.scala
// 从存活的ISR副本中选择一个作为新的leader，如果ISR为空且允许unclean election(脏选举)，那么就从存活的副本中选举一个作为新的leader。
private def leaderForOffline(partition: TopicPartition,
                             leaderAndIsrOpt: Option[LeaderAndIsr],
                             uncleanLeaderElectionEnabled: Boolean,
                             controllerContext: ControllerContext): ElectionResult = {

  val assignment = controllerContext.partitionReplicaAssignment(partition)
  val liveReplicas = assignment.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))
  leaderAndIsrOpt match {
    case Some(leaderAndIsr) =&gt;
      val isr = leaderAndIsr.isr
      // 选举操作
      // 从存活的ISR副本中选择一个作为新的leader，如果ISR为空且允许unclean election(脏选举)，那么就从存活的副本中选举一个作为新的leader。
      val leaderOpt = PartitionLeaderElectionAlgorithms.offlinePartitionLeaderElection(
        assignment, isr, liveReplicas.toSet, uncleanLeaderElectionEnabled, controllerContext)
      val newLeaderAndIsrOpt = leaderOpt.map { leader =&gt;
        val newIsr = if (isr.contains(leader)) isr.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))
        else List(leader)
        // 生成新的ISR
        leaderAndIsr.newLeaderAndIsr(leader, newIsr)
      }
      ElectionResult(partition, newLeaderAndIsrOpt, liveReplicas)

    case None =&gt;
      // 返回无可用副本
      ElectionResult(partition, None, liveReplicas)
  }
}

// 分区重分配时选举，例如新增分区时，重新选举leader
private def leaderForReassign(partition: TopicPartition,
                              leaderAndIsr: LeaderAndIsr,
                              controllerContext: ControllerContext): ElectionResult = {
  val targetReplicas = controllerContext.partitionFullReplicaAssignment(partition).targetReplicas
  // 获取所有在线的副本
  val liveReplicas = targetReplicas.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))
  val isr = leaderAndIsr.isr
  // 从存活的ISR副本中选择一个作为新的leader
  val leaderOpt = PartitionLeaderElectionAlgorithms.reassignPartitionLeaderElection(targetReplicas, isr, liveReplicas.toSet)
  // 生成新的ISR
  val newLeaderAndIsrOpt = leaderOpt.map(leader =&gt; leaderAndIsr.newLeader(leader))
  ElectionResult(partition, newLeaderAndIsrOpt, targetReplicas)
}

// 副本优选选举leader
private def leaderForPreferredReplica(partition: TopicPartition,
                                      leaderAndIsr: LeaderAndIsr,
                                      controllerContext: ControllerContext): ElectionResult = {
  val assignment = controllerContext.partitionReplicaAssignment(partition)
  // 获取所有在线的副本
  val liveReplicas = assignment.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))
  val isr = leaderAndIsr.isr
  // 从存活的ISR副本中选择第一个作为新的leader
  val leaderOpt = PartitionLeaderElectionAlgorithms.preferredReplicaPartitionLeaderElection(assignment, isr, liveReplicas.toSet)
  // 生成新的ISR
  val newLeaderAndIsrOpt = leaderOpt.map(leader =&gt; leaderAndIsr.newLeader(leader))
  ElectionResult(partition, newLeaderAndIsrOpt, assignment)
}

// controller shutdown时，选举新的leader
private def leaderForControlledShutdown(partition: TopicPartition,
                                        leaderAndIsr: LeaderAndIsr,
                                        shuttingDownBrokerIds: Set[Int],
                                        controllerContext: ControllerContext): ElectionResult = {
  val assignment = controllerContext.partitionReplicaAssignment(partition)
  // 获取所有副本，包括shutdown状态的
  val liveOrShuttingDownReplicas = assignment.filter(replica =&gt;
    controllerContext.isReplicaOnline(replica, partition, includeShuttingDownBrokers = true))
  val isr = leaderAndIsr.isr
  // 从存活的ISR副本中选择一个状态不为shutdown的作为新leader
  val leaderOpt = PartitionLeaderElectionAlgorithms.controlledShutdownPartitionLeaderElection(assignment, isr,
    liveOrShuttingDownReplicas.toSet, shuttingDownBrokerIds)
  val newIsr = isr.filter(replica =&gt; !shuttingDownBrokerIds.contains(replica))
  // 生成新的ISR
  val newLeaderAndIsrOpt = leaderOpt.map(leader =&gt; leaderAndIsr.newLeaderAndIsr(leader, newIsr))
  ElectionResult(partition, newLeaderAndIsrOpt, liveOrShuttingDownReplicas)
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/controller/PartitionStateMachine.scala
object PartitionLeaderElectionAlgorithms {
  // 从存活的ISR副本中选择一个作为新的leader，如果ISR为空且允许unclean election(脏选举)，那么就从存活的副本中选举一个作为新的leader。  
  def offlinePartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int], uncleanLeaderElectionEnabled: Boolean, controllerContext: ControllerContext): Option[Int] = {
    assignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id)).orElse {
      if (uncleanLeaderElectionEnabled) {
        val leaderOpt = assignment.find(liveReplicas.contains)
        if (leaderOpt.isDefined)
          controllerContext.stats.uncleanLeaderElectionRate.mark()
        leaderOpt
      } else {
        None
      }
    }
  }

  // 从存活的ISR副本中选择一个作为新的leader
  def reassignPartitionLeaderElection(reassignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int]): Option[Int] = {
    reassignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id))
  }

  // 从存活的ISR副本中选择第一个作为新的leader
  def preferredReplicaPartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int]): Option[Int] = {
    assignment.headOption.filter(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id))
  }

  // 从存活的ISR副本中选择一个状态不为shutdown的作为新leader
  def controlledShutdownPartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int], shuttingDownBrokers: Set[Int]): Option[Int] = {
    assignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id) &amp;&amp; !shuttingDownBrokers.contains(id))
  }
}

</code></pre>
<h3 id="控制器之管理topic">控制器之管理Topic</h3>
<p>负责topic的创建、删除，增加分区等操作。以sh命令创建topic为例，流程如下（源码）：</p>
<h4 id="启动命令">启动命令</h4>
<pre><code>bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --partitions 3 --replication-factor 3 --topic topic_test( Kafka 版本 &gt;= 2.2 支持此方式（推荐）)。
</code></pre>
<pre><code>// 文件地址：/bin/kafka-topics.sh
#!/bin/bash
exec $(dirname $0)/kafka-run-class.sh kafka.admin.TopicCommand &quot;$@&quot;
</code></pre>
<h4 id="topiccommand接受请求参数执行创建请求操作">TopicCommand接受请求参数，执行创建请求操作</h4>
<pre><code>// 文件地址：core/src/main/scala/kafka/admin/TopicCommand.scala
object TopicCommand extends Logging {

  def main(args: Array[String]): Unit = {
    val opts = new TopicCommandOptions(args)
    // 检查必填参数
    opts.checkArgs()

    // 创建topic服务
    val topicService = TopicService(opts.commandConfig, opts.bootstrapServer)

    var exitCode = 0
    try {
      if (opts.hasCreateOption)
        // 根据传入的参数判断判断是否有 --create 参数，有的话走创建主题逻辑。
        topicService.createTopic(opts)
      else if (opts.hasAlterOption)
        topicService.alterTopic(opts)
      else if (opts.hasListOption)
        topicService.listTopics(opts)
      else if (opts.hasDescribeOption)
        topicService.describeTopic(opts)
      else if (opts.hasDeleteOption)
        topicService.deleteTopic(opts)
    } catch {
      case e: ExecutionException =&gt;
        if (e.getCause != null)
          printException(e.getCause)
        else
          printException(e)
        exitCode = 1
      case e: Throwable =&gt;
        printException(e)
        exitCode = 1
    } finally {
      topicService.close()
      Exit.exit(exitCode)
    }
  }
  
    object TopicService {
    // 如果传入的参数有 --command-config，则将这个文件里的参数放到 commandConfig 一个 map 里，假如配置文件里面已经有了bootstrap.servers配置，那么会将其覆盖。  
    def createAdminClient(commandConfig: Properties, bootstrapServer: Option[String]): Admin = {
      bootstrapServer match {
        case Some(serverList) =&gt; commandConfig.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, serverList)
        case None =&gt;
      }
      // 生成一个客户端创建topic
      // Admin文件地址； clients/src/main/java/org/apache/kafka/clients/admin/Admin.java
      Admin.create(commandConfig)
    }

    def apply(commandConfig: Properties, bootstrapServer: Option[String]): TopicService =
      new TopicService(createAdminClient(commandConfig, bootstrapServer))
  }

</code></pre>
<pre><code>// 文件地址: clients/src/main/java/org/apache/kafka/clients/admin/Admin.java
@InterfaceStability.Evolving
public interface Admin extends AutoCloseable {

    /**
     * Create a new Admin with the given configuration.
     *
     * @param props The configuration.
     * @return The new KafkaAdminClient.
     */
    static Admin create(Properties props) {
        // 文件地址: clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.javas
        // 创建了一个客户端，用此客户端来管理topic
        return KafkaAdminClient.createInternal(new AdminClientConfig(props, true), null);
    }
}  

</code></pre>
<pre><code>  // 文件地址：core/src/main/scala/kafka/admin/TopicCommand.scala
  case class TopicService private (adminClient: Admin) extends AutoCloseable {

    def createTopic(opts: TopicCommandOptions): Unit = {
      val topic = new CommandTopicPartition(opts)
      if (Topic.hasCollisionChars(topic.name))
        println(&quot;WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could &quot; +
          &quot;collide. To avoid issues it is best to use either, but not both.&quot;)
      createTopic(topic)
    }

    def createTopic(topic: CommandTopicPartition): Unit = {
      //  假如配置了副本数，--replication-factor 需在1和32767之间。  
      if (topic.replicationFactor.exists(rf =&gt; rf &gt; Short.MaxValue || rf &lt; 1))
        throw new IllegalArgumentException(s&quot;The replication factor must be between 1 and ${Short.MaxValue} inclusive&quot;)
      // // 假如配置了分区数，--partitions 必须大于0。
      if (topic.partitions.exists(partitions =&gt; partitions &lt; 1))
        throw new IllegalArgumentException(s&quot;The partitions must be greater than 0&quot;)

      try {
        // 假如指定了 --replica-assignment 参数，则按照指定的方式来分配副本。  
        val newTopic = if (topic.hasReplicaAssignment)
          new NewTopic(topic.name, asJavaReplicaReassignment(topic.replicaAssignment.get))
        else {
          new NewTopic(
            topic.name,
            topic.partitions.asJava,
            topic.replicationFactor.map(_.toShort).map(Short.box).asJava)
        }

        // 将配置 --config 解析成一个配置 map
        val configsMap = topic.configsToAdd.stringPropertyNames()
          .asScala
          .map(name =&gt; name -&gt; topic.configsToAdd.getProperty(name))
          .toMap.asJava

        newTopic.configs(configsMap)
        //  调用 KafkaAdminClient.createTopics 创建 Topic
        val createResult = adminClient.createTopics(Collections.singleton(newTopic),
          new CreateTopicsOptions().retryOnQuotaViolation(false))
        // 获取创建结果  
        createResult.all().get()
        println(s&quot;Created topic ${topic.name}.&quot;)
      } catch {
        case e : ExecutionException =&gt;
          if (e.getCause == null)
            throw e
          if (!(e.getCause.isInstanceOf[TopicExistsException] &amp;&amp; topic.ifTopicDoesntExist()))
            throw e.getCause
      }
    }

</code></pre>
<pre><code>// 文件地址：clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java
@Override
public CreateTopicsResult createTopics(final Collection&lt;NewTopic&gt; newTopics,
                                       final CreateTopicsOptions options) {
    final Map&lt;String, KafkaFutureImpl&lt;TopicMetadataAndConfig&gt;&gt; topicFutures = new HashMap&lt;&gt;(newTopics.size());
    final CreatableTopicCollection topics = new CreatableTopicCollection();
    for (NewTopic newTopic : newTopics) {
        if (topicNameIsUnrepresentable(newTopic.name())) {
            KafkaFutureImpl&lt;TopicMetadataAndConfig&gt; future = new KafkaFutureImpl&lt;&gt;();
            future.completeExceptionally(new InvalidTopicException(&quot;The given topic name '&quot; +
                newTopic.name() + &quot;' cannot be represented in a request.&quot;));
            topicFutures.put(newTopic.name(), future);
        } else if (!topicFutures.containsKey(newTopic.name())) {
            topicFutures.put(newTopic.name(), new KafkaFutureImpl&lt;&gt;());
            topics.add(newTopic.convertToCreatableTopic());
        }
    }
    if (!topics.isEmpty()) {
        final long now = time.milliseconds();
        final long deadline = calcDeadlineMs(now, options.timeoutMs());
        // 远程调用执行创建操作
        final Call call = getCreateTopicsCall(options, topicFutures, topics,
            Collections.emptyMap(), now, deadline);
        runnable.call(call, now);
    }
    return new CreateTopicsResult(new HashMap&lt;&gt;(topicFutures));
}

private Call getCreateTopicsCall(final CreateTopicsOptions options,
                                 final Map&lt;String, KafkaFutureImpl&lt;TopicMetadataAndConfig&gt;&gt; futures,
                                 final CreatableTopicCollection topics,
                                 final Map&lt;String, ThrottlingQuotaExceededException&gt; quotaExceededExceptions,
                                 final long now,
                                 final long deadline) {
    //  使用ControllerNodeProvider选择controller进行远程调用，从此也可以看出，topic的创建是由controller控制                              
    return new Call(&quot;createTopics&quot;, deadline, new ControllerNodeProvider()) {
        @Override
        public CreateTopicsRequest.Builder createRequest(int timeoutMs) {
            return new CreateTopicsRequest.Builder(
                new CreateTopicsRequestData()
                    .setTopics(topics)
                    .setTimeoutMs(timeoutMs)
                    .setValidateOnly(options.shouldValidateOnly()));
        }

        @Override
        public void handleResponse(AbstractResponse abstractResponse) {
            // 处理响应结果
        }

        @Override
        void handleFailure(Throwable throwable) {
           // 处理失败
        }
    };
}

/**
 * Provides the controller node.
 */
private class ControllerNodeProvider implements NodeProvider {
    @Override
    public Node provide() {
        if (metadataManager.isReady() &amp;&amp;
                (metadataManager.controller() != null)) {
            // 返回集群中的broker控制器    
            return metadataManager.controller();
        }
        metadataManager.requestUpdate();
        return null;
    }
}  

</code></pre>
<h4 id="服务端接受请求并执行创建操作">服务端接受请求，并执行创建操作</h4>
<p>可参考上篇<a href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-qing-qiu-liu-cheng">《kafka深入理解之Broker请求流程》</a></p>
<pre><code>// 文件地址：/core/src/main/scala/kafka/server/KafkaRequestHandler.scala
// 核心函数是def run()中的 apis.handle(request, requestLocal)
// 文件地址：/core/src/main/scala/kafka/server/KafkaApis.scala
class KafkaApis(val requestChannel: RequestChannel,
                val metadataSupport: MetadataSupport,
                val replicaManager: ReplicaManager,
                val groupCoordinator: GroupCoordinator,
                val txnCoordinator: TransactionCoordinator,
                val autoTopicCreationManager: AutoTopicCreationManager,
                val brokerId: Int,
                val config: KafkaConfig,
                val configRepository: ConfigRepository,
                val metadataCache: MetadataCache,
                val metrics: Metrics,
                val authorizer: Option[Authorizer],
                val quotas: QuotaManagers,
                val fetchManager: FetchManager,
                brokerTopicStats: BrokerTopicStats,
                val clusterId: String,
                time: Time,
                val tokenManager: DelegationTokenManager,
                val apiVersionManager: ApiVersionManager) extends ApiRequestHandler with Logging {

  type FetchResponseStats = Map[TopicPartition, RecordConversionStats]
  this.logIdent = &quot;[KafkaApi-%d] &quot;.format(brokerId)
  val configHelper = new ConfigHelper(metadataCache, config, configRepository)
  val authHelper = new AuthHelper(authorizer)
  val requestHelper = new RequestHandlerHelper(requestChannel, quotas, time)
  val aclApis = new AclApis(authHelper, authorizer, requestHelper, &quot;broker&quot;, config)
  val configManager = new ConfigAdminManager(brokerId, config, configRepository)

  // 根据处理类型执行相关操作 
  private def maybeForwardToController(
    request: RequestChannel.Request,
    handler: RequestChannel.Request =&gt; Unit
  ): Unit = {
    def responseCallback(responseOpt: Option[AbstractResponse]): Unit = {
      responseOpt match {
        case Some(response) =&gt; requestHelper.sendForwardedResponse(request, response)
        case None =&gt; handleInvalidVersionsDuringForwarding(request)
      }
    }
    metadataSupport.maybeForward(request, handler, responseCallback)
  }

  /**
   * Top-level method that handles all requests and multiplexes to the right api
   */
  override def handle(request: RequestChannel.Request, requestLocal: RequestLocal): Unit = {
    try {
      trace(s&quot;Handling request:${request.requestDesc(true)} from connection ${request.context.connectionId};&quot; +
        s&quot;securityProtocol:${request.context.securityProtocol},principal:${request.context.principal}&quot;)

      if (!apiVersionManager.isApiEnabled(request.header.apiKey)) {
        // The socket server will reject APIs which are not exposed in this scope and close the connection
        // before handing them to the request handler, so this path should not be exercised in practice
        throw new IllegalStateException(s&quot;API ${request.header.apiKey} is not enabled&quot;)
      }

      // 根据api key匹配执行不同的操作
      request.header.apiKey match {
        ......  
        // 创建topic
        case ApiKeys.CREATE_TOPICS =&gt; maybeForwardToController(request, handleCreateTopicsRequest)
        ......
        case _ =&gt; throw new IllegalStateException(s&quot;No handler for request api key ${request.header.apiKey}&quot;)
      }
    } catch {
      case e: FatalExitError =&gt; throw e
      case e: Throwable =&gt;
        error(s&quot;Unexpected error handling request ${request.requestDesc(true)} &quot; +
          s&quot;with context ${request.context}&quot;, e)
        requestHelper.handleError(request, e)
    } finally {
      // try to complete delayed action. In order to avoid conflicting locking, the actions to complete delayed requests
      // are kept in a queue. We add the logic to check the ReplicaManager queue at the end of KafkaApis.handle() and the
      // expiration thread for certain delayed operations (e.g. DelayedJoin)
      replicaManager.tryCompleteActions()
      // The local completion time may be set while processing the request. Only record it if it's unset.
      if (request.apiLocalCompleteTimeNanos &lt; 0)
        request.apiLocalCompleteTimeNanos = time.nanoseconds
    }
  }

// 执行创建topic的请求  
def handleCreateTopicsRequest(request: RequestChannel.Request): Unit = {
  // 获取zookeeper支持器，用于在zookeeper上创建对应的topic节点
  val zkSupport = metadataSupport.requireZkOrThrow(KafkaApis.shouldAlwaysForward(request))
  val controllerMutationQuota = quotas.controllerMutation.newQuotaFor(request, strictSinceVersion = 6)
  
  // 响应结果回调
  def sendResponseCallback(results: CreatableTopicResultCollection): Unit = {
    def createResponse(requestThrottleMs: Int): AbstractResponse = {
      val responseData = new CreateTopicsResponseData()
        .setThrottleTimeMs(requestThrottleMs)
        .setTopics(results)
      val responseBody = new CreateTopicsResponse(responseData)
      trace(s&quot;Sending create topics response $responseData for correlation id &quot; +
        s&quot;${request.header.correlationId} to client ${request.header.clientId}.&quot;)
      responseBody
    }
    requestHelper.sendResponseMaybeThrottleWithControllerQuota(controllerMutationQuota, request, createResponse)
  }

  val createTopicsRequest = request.body[CreateTopicsRequest]
  val results = new CreatableTopicResultCollection(createTopicsRequest.data.topics.size)
  // 判断当前broker是否为controller，只有controller才能创建topic----创建topic过程中，可能出现controller变更
  if (!zkSupport.controller.isActive) {
    createTopicsRequest.data.topics.forEach { topic =&gt;
      results.add(new CreatableTopicResult().setName(topic.name)
        .setErrorCode(Errors.NOT_CONTROLLER.code))
    }
    sendResponseCallback(results)
  } else {
    createTopicsRequest.data.topics.forEach { topic =&gt;
      results.add(new CreatableTopicResult().setName(topic.name))
    }
    ......
    // 处理创建topic响应结果
    def handleCreateTopicsResults(errors: Map[String, ApiError]): Unit = {
      errors.foreach { case (topicName, error) =&gt;
        val result = results.find(topicName)
        result.setErrorCode(error.error.code)
          .setErrorMessage(error.message)
        // Reset any configs in the response if Create failed
        if (error != ApiError.NONE) {
          result.setConfigs(List.empty.asJava)
            .setNumPartitions(-1)
            .setReplicationFactor(-1)
            .setTopicConfigErrorCode(Errors.NONE.code)
        }
      }
      sendResponseCallback(results)
    }
    // 在zookeeper上创建topic节点
    zkSupport.adminManager.createTopics(
      createTopicsRequest.data.timeoutMs,
      createTopicsRequest.data.validateOnly,
      toCreate,
      authorizedForDescribeConfigs,
      controllerMutationQuota,
      handleCreateTopicsResults)
  }
}  

</code></pre>
<h4 id="写入zk的参数信息">写入zk的参数信息</h4>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/ZkAdminManager.scala
/**
  * Create topics and wait until the topics have been completely created.
  * The callback function will be triggered either when timeout, error or the topics are created.
  */
def createTopics(timeout: Int,
                 validateOnly: Boolean,
                 toCreate: Map[String, CreatableTopic],
                 includeConfigsAndMetadata: Map[String, CreatableTopicResult],
                 controllerMutationQuota: ControllerMutationQuota,
                 responseCallback: Map[String, ApiError] =&gt; Unit): Unit = {

  // 1. map over topics creating assignment and calling zookeeper
  val brokers = metadataCache.getAliveBrokers()
  val metadata = toCreate.values.map(topic =&gt;
    try {
      ......

      if (validateOnly) {
        CreatePartitionsMetadata(topic.name, assignments.keySet)
      } else {
        controllerMutationQuota.record(assignments.size)
        // 创建topic时把相关参数也写入zk中
        adminZkClient.createTopicWithAssignment(topic.name, configs, assignments, validate = false, config.usesTopicId)
        populateIds(includeConfigsAndMetadata, topic.name)
        CreatePartitionsMetadata(topic.name, assignments.keySet)
      }
    } catch {
      // Log client errors at a lower level than unexpected exceptions
      ......
    }).toBuffer
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/zk/ZkAdminManager.scala
/**
 * Create topic and optionally validate its parameters. Note that this method is used by the
 * TopicCommand as well.
 *
 * @param topic The name of the topic
 * @param config The config of the topic
 * @param partitionReplicaAssignment The assignments of the topic
 * @param validate Boolean indicating if parameters must be validated or not (true by default)
 * @param usesTopicId Boolean indicating whether the topic ID will be created
 */
def createTopicWithAssignment(topic: String,
                              config: Properties,
                              partitionReplicaAssignment: Map[Int, Seq[Int]],
                              validate: Boolean = true,
                              usesTopicId: Boolean = false): Unit = {
  if (validate)
    validateTopicCreate(topic, partitionReplicaAssignment, config)

  info(s&quot;Creating topic $topic with configuration $config and initial partition &quot; +
    s&quot;assignment $partitionReplicaAssignment&quot;)

  // write out the config if there is any, this isn't transactional with the partition assignments
  // 将topic配置信息写入zk，不包含分区参数
  zkClient.setOrCreateEntityConfigs(ConfigType.Topic, topic, config)

  // create the partition assignment
  // 写入分区参数
  writeTopicPartitionAssignment(topic, partitionReplicaAssignment.map { case (k, v) =&gt; k -&gt; ReplicaAssignment(v) },
    isUpdate = false, usesTopicId)
}

// 写入分区参数信息
// 对应的参数写入znode路径--TopicZNode.path(topic)----/brokers/topics/&lt;topic_name&gt;  
private def writeTopicPartitionAssignment(topic: String, replicaAssignment: Map[Int, ReplicaAssignment],
                                          isUpdate: Boolean, usesTopicId: Boolean = false): Unit = {
  try {
    val assignment = replicaAssignment.map { case (partitionId, replicas) =&gt; (new TopicPartition(topic,partitionId), replicas) }.toMap

    if (!isUpdate) {
      val topicIdOpt = if (usesTopicId) Some(Uuid.randomUuid()) else None
      // 不存在则创建
      zkClient.createTopicAssignment(topic, topicIdOpt, assignment.map { case (k, v) =&gt; k -&gt; v.replicas })
    } else {
      val topicIds = zkClient.getTopicIdsForTopics(Set(topic))
      // 存在就更新参数信息
      zkClient.setTopicAssignment(topic, topicIds.get(topic), assignment)
    }
    debug(&quot;Updated path %s with %s for replica assignment&quot;.format(TopicZNode.path(topic), assignment))
  } catch {
    case _: NodeExistsException =&gt; throw new TopicExistsException(s&quot;Topic '$topic' already exists.&quot;)
    case e2: Throwable =&gt; throw new AdminOperationException(e2.toString)
  }
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/zk/KafkaZkClient.scala
/**
 * Sets or creates the entity znode path with the given configs depending
 * on whether it already exists or not.
 * 根据给定的路径创建znode节点（如果不存在则创建，topic节点）---/config/topics/&lt;topic_name&gt;  
 *
 * If this is method is called concurrently, the last writer wins. In cases where we update configs and then
 * partition assignment (i.e. create topic), it's possible for one thread to set this and the other to set the
 * partition assignment. As such, the recommendation is to never call create topic for the same topic with different
 * configs/partition assignment concurrently.
 *
 * @param rootEntityType entity type
 * @param sanitizedEntityName entity name
 * @throws KeeperException if there is an error while setting or creating the znode
 */
def setOrCreateEntityConfigs(rootEntityType: String, sanitizedEntityName: String, config: Properties) = {

  def set(configData: Array[Byte]): SetDataResponse = {
    val setDataRequest = SetDataRequest(ConfigEntityZNode.path(rootEntityType, sanitizedEntityName),
      configData, ZkVersion.MatchAnyVersion)
    retryRequestUntilConnected(setDataRequest)
  }

  def createOrSet(configData: Array[Byte]): Unit = {
    // znode 节点路径名：/config/topics/&lt;topic_name&gt;  
    val path = ConfigEntityZNode.path(rootEntityType, sanitizedEntityName)
    try createRecursive(path, configData)
    catch {
      case _: NodeExistsException =&gt; set(configData).maybeThrow()
    }
  }

  val configData = ConfigEntityZNode.encode(config)

  val setDataResponse = set(configData)
  setDataResponse.resultCode match {
    case Code.NONODE =&gt; createOrSet(configData)
    case _ =&gt; setDataResponse.maybeThrow()
  }
}

</code></pre>
<h4 id="controller监听brokerstopicstopic_name-如果节点有变化则会通知controller进行处理">Controller监听/brokers/topics/&lt;topic_name&gt; ，如果节点有变化，则会通知Controller进行处理</h4>
<pre><code>// 文件地址：core/src/main/scala/kafka/controller/KafkaController.scala
override def process(event: ControllerEvent): Unit = {
  try {
    event match {
      case event: MockEvent =&gt;
        // Used only in test cases
        event.process()
      ......
      // 处理topic改变
      case TopicChange =&gt;
        processTopicChange()
      ...... 
    }
  } catch {
    case e: ControllerMovedException =&gt;
      info(s&quot;Controller moved to another broker when processing $event.&quot;, e)
      maybeResign()
    case e: Throwable =&gt;
      error(s&quot;Error processing event $event&quot;, e)
  } finally {
    updateMetrics()
  }
}

private def processTopicChange(): Unit = {
  // 非 controller 角色则直接返回，不作任何处理  
  if (!isActive) return
  // 获取zk集群中的所有topic信息---即/brokers/topics节点下的信息
  val topics = zkClient.getAllTopicsInCluster(true)
  // 获取新增状态的topic
  val newTopics = topics -- controllerContext.allTopics
  // 获取删除状态的topic
  val deletedTopics = controllerContext.allTopics.diff(topics)
  // 所有的topic都放入控制器上下文中
  controllerContext.setAllTopics(topics)

  // 注册分区变动处理器
  registerPartitionModificationsHandlers(newTopics.toSeq)
  // 对于新增topic，获取其可分配的副本及主题id
  val addedPartitionReplicaAssignment = zkClient.getReplicaAssignmentAndTopicIdForTopics(newTopics)
  // 从控制器上下文中移除已删除的topic
  deletedTopics.foreach(controllerContext.removeTopic)
  processTopicIds(addedPartitionReplicaAssignment)

  addedPartitionReplicaAssignment.foreach { case TopicIdReplicaAssignment(_, _, newAssignments) =&gt;
    newAssignments.foreach { case (topicAndPartition, newReplicaAssignment) =&gt;
      controllerContext.updatePartitionFullReplicaAssignment(topicAndPartition, newReplicaAssignment)
    }
  }
  info(s&quot;New topics: [$newTopics], deleted topics: [$deletedTopics], new partition replica assignment &quot; +
    s&quot;[$addedPartitionReplicaAssignment]&quot;)
  if (addedPartitionReplicaAssignment.nonEmpty) {
    val partitionAssignments = addedPartitionReplicaAssignment
      .map { case TopicIdReplicaAssignment(_, _, partitionsReplicas) =&gt; partitionsReplicas.keySet }
      .reduce((s1, s2) =&gt; s1.union(s2))
    // 处理新的可分配分区的状态  
    onNewPartitionCreation(partitionAssignments)
  }
}

/**
 * This callback is invoked by the topic change callback with the list of failed brokers as input.
 * It does the following -
 * 1. Move the newly created partitions to the NewPartition state--新创建的分区修改状态为NewPartition 
 * 2. Move the newly created partitions from NewPartition-&gt;OnlinePartition state--新创建的分区修改状态NewPartition为OnlinePartition 
 */
private def onNewPartitionCreation(newPartitions: Set[TopicPartition]): Unit = {
  info(s&quot;New partition creation callback for ${newPartitions.mkString(&quot;,&quot;)}&quot;)
  // 修改状态为 NewPartition
  partitionStateMachine.handleStateChanges(newPartitions.toSeq, NewPartition)
  // 修改状态为 NewReplica
  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions).toSeq, NewReplica)
  // 修改状态为 OnlinePartition
  partitionStateMachine.handleStateChanges(
    newPartitions.toSeq,
    OnlinePartition,
    Some(OfflinePartitionLeaderElectionStrategy(false))
  )
  // 修改状态为 OnlineReplica
  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions).toSeq, OnlineReplica)
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaApis.scala
def handleLeaderAndIsrRequest(request: RequestChannel.Request): Unit = {
  val zkSupport = metadataSupport.requireZkOrThrow(KafkaApis.shouldNeverReceive(request))
  // ensureTopicExists is only for client facing requests
  // We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they
  // stop serving data to clients for the topic being deleted
  val correlationId = request.header.correlationId
  val leaderAndIsrRequest = request.body[LeaderAndIsrRequest]

  authHelper.authorizeClusterOperation(request, CLUSTER_ACTION)
  if (isBrokerEpochStale(zkSupport, leaderAndIsrRequest.brokerEpoch)) {
    // When the broker restarts very quickly, it is possible for this broker to receive request intended
    // for its previous generation so the broker should skip the stale request.
    info(&quot;Received LeaderAndIsr request with broker epoch &quot; +
      s&quot;${leaderAndIsrRequest.brokerEpoch} smaller than the current broker epoch ${zkSupport.controller.brokerEpoch}&quot;)
    requestHelper.sendResponseExemptThrottle(request, leaderAndIsrRequest.getErrorResponse(0, Errors.STALE_BROKER_EPOCH.exception))
  } else {
    // 创建本地log，如果日志已经存在，只返回现有日志的副本，否则如果 isNew=true 或者如果没有离线日志目录，则为给定的主题和给定的分区创建日志，否则抛出 KafkaStorageException。
    // 源码调用链：kafka.server.ReplicaManager#becomeLeaderOrFollower -&gt; kafka.server.ReplicaManager#makeLeaders 
    // -&gt; kafka.cluster.Partition#makeLeader -&gt; kafka.cluster.Partition#createLogIfNotExists
    // -&gt; kafka.cluster.Partition#createLog-&gt; kafka.log.LogManager#getOrCreateLog
    val response = replicaManager.becomeLeaderOrFollower(correlationId, leaderAndIsrRequest,
      RequestHandlerHelper.onLeadershipChange(groupCoordinator, txnCoordinator, _, _))
    requestHelper.sendResponseExemptThrottle(request, response)
  }
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/cluster/Partition.scala
def createLogIfNotExists(isNew: Boolean, isFutureReplica: Boolean, offsetCheckpoints: OffsetCheckpoints, topicId: Option[Uuid]): Unit = {
  def maybeCreate(logOpt: Option[UnifiedLog]): UnifiedLog = {
    logOpt match {
      case Some(log) =&gt;
        trace(s&quot;${if (isFutureReplica) &quot;Future UnifiedLog&quot; else &quot;UnifiedLog&quot;} already exists.&quot;)
        if (log.topicId.isEmpty)
          topicId.foreach(log.assignTopicId)
        log
      case None =&gt;
        // 不存在才创建
        createLog(isNew, isFutureReplica, offsetCheckpoints, topicId)
    }
  }

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之Broker请求流程]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-qing-qiu-liu-cheng/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-qing-qiu-liu-cheng/">
        </link>
        <updated>2022-07-02T06:17:03.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka broker请求流程。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/nature-21474001_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="broker请求流程详解">Broker请求流程详解</h2>
<h3 id="reactor-模型">Reactor 模型</h3>
<p>在介绍kafka的broker通信前，有必要简述一下reactor(响应式)模型。Reactor核心是基于事件驱动（IO多路复用），整体架构流程图如下所示(源自Doug Lea)：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316001.png" alt="img" loading="lazy"></figure>
<ul>
<li>整个模型中所有的输入都由一个Reactor进行接受，随后通过一个dispatch loop（即acceptor）轮询将事件推送至不同的工作线程处理器进行相关逻辑处理。</li>
<li>在这个架构中，Acceptor 线程只是用来进行请求分发，所以非常轻量级，因此会有很高的吞吐量。而那些工作线程则可以根据实际系统负载情况动态的调节系统负载能力，从而达到请求处理的平衡性。</li>
<li>多核处理时，还可以针对reactor进行池化处理(pool)，来提高IO效率。在此过程中，每个Reactor都有自己的选择器，线程处理器及分发轮询器，由主acceptor将数据分发给其他reactors。如下图所示：</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316002.png" alt="img" loading="lazy"></figure>
<h3 id="broker-处理流程">Broker 处理流程</h3>
<p>在kafka中，集群中的每个broker都面临着巨量的网络请求，这个时候如果使用单线程或者多线程进行网络连接处理，那吞吐量将难以达到期望值。此时，上文所说的reactor模型便有了应用场景。</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316003.png" alt="img" loading="lazy"></figure>
<p>在Kafka的架构中，会有很多客户端向Broker端发送请求，Kafka 的 Broker 端有个 SocketServer 组件，主要负责和客户端建立连接，并将相关请求通过Acceptor转发给对应的处理器线程池；而网络连接池(RequestHandlerPool)则主要负责真实的逻辑处理。</p>
<p>SocketServer 组件是 Kafka 超高并发网络通信层中最重要的子模块。它包含 Acceptor 线程、Processor 线程和 RequestChannel 等对象，都是网络通信的重要组成部分。它主要实现了 Reactor 设计模式，用来处理外部多个 Clients（这里的 Clients 可能包含 Producer、Consumer 或其他 Broker）的并发请求，并负责将处理结果封装进 Response 中，返还给 Clients。</p>
<p>其中Acceptor 线程采用轮询的方式将入站请求公平地发到所有网络线程中，网络线程池默认大小是 3个，表示每台 Broker 启动时会创建 3 个网络线程，专门处理客户端发送的请求，可以通过Broker 端参数 num.network.threads（可适当的设置为CPU核数*2，这个值过低时可能会出现因网络空闲太低而缺失副本。）来进行修改。</p>
<p>关于网络线程处理流程如下所示：</p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316004.png" alt="img" loading="lazy"></figure>
<p>当网络线程拿到请求后，会将请求放入到一个共享请求队列中。Broker 端还有个 IO 线程池，负责从该队列中取出请求，执行真正的处理。如果是 PRODUCE 生产请求，则将消息写入到底层的磁盘日志中；如果是 FETCH 请求，则从磁盘或页缓存中读取消息。</p>
<p>IO 线程池中的线程负责执行具体的请求逻辑，默认是8，表示每台 Broker 启动后自动创建 8 个 IO 线程处理请求，可以通过Broker 端参数 num.io.threads（可设置为broker磁盘个数*2）调整。</p>
<p>Purgatory组件是用来缓存延时请求（Delayed Request）的。比如设置了 acks=all 的 PRODUCE 请求，一旦设置了 acks=all，那么该请求就必须等待 ISR 中所有副本都接收了消息后才能返回，此时处理该请求的 IO 线程就必须等待其他 Broker 的写入结果。</p>
<h4 id="socketserver">SocketServer</h4>
<p>负责管理Acceptor 线程、Processor线程和 RequestChannel等对象。</p>
<p>源码地址：<a href="https://github.com/apache/kafka">点击此处跳转github-kafka源码地址</a></p>
<pre><code>// 文件地址：core/src/main/scala/kafka/network/SocketServer.scala
class SocketServer(val config: KafkaConfig,
                   val metrics: Metrics,
                   val time: Time,
                   val credentialProvider: CredentialProvider,
                   val apiVersionManager: ApiVersionManager)
  extends Logging with KafkaMetricsGroup with BrokerReconfigurable {
  // 共享队列长度，即网络阻塞之前可容纳的等待数。由broker端的queued.max.requests参数控制，默认值500
  private val maxQueuedRequests = config.queuedMaxRequests

  private val nodeId = config.brokerId

  private val logContext = new LogContext(s&quot;[SocketServer listenerType=${apiVersionManager.listenerType}, nodeId=$nodeId] &quot;)

  this.logIdent = logContext.logPrefix

  private val memoryPoolSensor = metrics.sensor(&quot;MemoryPoolUtilization&quot;)
  private val memoryPoolDepletedPercentMetricName = metrics.metricName(&quot;MemoryPoolAvgDepletedPercent&quot;, MetricsGroup)
  private val memoryPoolDepletedTimeMetricName = metrics.metricName(&quot;MemoryPoolDepletedTimeTotal&quot;, MetricsGroup)
  memoryPoolSensor.add(new Meter(TimeUnit.MILLISECONDS, memoryPoolDepletedPercentMetricName, memoryPoolDepletedTimeMetricName))
  private val memoryPool = if (config.queuedMaxBytes &gt; 0) new SimpleMemoryPool(config.queuedMaxBytes, config.socketRequestMaxBytes, false, memoryPoolSensor) else MemoryPool.NONE
 
   // data-plane
  // 处理数据面请求的 processor线程池 
  private val dataPlaneProcessors = new ConcurrentHashMap[Int, Processor]()
  // 处理数据面请求的 acceptor线程池，一个监听器对应一个acceptor线程
  private[network] val dataPlaneAcceptors = new ConcurrentHashMap[EndPoint, Acceptor]()
  // 处理数据面的requestChannel对象
  val dataPlaneRequestChannel = new RequestChannel(maxQueuedRequests, DataPlaneMetricPrefix, time, apiVersionManager.newRequestMetrics)
  
  // control-plane
  // 处理控制面请求的 processor，只一个
  private var controlPlaneProcessorOpt : Option[Processor] = None
  // 处理控制面请求的 acceptor，只一个
  private[network] var controlPlaneAcceptorOpt : Option[Acceptor] = None
  // 处理控制面的requestChannel对象
  val controlPlaneRequestChannelOpt: Option[RequestChannel] = config.controlPlaneListenerName.map(_ =&gt;
    new RequestChannel(20, ControlPlaneMetricPrefix, time, apiVersionManager.newRequestMetrics))

  private var nextProcessorId = 0
  val connectionQuotas = new ConnectionQuotas(config, time, metrics)
  private var startedProcessingRequests = false
  private var stoppedProcessingRequests = false

</code></pre>
<h4 id="requestchannel">RequestChannel</h4>
<p>负责管理Processor，并作为传输request和response的中转站。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/network/RequestChannel.scala
class RequestChannel(val queueSize: Int,
                     val metricNamePrefix: String,
                     time: Time,
                     val metrics: RequestChannel.Metrics) extends KafkaMetricsGroup {
  import RequestChannel._
  // 共享请求阻塞队列
  private val requestQueue = new ArrayBlockingQueue[BaseRequest](queueSize)
  // processor 线程池
  private val processors = new ConcurrentHashMap[Int, Processor]()
  val requestQueueSizeMetricName = metricNamePrefix.concat(RequestQueueSizeMetric)
  val responseQueueSizeMetricName = metricNamePrefix.concat(ResponseQueueSizeMetric)

  newGauge(requestQueueSizeMetricName, () =&gt; requestQueue.size)

  newGauge(responseQueueSizeMetricName, () =&gt; {
    processors.values.asScala.foldLeft(0) {(total, processor) =&gt;
      total + processor.responseQueueSize
    }
  })
  // 添加processor线程 
  def addProcessor(processor: Processor): Unit = {
    if (processors.putIfAbsent(processor.id, processor) != null)
      warn(s&quot;Unexpected processor with processorId ${processor.id}&quot;)

    newGauge(responseQueueSizeMetricName, () =&gt; processor.responseQueueSize,
      Map(ProcessorMetricTag -&gt; processor.id.toString))
  }
  // 删除processor线程 
  def removeProcessor(processorId: Int): Unit = {
    processors.remove(processorId)
    removeMetric(responseQueueSizeMetricName, Map(ProcessorMetricTag -&gt; processorId.toString))
  }

  /** Send a request to be handled, potentially blocking until there is room in the queue for the request */
  // 发送请求到队列中，如果空间不足将会处于阻塞状态
  def sendRequest(request: RequestChannel.Request): Unit = {
    requestQueue.put(request)
  }
  // 关闭连接
  def closeConnection(
    request: RequestChannel.Request,
    errorCounts: java.util.Map[Errors, Integer]
  ): Unit = {
    // This case is used when the request handler has encountered an error, but the client
    // does not expect a response (e.g. when produce request has acks set to 0)
    updateErrorMetrics(request.header.apiKey, errorCounts.asScala)
    sendResponse(new RequestChannel.CloseConnectionResponse(request))
  }

  def sendResponse(
    request: RequestChannel.Request,
    response: AbstractResponse,
    onComplete: Option[Send =&gt; Unit]
  ): Unit = {
    updateErrorMetrics(request.header.apiKey, response.errorCounts.asScala)
    sendResponse(new RequestChannel.SendResponse(
      request,
      request.buildResponseSend(response),
      request.responseNode(response),
      onComplete
    ))
  }

  def sendNoOpResponse(request: RequestChannel.Request): Unit = {
    sendResponse(new network.RequestChannel.NoOpResponse(request))
  }

  def startThrottling(request: RequestChannel.Request): Unit = {
    sendResponse(new RequestChannel.StartThrottlingResponse(request))
  }

  def endThrottling(request: RequestChannel.Request): Unit = {
    sendResponse(new EndThrottlingResponse(request))
  }

  /** Send a response back to the socket server to be sent over the network */
  // 将响应结果发回socket服务，并通过网络传输
  private[network] def sendResponse(response: RequestChannel.Response): Unit = {
    if (isTraceEnabled) {
      val requestHeader = response.request.headerForLoggingOrThrottling()
      val message = response match {
        case sendResponse: SendResponse =&gt;
          s&quot;Sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} of ${sendResponse.responseSend.size} bytes.&quot;
        case _: NoOpResponse =&gt;
          s&quot;Not sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} as it's not required.&quot;
        case _: CloseConnectionResponse =&gt;
          s&quot;Closing connection for client ${requestHeader.clientId} due to error during ${requestHeader.apiKey}.&quot;
        case _: StartThrottlingResponse =&gt;
          s&quot;Notifying channel throttling has started for client ${requestHeader.clientId} for ${requestHeader.apiKey}&quot;
        case _: EndThrottlingResponse =&gt;
          s&quot;Notifying channel throttling has ended for client ${requestHeader.clientId} for ${requestHeader.apiKey}&quot;
      }
      trace(message)
    }

    response match {
      // We should only send one of the following per request
      case _: SendResponse | _: NoOpResponse | _: CloseConnectionResponse =&gt;
        val request = response.request
        val timeNanos = time.nanoseconds()
        request.responseCompleteTimeNanos = timeNanos
        if (request.apiLocalCompleteTimeNanos == -1L)
          request.apiLocalCompleteTimeNanos = timeNanos
      // For a given request, these may happen in addition to one in the previous section, skip updating the metrics
      case _: StartThrottlingResponse | _: EndThrottlingResponse =&gt; ()
    }

    val processor = processors.get(response.processor)
    // The processor may be null if it was shutdown. In this case, the connections
    // are closed, so the response is dropped.
    if (processor != null) {
      processor.enqueueResponse(response)
    }
  }

  /** Get the next request or block until specified time has elapsed */
  // 在过期时间之前一直阻塞获取下一个请求
  def receiveRequest(timeout: Long): RequestChannel.BaseRequest =
    requestQueue.poll(timeout, TimeUnit.MILLISECONDS)

  /** Get the next request or block until there is one */
  // 一直阻塞获取下一个请求
  def receiveRequest(): RequestChannel.BaseRequest =
    requestQueue.take()

  def updateErrorMetrics(apiKey: ApiKeys, errors: collection.Map[Errors, Integer]): Unit = {
    errors.forKeyValue { (error, count) =&gt;
      metrics(apiKey.name).markErrorMeter(error, count)
    }
  }

  def clear(): Unit = {
    requestQueue.clear()
  }

  def shutdown(): Unit = {
    clear()
    metrics.close()
  }

  def sendShutdownRequest(): Unit = requestQueue.put(ShutdownRequest)

}

</code></pre>
<h4 id="acceptor-线程">Acceptor 线程</h4>
<p>Reactor模型中，有一种重要的Dispatcher角色，主要用来接受外部请求，并进行分发操作；这个角色也被称为Acceptor。在kafka中的，每个broker端的每个SocketServer实例只会创建一个Acceptor线程，这个Acceptor线程主要用来创建连接，并分发请求给Processor线程处理。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/network/SocketServer.scala
private[kafka] class Acceptor(val endPoint: EndPoint,
                              val sendBufferSize: Int,
                              val recvBufferSize: Int,
                              nodeId: Int,
                              connectionQuotas: ConnectionQuotas,
                              metricPrefix: String,
                              time: Time,
                              logPrefix: String = &quot;&quot;) extends AbstractServerThread(connectionQuotas) with KafkaMetricsGroup {

  this.logIdent = logPrefix
  // 创建nio selector对象，用来检查一个或多个NIO Channel的状态是否处于可读、可写
  private val nioSelector = NSelector.open()
  // broker 端创建对应的socketServer channel实例，并注册到selector上。
  val serverChannel = openServerSocket(endPoint.host, endPoint.port)
  // 创建processors 线程池
  private val processors = new ArrayBuffer[Processor]()
  // processors启动标志
  private val processorsStarted = new AtomicBoolean
  // 阻塞状态记录器
  private val blockedPercentMeter = newMeter(s&quot;${metricPrefix}AcceptorBlockedPercent&quot;,
    &quot;blocked time&quot;, TimeUnit.NANOSECONDS, Map(ListenerMetricTag -&gt; endPoint.listenerName.value))
  private var currentProcessorIndex = 0
  private[network] val throttledSockets = new mutable.PriorityQueue[DelayedCloseSocket]()

  // 延迟关闭socket
  private[network] case class DelayedCloseSocket(socket: SocketChannel, endThrottleTimeMs: Long) extends Ordered[DelayedCloseSocket] {
    override def compare(that: DelayedCloseSocket): Int = endThrottleTimeMs compare that.endThrottleTimeMs
  }
  
    /**
   * Accept loop that checks for new connection attempts
   * 循环检查是否有新连接进入，如果有的话就给其分配处理器
   */
  def run(): Unit = {
    // 注册  OP_ACCEPT
    serverChannel.register(nioSelector, SelectionKey.OP_ACCEPT)
    // 等待 Acceptor启动完成
    startupComplete()
    try {
      while (isRunning) {
        try {
          // 循环检查是否有可用的新连接  
          acceptNewConnections()
          // 关闭连接
          closeThrottledConnections()
        }
        catch {
          // We catch all the throwables to prevent the acceptor thread from exiting on exceptions due
          // to a select operation on a specific channel or a bad request. We don't want
          // the broker to stop responding to requests from other clients in these scenarios.
          case e: ControlThrowable =&gt; throw e
          case e: Throwable =&gt; error(&quot;Error occurred&quot;, e)
        }
      }
    } finally {
      debug(&quot;Closing server socket, selector, and any throttled sockets.&quot;)
      CoreUtils.swallow(serverChannel.close(), this, Level.ERROR)
      CoreUtils.swallow(nioSelector.close(), this, Level.ERROR)
      throttledSockets.foreach(throttledSocket =&gt; closeSocket(throttledSocket.socket))
      throttledSockets.clear()
      shutdownComplete()
    }
  }
  
    /**
   * Listen for new connections and assign accepted connections to processors using round-robin.
   * 轮询监听并分配连接给处理器
   */
  private def acceptNewConnections(): Unit = {
    // 每500毫秒获取一次就绪的IO事件  
    val ready = nioSelector.select(500)
    // 如果存在就绪的IO事件
    if (ready &gt; 0) {
      // 获取selector中的key  
      val keys = nioSelector.selectedKeys()
      val iter = keys.iterator()
      while (iter.hasNext &amp;&amp; isRunning) {
        try {
          val key = iter.next
          iter.remove()

          // 如果key可接受数据
          if (key.isAcceptable) {
            // 针对key创建socket连接  
            accept(key).foreach { socketChannel =&gt;
              // Assign the channel to the next processor (using round-robin) to which the
              // channel can be added without blocking. If newConnections queue is full on
              // all processors, block until the last one is able to accept a connection.
              var retriesLeft = synchronized(processors.length)
              var processor: Processor = null
              do {
                retriesLeft -= 1
                // 指定处理线程processor
                processor = synchronized {
                  // adjust the index (if necessary) and retrieve the processor atomically for
                  // correct behaviour in case the number of processors is reduced dynamically
                  currentProcessorIndex = currentProcessorIndex % processors.length
                  processors(currentProcessorIndex)
                }
                // 递增当前处理器索引
                currentProcessorIndex += 1
              } while (!assignNewConnection(socketChannel, processor, retriesLeft == 0))
            }
          } else
            throw new IllegalStateException(&quot;Unrecognized key state for acceptor thread.&quot;)
        } catch {
          case e: Throwable =&gt; error(&quot;Error while accepting connection&quot;, e)
        }
      }
    }
  }

</code></pre>
<h4 id="processor-线程">Processor 线程</h4>
<p>Processor线程负责核心的逻辑处理。</p>
<p>核心参数说明：</p>
<ul>
<li>newConnections 队列: 是一个阻塞队列，主要用来保存要创建的新连接信息，也就是SocketChannel 对象，其队列长度大小为20(源码中直接硬编码了)。每当 Processor 线程接收到新的连接请求时，都会将对应的 SocketChannel 对象放入队列，等到后面创建连接时，从该队列中获取 SocketChannel，然后注册新的连接。</li>
<li>inflightResponse 队列：是一个临时的 Response 队列， 当 Processor 线程将 Repsonse 返回给 Client 之后，要将 Response 放入该队列。它存在的意义：由于有些 Response 回调逻辑要在 Response 被发送回 Request 发送方后，才能执行，因此需要暂存到临时队列。</li>
<li>ResponseQueue 队列：它主要是存放需要返回给Request 发送方的所有 Response 对象。通过源码得知：每个 Processor 线程都会维护自己的 Response 队列。</li>
</ul>
<pre><code>  // 文件地址：core/src/main/scala/kafka/network/SocketServer.scala
  private val newConnections = new ArrayBlockingQueue[SocketChannel](connectionQueueSize)
  private val inflightResponses = mutable.Map[String, RequestChannel.Response]()
  private val responseQueue = new LinkedBlockingDeque[RequestChannel.Response]()
  
  override def run(): Unit = {
    // 等待processor线程启动完成  
    startupComplete()
    try {
      while (isRunning) {
        try {
          // setup any new connections that have been queued up
          // 配置新的已就绪连接（将newConnections 里的channel取出来注册到selector中）；
          // 为了确保及时处理现有通道的流量和连接关闭的通知，每次迭代处理的连接数是有限的。
          configureNewConnections()
          // register any new responses for writing
          // selector将response通过网络发出，并将其存入inflightResponses 用作后续调用
          processNewResponses()
          // 执行nio poll，获取socketChannel上已就绪的IO事件
          poll()
          // 将接收到的请求放入request阻塞队列中
          processCompletedReceives()
          // 为了inflightResponses 中成功发送的response执行回调逻辑
          processCompletedSends()
          // 将断开连接的response从inflightResponses 中移除
          processDisconnected()
          // 关闭超过配额限制的连接
          closeExcessConnections()
        } catch {
          // We catch all the throwables here to prevent the processor thread from exiting. We do this because
          // letting a processor exit might cause a bigger impact on the broker. This behavior might need to be
          // reviewed if we see an exception that needs the entire broker to stop. Usually the exceptions thrown would
          // be either associated with a specific socket channel or a bad request. These exceptions are caught and
          // processed by the individual methods above which close the failing channel and continue processing other
          // channels. So this catch block should only ever see ControlThrowables.
          case e: Throwable =&gt; processException(&quot;Processor got uncaught exception.&quot;, e)
        }
      }
    } finally {
      debug(s&quot;Closing selector - processor $id&quot;)
      CoreUtils.swallow(closeAll(), this, Level.ERROR)
      shutdownComplete()
    }
  }

</code></pre>
<h4 id="kafkarequesthandler-核心逻辑处理器">KafkaRequestHandler 核心逻辑处理器</h4>
<p>KafkaRequestHandler 是kafka中真正的逻辑处理代码。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaRequestHandler.scala
// IO线程池
class KafkaRequestHandlerPool(val brokerId: Int,
                              val requestChannel: RequestChannel,
                              val apis: ApiRequestHandler,// 具体逻辑处理器
                              time: Time,
                              numThreads: Int,// 线程数
                              requestHandlerAvgIdleMetricName: String,
                              logAndThreadNamePrefix : String) extends Logging with KafkaMetricsGroup {
   // 可动态扩展
  private val threadPoolSize: AtomicInteger = new AtomicInteger(numThreads)
  /* a meter to track the average free capacity of the request handlers */
  private val aggregateIdleMeter = newMeter(requestHandlerAvgIdleMetricName, &quot;percent&quot;, TimeUnit.NANOSECONDS)

  // 线程数组
  this.logIdent = &quot;[&quot; + logAndThreadNamePrefix + &quot; Kafka Request Handler on Broker &quot; + brokerId + &quot;], &quot;
  val runnables = new mutable.ArrayBuffer[KafkaRequestHandler](numThreads)
  for (i &lt;- 0 until numThreads) {
    createHandler(i)
  }
  
  // 创建IO处理线程，即KafkaRequestHandler
  def createHandler(id: Int): Unit = synchronized {
    runnables += new KafkaRequestHandler(id, brokerId, aggregateIdleMeter, threadPoolSize, requestChannel, apis, time)
    KafkaThread.daemon(logAndThreadNamePrefix + &quot;-kafka-request-handler-&quot; + id, runnables(id)).start()
  }
  
// IO线程  
class KafkaRequestHandler(id: Int,
                          brokerId: Int,
                          val aggregateIdleMeter: Meter,
                          val totalHandlerThreads: AtomicInteger,
                          val requestChannel: RequestChannel,
                          apis: ApiRequestHandler,
                          time: Time) extends Runnable with Logging {
  this.logIdent = s&quot;[Kafka Request Handler $id on Broker $brokerId], &quot;
  private val shutdownComplete = new CountDownLatch(1)
  private val requestLocal = RequestLocal.withThreadConfinedCaching
  @volatile private var stopped = false

  def run(): Unit = {
    while (!stopped) {
      // We use a single meter for aggregate idle percentage for the thread pool.
      // Since meter is calculated as total_recorded_value / time_window and
      // time_window is independent of the number of threads, each recorded idle
      // time should be discounted by # threads.
      val startSelectTime = time.nanoseconds

      // 从requestChannel获取请求
      val req = requestChannel.receiveRequest(300)
      val endTime = time.nanoseconds
      // 统计线程空闲时间
      val idleTime = endTime - startSelectTime
      // 更新线程空闲百分比指标
      aggregateIdleMeter.mark(idleTime / totalHandlerThreads.get)

      req match {
        // 关闭请求  
        case RequestChannel.ShutdownRequest =&gt;
          debug(s&quot;Kafka request handler $id on broker $brokerId received shut down command&quot;)
          completeShutdown()
          return

        // 正常请求
        case request: RequestChannel.Request =&gt;
          try {
            request.requestDequeueTimeNanos = endTime
            trace(s&quot;Kafka request handler $id on broker $brokerId handling request $request&quot;)
            // 真正开始处理相关逻辑
            apis.handle(request, requestLocal)
          } catch {
            case e: FatalExitError =&gt;
              completeShutdown()
              Exit.exit(e.statusCode)
            case e: Throwable =&gt; error(&quot;Exception when handling request&quot;, e)
          } finally {
            request.releaseBuffer()
          }

        case null =&gt; // continue
      }
    }
    completeShutdown()
  }

</code></pre>
<h4 id="完整处理流程">完整处理流程</h4>
<ol>
<li>SocketServer 中的Acceptor 线程接受clients的请求。</li>
<li>Acceptor 线程会创建 NIO Selector 对象及 ServerSocketChannel 实例，并将其与 OP_ACCEPT 事件注册到 Selector 多路复用器上。</li>
<li>与此同时，Acceptor 线程还会创建默认大小为3的 Processor 线程池，（可通过broker端参数：num.network.threads进行修改） 。</li>
<li>Acceptor线程开始轮询监听，如果有新的请求对象 SocketChannel 进入，便会将其放入到连接队列中（newConnections），同时会触发一个processor线程进行处理。</li>
<li>Processor 线程向 SocketChannel 注册 OP_READ/OP_WRITE 事件，同时，轮询获取已就绪的IO事件。</li>
<li>Processor 线程会根据Channel中获取已经完成的 Receive 对象，构建 Request 对象，并将其存入到 Requestchannel 的 RequestQueue 请求队列中 。</li>
<li>KafkaRequestHandler 线程循环地从请求队列中获取 Request 实例，然后交由KafkaApis 的 handle 方法，执行真正的请求处理逻辑，并最终将数据存储到磁盘中。</li>
<li>待处理完请求后，KafkaRequestHandler 线程会将 Response 对象放入 Processor 线程的 Response 队列。</li>
<li>Processor 线程通过 Request 中的 ProcessorID 不停地从 Response 队列中来定位并取出 Response 对象，并返还给 Request 发送方。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之可用性与持久性保证]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-ke-yong-xing-yu-chi-jiu-xing-bao-zheng/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-ke-yong-xing-yu-chi-jiu-xing-bao-zheng/">
        </link>
        <updated>2022-06-18T08:44:35.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka可用性与持久化保证。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/lake-71208_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="kafka体系架构">kafka体系架构</h2>
<h3 id="总览">总览</h3>
<p>标准的kafka集群中一般包含多个broker，若干个producer，若干个consumer，以及一个zookeeper集群。</p>
<p>Kafka 中的message以topic为单位进行归类，producer负责将消息发送到特定的topic（发送到 Kafka 集群中的每一条消息都要指定一个topic），而consumer负责订阅topic并进行消费。</p>
<p>topic只是一个逻辑上的概念，每个主题都会被划分为多个partition，partition是实际存在的存储介质，消息在单个partition中有序。</p>
<p>架构图如下所示：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315004.png" alt="img" loading="lazy"></figure>
<h3 id="消息传递语义message-delivery-semantics">消息传递语义（Message Delivery Semantics）</h3>
<p>关于producer与consumer之间消息传递保证，kafka提供了如下三种语义（需要注意的是，此过程分为发布消息的持久化保证及消费消息的保证）：</p>
<ul>
<li>At most once：消息可能会丢失，但绝不会重传。</li>
<li>At least once：消息绝不会丢失，但可能会重传。</li>
<li>Exactly once：每条消息会传递一次且仅有一次，这正是用户真正所需要的。</li>
</ul>
<h4 id="发送消息持久化保证">发送消息持久化保证</h4>
<p>针对发送消息的持久化保证：在 kafka-0.11.0.0 之前，如果生产者没有收到表明消息已提交的响应，那么消息将会被重发，这仅提供了At least once传递语义，因为如果原始请求实际上已经成功，则在重新发送期间消息可能会再次写入日志。</p>
<p>在 kafka-0.11.0.0 之后，Kafka 生产者开始支持幂等交付选项，以保证重新发送不会导致日志中出现重复条目。 为实现这个目标，broker会为每个生产者分配一个 ID，并使用生产者与每条消息一起发送的序列号对消息进行去重操作。</p>
<p>同样从 kafka-0.11.0.0 开始，生产者支持使用类似事务(transaction-like)的语义将消息发送到多个主题分区的能力；即，所有消息都已成功写入，或者都没有（原子性）。</p>
<h4 id="消费消息的保证">消费消息的保证</h4>
<p>关于consumer读取消息时，该如何处理消息和更新位置的几个场景如下：</p>
<ol>
<li>consumer可以读取消息，然后将其位置保存在日志中，最后处理消息。在这种情况下，消费者进程可能会在保存其位置之后但在保存其消息处理的输出之前崩溃，而接管处理的进程却会从保存的位置开始，即使该位置之前的一些消息尚未处理。 这对应于“At most once”语义，因为在消费者失败的情况下，消息可能不会被处理。</li>
<li>consumer可以读取消息，处理消息，最后保存它的位置。 在这种情况下，消费者进程可能会在处理消息之后但在保存其位置之前崩溃；这样的话，当一个新进程接管它收到的前几条消息时，这些消息实际已经被处理了。这对应于消费者失败情况下的“At least once”语义。 在许多情况下，消息有一个主键，因此更新是幂等的（两次接收相同的消息只会用另一个自身的副本覆盖记录）。</li>
<li>关于如何保证“Exactly once”，当从 Kafka 主题消费并生产到另一个主题时（如在 Kafka Streams 应用程序中），可以利用上面提到的 kafka-0.11.0.0 中新的事务生产者功能。消费者的位置作为消息存储在主题中，与此同时，在和接收处理数据的输出主题相同的事务中将偏移量写入 Kafka。 如果传输中止，消费者的位置将恢复到其旧值，并且输出主题的生成数据将不会对其他消费者可见，当然，这具体取决于他们的“隔离级别”。 在默认的“read_uncommitted”隔离级别中，所有消息对消费者都是可见的，即使它们是中止事务的一部分，但在“read_committed”中，消费者将只返回来自已提交事务的消息（以及任何非事务消息）。</li>
<li>写入外部系统时，限制在于需要将消费者的位置与实际存储为输出的内容相协调。实现这一点的经典方法是在存储消费者位置和输出之间引入两阶段提交。但是通过让消费者将其偏移量与其输出存储在同一位置可以让其中的处理更简单与更通用化。这种方式是比较友好的，因为消费者可能想要写入的许多输出系统不支持两阶段提交。举个例子，Kafka Connect 连接器，它将所读取的数据和数据的 offset 一起写入到 HDFS，以保证数据和 offset 都被更新，或者两者都不被更新。</li>
</ol>
<h3 id="副本replication">副本（Replication）</h3>
<p>kafka为每个主题的分区提供了备份副本功能（可在服务端进行相关配置）；当集群中的某个服务宕机时，副本能够自动进行故障转移，以保证数据的可用性（不丢失，提供容灾能力）。</p>
<p>创建副本的单位是 topic 的 partition ，正常情况下，每个分区都有一个 leader 和零或多个 followers 。总的副本数包含 leader及followers，被称为副本因子。所有的读写操作都由 leader 处理。一般 partition 的数量都比 broker 的数量多的多，各分区的 leader 均匀的分布在 brokers 中。所有的 followers 节点都同步 leader 节点的日志，日志中的消息和偏移量都和 leader 中的一致。（当然，在任何给定时间，leader 节点的日志末尾时可能有几个消息尚未被备份完成）。</p>
<p>如下图所示，Kafka 集群中有4个 broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个 leader 副本和2个 follower 副本。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315005.png" alt="img" loading="lazy"></figure>
<h3 id="选举算法leader-election">选举算法（Leader Election）</h3>
<p>kafka分区的核心便是副本日志（replicated log），这同样也是分布式系统中最重要的基础元素之一（容灾）。</p>
<p>副本日志按照一系列有序的值（通常是编号为 0、1、2、…) 进行建模。有很多方法可以实现这一点，但最简单和最快的方法是由 leader 节点选择需要提供的有序的值，只要 leader 节点还存活，所有的 follower 只需要拷贝数据并按照 leader 节点的顺序排序。</p>
<p>理想情况中，如果leader永远存活，那么也就不需要follower了；然而实际生产环境中依旧可能出现机器宕机的场景（随着系统越大，机器越多，概率也将越高）；因此，为了应对这种情形，需要从存活的followers中选择一个新的leader，但是followers本身数据可能会落后于leader或者crash掉，在这种情况下，有必要选择数据最全的follower（up-to-date follower）作为新的leader。</p>
<p>在这个过程中，有一个基本原则：一条数据如果明确返回给客户端为已提交（committed），则当leader crash掉后出现的新leader必须拥有刚刚已提交过的所有消息。这时候便需要做出权衡：如果leader在表明一条消息已提交（committed）前等待更多的follower进行确认，那么leader宕机后便有更多的follower可以作为新的leader，但与此同时吞吐率也会有所下降。</p>
<p>一种非常常见的leader election算法便是“Majority Vote”。在这种模式下，假设拥有2f+1个副本（包含leader和follower），如果需要确保f+1个副本收到消息，且在这f+1个副本中选择一个作为新的leader，那么fail的副本不能超过f个。这是因为在f+1个副本中，任意一个都拥有最新的所有备份数据。</p>
<p>“Majority Vote”有一个非常好的特性：那就是系统的延迟只取决于最快的服务器；当然其劣势便是需要更多的副本来保证数据的完整性（所能允许fail的副本相对太少）。例如：如果要容忍1个follower宕机，那便需要3个及以上副本；如果要容忍2个follower宕机，那便需要5个及以上副本；也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降，且这种方式对于资源也过于浪费，毕竟生产资源很珍贵。一般情况下，这种算法用于Zookeeper之类的共享集群配置的系统中而很少在需要存储大量数据的系统中使用。</p>
<p>实际上，Leader Election算法非常多，比如Zookeeper的Zab, Raft和Viewstamped Replication。而Kafka所使用的Leader Election算法更像微软的PacificA算法。</p>
<p>kafka采用的选举算法与“Majority Vote”有所不同，它会在zookeeper中动态的维护一组ISR，这些副本基本与leader保持一致（未同步的副本将会被踢出ISR），一般情况下只有ISR中的副本才有机会成为新的leader。</p>
<p>在这种模式下，对于f+1个副本，一个分区能在保证不丢失已经committed的消息的前提下容忍f个副本发生故障。在大多数应用场景中，这种模式是非常有利的。而在实际中，为了冗余f个副本发生故障，Majority Vote和ISR在commit前需要等待的Replica数量是一样的（例如在一次故障恢复中，“Majority Vote”的 quorum 需要三个备份节点和一次确认，而ISR 需要两个备份节点和一次确认），但是ISR需要的总副本数几乎是Majority Vote的一半。</p>
<p>关于kafka另一个重要的特性设计便是其并不需要崩溃节点在拥有完好无损的数据下进行恢复，简而言之，kafka允许丢失部分数据（数据不一致性）。</p>
<p>关于分布式系统中可用性与一致性之间的权衡，往往取决于实际的业务场景需求（如资金类业务需要强一致性，用户活动跟踪只需保证可用性等）。</p>
<p>当kafka中的发生了all replica die时，它提供了两种恢复性方案（可用性与一致性抉择，不止kafka有此等困境，其余框架一样会遇到这种艰难的选择）：</p>
<ol>
<li>
<p>等待 ISR 中的副本恢复并选择此副本作为新的leader（希望它仍然拥有所有数据）。</p>
</li>
<li>
<p>选择第一个恢复过来的副本作为leader（不一定在 ISR 中）。</p>
<p>第一种方案中，如果等待 ISR 中副本恢复，那么只要这些副本一直处于宕机状态，kafka将始终不可用。如果ISR副本被破坏或它们的数据丢失，kafka便彻底宕机了。</p>
<p>第二种方案中，如果一个非同步副本从宕机状态中恢复并且允许它成为leader，那么它的数据将成为新的可信任来源，即使它不能保证拥有每条已提交的消息。</p>
<p>默认情况下，从 kafka-0.11.0.0 版本开始，Kafka 选择第一个策略并倾向于等待一致的副本。可以使用配置属性 unclean.leader.election.enable 更改此行为，以支持正常运行时间优于一致性的用例。</p>
</li>
</ol>
<h3 id="节点存活与ack机制可用性与持久性">节点存活与ack机制（可用性与持久性）</h3>
<p>kafka为节点定义了一个 “in sync” 状态，区别于 “alive” 和 “failed” 。Leader 会追踪所有 “in sync” 的节点。如果有节点挂掉了，或是写超时，或是心跳超时，leader 就会把它从同步副本列表（in sync replicas，缩写为ISR）中移除。同步超时和写超时的时间由 replica.lag.time.max.ms 配置确定。</p>
<p>kafka定义节点处于“in sync”状态的的条件如下：</p>
<ul>
<li>节点必须维护和 ZooKeeper 的会话连接，Zookeeper 可以通过心跳机制来检查每个节点的连接。</li>
<li>如果是 follower 节点，它必须能及时的同步 leader 的写操作，并且延时不能太久。</li>
</ul>
<p>分布式系统中，kafka只尝试处理 “fail/recover” 模式的故障，即节点突然停止工作，然后又恢复（节点可能不知道自己曾经挂掉）的状况。Kafka 没有处理所谓的 “Byzantine” 故障，即一个节点出现了随意响应和恶意响应（可能由于 bug 或 非法操作导致）。</p>
<p>当分区上的所有ISR都将消息保存至log中时，该消息可以被视为已提交，只有已提交的消息才会被消费者所消费。这意味着消费者不必担心当leader fail时，可能会看到丢失的消息。</p>
<p>另一方面，producer可以选择等待或不提交消息，这取决于他们对延迟和持久性之间的权衡，此功能由producer中的ACK机制实现。请注意，主题具有ISR的“最小数量”设置，当生产者请求确认消息已写入完整的ISR集时，会检查该设置。如果生产者请求不那么严格的确认，则可以提交和消费消息，即使同步副本的数量低于最小值（例如，它可以低至仅leader）。</p>
<p>在任何时候，只要至少有一个同步中的节点存活，kafka中的消息就不会丢失；这保证了在短暂的故障转移后，kafka仍具有可用性，不过在网络分区的场景中，可能无法保持可用。</p>
<p>关于ack机制如下：</p>
<ul>
<li>acks = 0，如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障）。此配置可以获得最高的吞吐量。</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315006.png" alt="img" loading="lazy"></figure>
<ul>
<li>acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应；一旦消息无法写入leader分区副本(比如网络原因、leader节点崩溃),生产者会收到一个错误响应，当生产者接收到该错误响应之后，为了避免数据丢失，会尝试重新发送数据（前提是配置了重试次数），这种方式的吞吐量取决于使用的是异步发送还是同步发送。在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录依旧会丢失。</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315007.png" alt="img" loading="lazy"></figure>
<ul>
<li>acks = all 这意味着leader将等待完整的同步副本（ISR）集以确认记录（此时如果ISR同步副本的个数小于min.insync.replicas的值，消息将不会被写入.），这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，相当于acks = -1的设置。此配置最安全，但延迟相对来说也最高。</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315008.png" alt="img" loading="lazy"></figure>
<p>关于最小同步副本配置：</p>
<ul>
<li>Kafka的Broker端提供了一个参数min.insync.replicas，它明确指定了数据要被同步到多少个副本才算真正的成功写入，此参数一般与acks=all配合使用，可以获得更好的持久性保证。该值默认为1，生产环境设定为一个大于1的值可以提升消息的持久性（例如复制因子为3则可以设置该参数为2）。 因为如果同步副本的数量低于该配置值，则生产者会收到错误响应，从而确保消息不丢失。</li>
<li>当min.insync.replicas=2且acks=all时，如果此时ISR列表只有[1,2],3被踢出ISR列表，只需要保证两个副本同步了，生产者就会收到成功响应。</li>
</ul>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315009.png" alt="img" loading="lazy"></figure>
<ul>
<li>当min.insync.replicas=2，如果此时ISR列表只有[1],2和3被踢出ISR列表，那么当acks=all时，则不能成功写入数；当acks=0或者acks=1可以成功写入数据。</li>
</ul>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315010.png" alt="img" loading="lazy"></figure>
<ul>
<li>如果acks=all且min.insync.replicas=2，此时ISR列表为[1,2,3]，这种情况下kafka还是会等到所有的同步副本都同步了消息，才会向生产者发送成功响应的ack。因为min.insync.replicas=2只是一个最低限制，即同步副本少于该配置值，才会抛异常，而acks=all，是需要保证所有的ISR列表的副本都同步了才可以发送成功响应。</li>
</ul>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315011.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[消息队列与kafka]]></title>
        <id>https://philosopherzb.github.io/post/xiao-xi-dui-lie-yu-kafka/</id>
        <link href="https://philosopherzb.github.io/post/xiao-xi-dui-lie-yu-kafka/">
        </link>
        <updated>2022-06-11T05:32:03.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述消息队列以及kafka整体架构。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/mountain-547363_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="消息队列message-queue">消息队列(message queue)</h2>
<h3 id="简介">简介</h3>
<p>消息队列是一种进程间通信或同一进程的不同线程间的通信方式；针对如今微服务或云架构，则指代服务间的异步通信方式。</p>
<p>在现代云架构或微服务架构中，应用程序被分解为多个规模较小且易于开发、部署和维护的构建块。消息队列可以为这些分布式应用程序提供通信与协调；同时，消息队列也可以显著的简化分离应用编码，并提高性能、可靠性和可扩展性等。</p>
<h3 id="消息投递模式">消息投递模式</h3>
<h4 id="端到端point-to-point">端到端(Point-to-point)</h4>
<p>在端到端消息模式中，一条消息将会一直存储在队列中，直到被消费者所接收；且，发送者必须要知道消费者的一些关键信息，例如：将要发送消息的队列名或者特定的队列管理器名。</p>
<p>此模式中，消息队列将会提供一个临时存储消息的轻量级缓冲区，并允许微服务连接到队列以发送和接收消息的终端节点；这种情况下，消息的规模一般较小，如请求，恢复，错误消息及明文消息等；同时一条消息只能由一个接收者处理一次。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314008.png" alt="img" loading="lazy"></figure>
<h4 id="发布订阅publishsubscribe">发布订阅(Publish/Subscribe)</h4>
<p>发布订阅模式中，一条消息由发送者推送至特定的主题(topic)，随后所有订阅了该topic的消费者都将收到同一条广播消息。</p>
<p>消息主题的订阅者通常会执行不同的功能，并可以同时对消息执行不同的操作。发布者无需知道谁在使用广播的信息，而订阅者也无需知道消息来自哪里。这种消息收发模式与端到端稍有不同，在端到端中，发送消息的组件通常知道发送的目的地。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314009.png" alt="img" loading="lazy"></figure>
<h3 id="消息队列优势">消息队列优势</h3>
<ul>
<li>解耦：在分布式系统中，同一份数据可能要分发给不同的程序进行处理；例如订单生成后，要扣库存，加积分；此时如果由订单服务直接调用库存服务及积分服务的接口，将会让订单服务变得十分臃肿，且不利于后续扩展（比如再加一个支付服务）。这时便可以加入消息队列，采用发布订阅模式，订单服务只需将消息发送至指定topic即可，其他服务选择性订阅。这样便将系统业务进行了解耦操作，增加了系统的可扩展性。</li>
<li>异步通信：消息队列天然支持异步操作（所有的消息都会存储在队列中，消费者可以延迟处理），针对分布式系统中的异步操作，可采用消息队列进行处理。</li>
<li>削峰填谷：针对高并发大数据的场景，消息队列可以有效地做到削峰填谷。数据推送高峰时，将由消息队列暂存所有的数据，等到数据推送低谷时，由消费者逐步消费所有的数据。必要时，也可以额外的增加消费者进行数据处理。</li>
<li>缓冲：消息队列通过一个缓冲层来帮助任务最高效率的执行；写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。</li>
<li>跨平台：消息队列支持跨平台消息处理，只需要发送者及消费者约定数据格式即可。例如，由Java发送消息，go消费消息。</li>
<li>灵活性与可扩展性：在高流量期间，可以通过动态的扩展机器来接受更多的消息，防止高并发冲击服务，导致服务挂掉。</li>
</ul>
<h2 id="kafka">kafka</h2>
<h3 id="简介-2">简介</h3>
<p>Kafka 是由 Linkedin 公司开发的，它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的消息流平台，它同时也是一款开源的基于发布订阅模式的消息引擎系统；</p>
<p>Kafka由服务端及客户端组成，且服务端和客户端可以通过高性能的TCP网络协议进行通讯。关于kafka的部署环境，无论本地还是云环境中的裸机，虚拟机或者容器都可以支持。</p>
<h3 id="基本术语">基本术语</h3>
<ul>
<li>
<p>生产者(Producer)：向kafka中的主题(topic)发布消息事件的客户端应用程序被称为生产者。</p>
</li>
<li>
<p>消费者(Consumer)：订阅了kafka消息事件所在的主题(topic)的客户端应用程序称为消费者。</p>
</li>
<li>
<p>主题(Topic)：用于存储同一类消息事件；打个简单的比喻：主题类似于文件系统中的文件夹，而消息事件则类似于文件夹中的文件。</p>
</li>
<li>
<p>消息(Message)：kafka中的数据单元被称为消息，也可以叫记录(record)。</p>
</li>
<li>
<p>偏移量(Offset)：是一种元数据，且是一个不断递增的整数值；当消费者处理完消息后，将会提交offset+1到broker中。</p>
</li>
<li>
<p>broker： 一个独立的 Kafka 服务器就被称为 broker，broker 接收来自生产者的消息，并为消息设置偏移量，同时持久化消息到磁盘。</p>
</li>
<li>
<p>分区(Partition)：在一个主题中可以存在一个或多个分区，每个分区中消息有序；同一主题中的多个分区可以位于不同机器上，方便后续扩容操作。</p>
</li>
<li>
<p>副本(Replica)：消息的备份称为副本，在创建主题时可以指定副本数量。</p>
</li>
<li>
<p>重平衡(Rebalance)：当消费组(多个消费者将会形成一个消费组)中的某个消费者实例挂掉或者新增一个消费者实例时，其他消费者实例可以自动地重新分配订阅主题，这个过程叫重平衡。</p>
</li>
<li>
<p>AR与ISR：AR即可分配副本 Assigned Replicas；而所有与leader副本保持一定同步状态的副本（包含leader副本）构成ISR（In-Sync Replicas）；ISR集合是AR集合中的一个子集。</p>
</li>
<li>
<p>ISR的伸缩：leader 副本负责维护和跟踪 ISR 集合中所有 follower 副本的滞后状态，当 follower 副本落后太多或失效时，leader 副本会把它从 ISR 集合中剔除。如果 OSR（Out-Sync Replicas） 集合中有 follower 副本“追上”了 leader 副本，那么 leader 副本会把它从 OSR 集合转移至 ISR 集合。默认情况下，当 leader 副本发生故障时，只有在 ISR 集合中的副本才有资格被选举为新的 leader，而在 OSR 集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变）。</p>
<p>replica.lag.time.max.ms ： 这个参数的含义是 Follower 副本能够落后 Leader 副本的最长时间间隔，默认10s。</p>
<p>unclean.leader.election.enable：是否允许 Unclean 领导者选举。开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。</p>
</li>
<li>
<p>HW与LW：HW（High Watermark）俗称高水位，它标识了一个特定的偏移量(offset)，消费者只能拉取此偏移量之前的数据；LW（Low Watermark）俗称低水位，它同样标识了一个特定的偏移量(offset)，一般为AR集合中最小的LSO值；LW值的增长与副本的拉取或删除请求息息相关。</p>
</li>
<li>
<p>LSO：LSO 是LogStartOffset的缩写，一般情况下，日志文件的起始偏移量 logStartOffset 等于第一个日志分段的 baseOffset，但这并不是绝对的，logStartOffset 的值可以通过 DeleteRecordsRequest 请求(比如使用 KafkaAdminClient 的 deleteRecords()方法、使用 kafka-delete-records.sh 脚本、日志的清理和截断等操作）进行修改。</p>
</li>
<li>
<p>LEO：LEO是 LogEndOffset 的缩写，它标识当前日志文件中下一条待写入消息的 offset，如下图中 offset 为9的位置即为当前日志文件的 LEO，LEO 的大小相当于当前日志分区中最后一条消息的 offset 值加1。分区 ISR 集合中的每个副本都会维护自身的 LEO，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息</p>
</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314010.png" alt="img" loading="lazy"></figure>
<ul>
<li>发送者，broker集群，消费者基本协作结构图</li>
</ul>
<p><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314011.png" alt="img" loading="lazy"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314011.png" alt="" loading="lazy"></p>
<ul>
<li>主题剖析结构图</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315001.png" alt="img" loading="lazy"></figure>
<ul>
<li>一个两节点的 kafka 集群支持的 2 个消费组的四个分区 (P0-P3)。消费者 A 有两个消费者实例，消费者 B 有四个消费者实例。</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315002.png" alt="img" loading="lazy"></figure>
<h3 id="基本特性">基本特性</h3>
<ul>
<li>高吞吐、低延迟：收发消息快是kafka最重要的特性之一，即使在非常廉价的机器上，kafka也可以轻松达到每秒几十万条消息的传输速率，且此过程中最低延迟仅有几毫秒；一般可以用来做实时日志聚合。</li>
<li>持久化：kafka中的所有消息都会被其持久化存储在文件系统中；在这个过程中，kafka采用顺序写磁盘及直接写页缓存的机制提高IO效率（顺序写磁盘的速度接近于随机写内存的速度：<a href="https://queue.acm.org/detail.cfm?id=1563874">点击此处跳转文章页面</a>）。</li>
<li>水平扩容(Scale out)：kafka提供分区机制以达到动态扩容的效果，即增加分区数（不支持减少分区操作）；</li>
<li>分区容错(Partition-tolerance)：即使集群中的某个节点挂掉，kafka仍然可以提供可用性（可用性与一致性间的冲突，kafka目前采用的是ISR机制进行了部分妥协）。</li>
<li>核心API：Producer API，Consumer API，Streams API，Connect API，Admin API</li>
</ul>
<h3 id="生产者producer">生产者(Producer)</h3>
<h4 id="简介-3">简介</h4>
<p>生产者无需经过路由便可将消息发送至主分区所在的服务器上；为了实现这个功能，所有的kafka服务器节点都能够响应这样的元数据请求：哪些服务器存活，主题的主分区位于哪台服务器上。</p>
<p>对于生产者而言，只需要执行发送请求，消息将会自动根据路由规则分配到不同的分区上。</p>
<p>消息路由规则：如果指定了 partition，则直接使用；如果未指定 partition 但指定了 key，则通过对 key 的 value 进行hash 选出一个 partition（形如：Math.abs(key.hashCode()) % partitions.size()）； 如果partition 和 key 都未指定，便会轮询选出一个 partition。</p>
<h4 id="整体流程架构">整体流程架构</h4>
<p>整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。</p>
<p>在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。</p>
<p>Sender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。</p>
<p>RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315003.png" alt="img" loading="lazy"></figure>
<h5 id="序列化器">序列化器</h5>
<p>生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka；在消费者侧同样需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换成相应的对象。网络传输过程中，序列化是必须的操作。自定义时需要实现org.apache.kafka.common.serialization.Serializer</p>
<pre><code>package org.apache.kafka.common.serialization;

import org.apache.kafka.common.header.Headers;

import java.io.Closeable;
import java.util.Map;

/**
 * 一个用于转换对象为字节的接口
 *
 * 实现此接口的类应具有不带参数的构造函数
 * 
 * @param &lt;T&gt; Type to be serialized from.
 */
public interface Serializer&lt;T&gt; extends Closeable {

    /**
     * 配置当前类，此方法一般是在创建 KafkaProducer 实例的时候调用的，主要用来确定编码类型。
     * @param configs configs in key/value pairs
     * @param isKey whether is for key or value
     */
    default void configure(Map&lt;String, ?&gt; configs, boolean isKey) {
        // intentionally left blank
    }

    /**
     * 转换数据为字节数组（执行序列化操作）
     * 可以进行编解码，如果 Kafka 客户端提供的几种序列化器都无法满足应用需求，
     * 则可以选择使用如 Avro、JSON、Thrift、ProtoBuf 和 Protostuff 等通用的序列化工具来实现，
     * 或者使用自定义类型的序列化器来实现。  
     *
     * @param topic topic associated with data
     * @param data typed data
     * @return serialized bytes
     */
    byte[] serialize(String topic, T data);

    /**
     * 转换数据为字节数组（执行序列化操作）
     * 可以进行编解码，如果 Kafka 客户端提供的几种序列化器都无法满足应用需求，
     * 则可以选择使用如 Avro、JSON、Thrift、ProtoBuf 和 Protostuff 等通用的序列化工具来实现，
     * 或者使用自定义类型的序列化器来实现。  
     *
     * @param topic topic associated with data
     * @param headers headers associated with the record
     * @param data typed data
     * @return serialized bytes
     */
    default byte[] serialize(String topic, Headers headers, T data) {
        return serialize(topic, data);
    }

    /**
     * 关闭序列化器
     * 注意：此方法必须是幂等的，因为它可能会被多次调用。
     */
    @Override
    default void close() {
        // intentionally left blank
    }
}

</code></pre>
<h5 id="分区器">分区器</h5>
<p>分区器的作用就是为消息分配partition。如果消息 ProducerRecord 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值；或者依赖轮询分配partition（可参考消息路由规则）。Kafka 中提供的默认分区器是 org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了 org.apache.kafka.clients.producer.Partitioner 接口。</p>
<pre><code>package org.apache.kafka.clients.producer.internals;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.utils.Utils;

import java.util.Map;

/**
 * 默认的分区策略器
 * &lt;ul&gt;
 * &lt;li&gt;如果记录中指定了分区，则直接使用
 * &lt;li&gt;如果未制定分区，但存在key，则基于key的hash值选择一个分区（拥有相同 key 的消息会被写入同一个分区，前提是分区数不会变更）
 * &lt;li&gt;如果分区和key都不存在，则选择在批处理已满时更改的粘性分区（轮询操作）。
 * 
 * See KIP-480 for details about sticky partitioning.
 */
public class DefaultPartitioner implements Partitioner {

    private final StickyPartitionCache stickyPartitionCache = new StickyPartitionCache();

    public void configure(Map&lt;String, ?&gt; configs) {}

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param key The key to partition on (or null if no key)
     * @param keyBytes serialized key to partition on (or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        return partition(topic, key, keyBytes, value, valueBytes, cluster, cluster.partitionsForTopic(topic).size());
    }

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param numPartitions The number of partitions of the given {@code topic}
     * @param key The key to partition on (or null if no key)
     * @param keyBytes serialized key to partition on (or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster,
                         int numPartitions) {
        if (keyBytes == null) {
            return stickyPartitionCache.partition(topic, cluster);
        }
        // 基于key 的hash值选择一个翻去
        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
    }

    public void close() {}
  
    /**
     * 轮询选择分区
     */
    public void onNewBatch(String topic, Cluster cluster, int prevPartition) {
        stickyPartitionCache.nextPartition(topic, cluster, prevPartition);
    }
}

</code></pre>
<pre><code>package org.apache.kafka.clients.producer;

import org.apache.kafka.common.Configurable;
import org.apache.kafka.common.Cluster;

import java.io.Closeable;

/**
 * 分区器接口，自定义分区器时，需要实现此接口。
 */
public interface Partitioner extends Configurable, Closeable {

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param key The key to partition on (or null if no key)
     * @param keyBytes The serialized key to partition on( or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes The serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);

    /**
     * 关闭分区器时执行相关操作
     */
    public void close();


    /**
     * 通知分区器有一个已创建的新批处理数据。当时用粘性分区器时，次方法会自动为新批处理数据选择一个粘性分区。
     * @param topic The topic name
     * @param cluster The current cluster metadata
     * @param prevPartition The partition previously selected for the record that triggered a new batch
     */
    default public void onNewBatch(String topic, Cluster cluster, int prevPartition) {
    }
}

</code></pre>
<h5 id="拦截器">拦截器</h5>
<p>生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。消费者拦截器主要在消费到消息或在提交消费位移时进行一些定制化的操作。自定义时需要实现org.apache.kafka.clients.producer. ProducerInterceptor</p>
<pre><code>package org.apache.kafka.clients.producer;

import org.apache.kafka.common.Configurable;

/**
 * 一个插件接口，允许拦截（并可能改变）生产者发布到kafka集群中的记记录
*/
public interface ProducerInterceptor&lt;K, V&gt; extends Configurable {
    /**
     * 将消息序列化及分区（如果分区未指定）前对消息进行相关定制化处理。
     * @param record the record from client or the record returned by the previous interceptor in the chain of interceptors.
     * @return producer record to send to topic/partition
     */
    public ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; record);

    /**
     * 当发送到服务器的记录已被确认，或者在发送到服务器之前发送记录失败时调用此方法。
     * 此方法通常在调用用户callback之前调用
     * 此方法通常运行在 Producer 的 background I/O线程中，因此这个方法中的实现代码逻辑越简单越好，否则会影响消息的发送速度。
     * 
     * This method will generally execute in the background I/O thread, so the implementation should be reasonably fast.
     * Otherwise, sending of messages from other threads could be delayed.
     *
     * @param metadata The metadata for the record that was sent (i.e. the partition and offset).
     *                 If an error occurred, metadata will contain only valid topic and maybe
     *                 partition. If partition is not given in ProducerRecord and an error occurs
     *                 before partition gets assigned, then partition will be set to RecordMetadata.NO_PARTITION.
     *                 The metadata may be null if the client passed null record to
     *                 {@link org.apache.kafka.clients.producer.KafkaProducer#send(ProducerRecord)}.
     * @param exception The exception thrown during processing of this record. Null if no error occurred.
     */
    public void onAcknowledgement(RecordMetadata metadata, Exception exception);

    /**
     * 用于关闭拦截器时执行逻辑，例如清理相关资源等
     */
    public void close();
}

</code></pre>
<h5 id="处理顺序">处理顺序</h5>
<p>拦截器-&gt;序列化器-&gt;分区器；KafkaProducer 在将消息序列化和计算分区之前会调用生产者拦截器的 onSend() 方法来对消息进行相应的定制化操作。然后生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka。最后可能会被发往分区器为消息分配分区。</p>
<h3 id="消费者consumer">消费者(Consumer)</h3>
<h4 id="简介-4">简介</h4>
<p>kafka consumer 订阅感兴趣的主题，同时向其所在的主分区发送fetch请求，获取需要进行消费的消息；需要注意的是，此过程中，consumer的每个请求都需要在partition中指定offset，从而消费从offset处开始的message。因此，consumer对offset的控制便尤为重要，可以依此来进行回退重消费操作。</p>
<h4 id="关于offset">关于offset</h4>
<p>大多数消息系统都在 broker 上保存被消费消息的元数据。也就是说，当消息被传递给 consumer，broker 要么立即在本地记录该事件，要么等待 consumer 的确认后再记录。这是一种相当直接的选择，而且事实上对于单机服务器来说，也没其它地方能够存储这些状态信息。</p>
<p>由于大多数消息系统用于存储的数据结构规模都很小，所以这也是一个很实用的选择，因为只要 broker 知道哪些消息被消费了，就可以在本地立即进行删除，一直保持较小的数据量。</p>
<p>然而，此过程中要一直保持broker与consumer的数据一致性不是一件容易的事；例如consumer已消费了数据，但是回执给broker时出现网络错误导致broker没有删除本地记录，这时消息便有可能会重复消费；并且这种记录操作对于broker而言也是一个不小的性能负担（首先对其加锁，确保该消息只被发送一次，然后将其永久的标记为 consumed，以便将其移除）。</p>
<p>为了应对上述问题，Kafka 使用了完全不同的方式来解决消息丢失问题。</p>
<p>首先Kafka 的 topic 被分割成了一组完全有序的 partition，其中每一个 partition 在任意给定的时间内只能被每个订阅了这个 topic 的 consumer group中的一个 consumer 消费。这意味着 partition 中 每一个 consumer 的位置仅仅是一个数字，即下一条要消费的消息的 offset。这使得被消费的消息的状态信息相当少，每个 partition 只需要一个数字。这个状态信息还可以作为周期性的 checkpoint。这以非常低的代价实现了和消息确认机制等同的效果。</p>
<p>这种方式还有一个附加的好处，那就是consumer 可以回退到之前的 offset 来再次消费之前的数据，这个操作违反了队列的基本原则，但事实证明对大多数 consumer 来说这是一个必不可少的特性。 例如，如果 consumer 的代码有 bug，并且在 bug 被发现前已经有一部分数据被消费了，那么 consumer 可以在 bug 修复后通过回退到之前的 offset 来再次消费这些数据。需要注意的是，对于重复消费的消息要保持幂等操作。</p>
<h4 id="push-vs-pull">Push vs Pull</h4>
<p>关于消息的推送拉取，kafka采用的是：Producer push数据到broker，Consumer从broker pull数据。针对消费者而言，无论是pull-based还是push-based都有各自的优缺点。</p>
<p>pull-based：数据传输速率由Consumer控制，可防止服务因大量数据冲击而宕机，同时也简化了broker的设计；且，此模式中，Producer可以达到最大化量产消息。当然，其缺点就是如果 broker 中没有数据，consumer 可能会在一个紧密的循环中结束轮询，实际上却是在忙于等待数据的到达。为了避免 busy-waiting，kafka在 pull 请求中加入参数，使得 consumer 在一个“long pull”中阻塞等待，直到数据到来（还可以选择等待给定字节长度的数据来确保传输长度）。</p>
<p>push-based：消息能最快的被指定消费者所消费，但是相应的broker设计将会更复杂，且数据传输速率过大时，容易冲击消费者服务，导致其宕机无法提供服务。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka安装&使用&配置]]></title>
        <id>https://philosopherzb.github.io/post/kafka-an-zhuang-andshi-yong-andpei-zhi/</id>
        <link href="https://philosopherzb.github.io/post/kafka-an-zhuang-andshi-yong-andpei-zhi/">
        </link>
        <updated>2022-06-04T08:42:31.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka安装，使用以及spring-kafka配置信息。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/lake-6278825_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="win10下部署kafka">Win10下部署kafka</h2>
<p>主要涉及Java，zookeeper，kafka，步骤如下。</p>
<h3 id="java安装">Java安装</h3>
<p>JDK下载：<a href="https://www.oracle.com/java/technologies/javase-downloads.html">点此此处跳转官网下载页面</a></p>
<p>安装过程比较简单，基本都是下一步即可，唯一需要注意的是环境变量的设置；安装完后效果如下图所示（注意：Java安装路径不要太深（目录过多），同时文件夹命名不要存在空格，否则将导致kafka部署失败）</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314001.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314002.png" alt="img" loading="lazy"></figure>
<h3 id="zookeeper安装">zookeeper安装</h3>
<p>下载地址：<a href="https://zookeeper.apache.org/releases.html">点击此处跳转官网下载页面</a></p>
<p>找一个稳定的版本下载即可：</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314003.png" alt="img" loading="lazy"></figure>
<ol>
<li>下载后解压到一个目录：eg: D:\Program\zookeeper\zookeeper-3.4.14</li>
<li>在zookeeper-3.4.14目录下，新建两个文件夹，并命名(eg: data,log)，(路径：D:\Program\zookeeper\zookeeper-3.4.14\data,D:\Program\zookeeper\zookeeper-3.4.14\log)</li>
<li>进入Zookeeper设置目录，eg: D:\Program\zookeeper\zookeeper-3.4.14\conf；复制“zoo_sample.cfg”副本并将副本重命名为“zoo.cfg”；在任意文本编辑器（eg：记事本）中打开zoo.cfg；找到并编辑dataDir=D:\Program\zookeeper\zookeeper-3.4.14\data；dataLogDir=D:\Program\zookeeper\zookeeper-3.4.14\log</li>
<li>进入D:\Program\zookeeper\zookeeper-3.4.14\bin目录，双击zkServer.cmd即可运行zk。</li>
</ol>
<h3 id="kafka安装">kafka安装</h3>
<p>下载地址：<a href="http://kafka.apache.org/downloads.html">点击此处跳转官网下载页面</a></p>
<p>2.8之后kafka支持不依赖zookeeper启动：<a href="https://kafka.apache.org/quickstart">点击此处跳转详细说明页面</a></p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314004.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>下载后解压缩。eg: D:\Program\kafka_2.13-2.7.0</p>
</li>
<li>
<p>建立一个空文件夹 logs. eg: D:\Program\kafka_2.13-2.7.0\logs</p>
</li>
<li>
<p>进入config目录，编辑 server.properties文件(eg: 用“写字板”打开)。；找到并编辑log.dirs=D:\Program\kafka_2.13-2.7.0\logs；找到并编辑zookeeper.connect=localhost:2181。表示本地运行。(Kafka会按照默认，在9092端口上运行，并连接zookeeper的默认端口：2181)</p>
</li>
<li>
<p>运行：命令行下切入D:\Program\kafka_2.13-2.7.0目录，随后执行如下命令即可（注意先开启zookeeper）：.\bin\windows\kafka-server-start.bat .\config\server.properties</p>
</li>
<li>
<p>可能存在的报错：</p>
<p>输入行太长。 命令语法不正确：目录树太深了，减少几个目录即可。</p>
<p>找不到或无法加载主类 Files\java\jdk8\lib;D:\Program：此错误由目录存在空格所导致。解决：找到D:\Program\kafka_2.13-2.7.0\bin\windows，用编辑器（eg：记事本）打开kafka-run-class.bat，查看配置是否有加上双引号</p>
</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314005.png" alt="img" loading="lazy"></figure>
<p>如果加了双引号仍然无法启动，那可以去查看java安装路径中的文件夹是否存在空格，如果存在，将空格去掉即可。</p>
<p>启动图如下：</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314006.png" alt="img" loading="lazy"></figure>
<h2 id="简单kafka命令">简单kafka命令</h2>
<p>部分kafka命令，基于win10（如果非win系统，使用bin目录下的.sh即可）</p>
<h3 id="创建topic两个副本-四个分区">创建topic（两个副本。四个分区）</h3>
<pre><code>kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic testTopic  
$ bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --partitions 3 --replication-factor 3 --topic topic_test( Kafka 版本 &gt;= 2.2 支持此方式（推荐）)  
$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
</code></pre>
<h3 id="查看topic">查看topic</h3>
<p>查看topic列表：kafka-topics.bat --list --zookeeper localhost:2181</p>
<p>查看topic详情：bin/kafka-topics.sh --zookeeper host12:2181  --describe --topic ltopicName</p>
<pre><code>$ bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092
Topic:quickstart-events  PartitionCount:1    ReplicationFactor:1 Configs:
Topic: quickstart-events Partition: 0    Leader: 0   Replicas: 0 Isr: 0
</code></pre>
<p>清除Kafka topic下所有消息：kafka-topics.sh --zookeeper zookeeper地址:端口 --delete --topic topic_name</p>
<h3 id="删除topic">删除topic</h3>
<p>linux：./bin/kafka-topics  --delete --zookeeper 【zookeeper server】  --topic 【topic name】</p>
<p>win：kafka-topics.bat --delete --zookeeper localhost:2181 --topic testTopic</p>
<p>如果kafaka启动时加载的配置文件中server.properties没有配置delete.topic.enable=true，那么此时的删除并不是真正的删除，而是把topic标记为：marked for deletion</p>
<p>彻底删除topic，可以如下操作：</p>
<ol>
<li>删除kafka存储目录（server.properties文件log.dirs配置，默认为&quot;/tmp/kafka-logs&quot;）相关topic目录</li>
<li>登录zookeeper客户端：命令：./bin/zookeeper-client</li>
<li>找到topic所在的目录：ls /brokers/topics</li>
<li>找到要删除的topic，执行命令：rmr /brokers/topics/【topic name】即可，此时topic被彻底删除。</li>
</ol>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314007.png" alt="img" loading="lazy"></figure>
<h3 id="发送消息至topic">发送消息至topic</h3>
<pre><code>$ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
This is my first event
This is my second event
</code></pre>
<h3 id="消费topic消息">消费topic消息</h3>
<p>从头开始查看kafka topic下的数据：kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic_name --from-beginning</p>
<p>按照偏移量查看topic下数据：kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic_name --offset latest --partition 0</p>
<p>--offset设置偏移量 latest代表最后 ，可以设置区间，不设置结尾的话默认为查询到latest(最后)</p>
<p>--partition 设置分区 使用偏移量查询时一定要设置分区才能查询</p>
<pre><code>$ bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
This is my first event
This is my second event
</code></pre>
<h2 id="spring-kafka属性配置">spring-kafka属性配置</h2>
<p>spring已经封装了kafka，所以在springboot项目中可以非常便捷的集成kafka，引入其依赖，如下：</p>
<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;
  &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;
  &lt;version&gt;${last.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>文档地址：<a href="https://docs.spring.io/spring-kafka/docs/current/reference/html/">点击此处跳转官网页面</a></p>
<p>属性配置：<a href="https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html#spring.kafka.admin.client-id">点击此处跳转说明页面</a></p>
<h3 id="producer的配置参数">producer的配置参数</h3>
<pre><code>#procedure要求leader在考虑完成请求之前收到的确认数，用于控制发送记录在服务端的持久化，其值可以为如下：
#acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为-1。
#acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。
#acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1的设置。
#可以设置的值为：all(-1), 0, 1
spring.kafka.producer.acks=1
 
#每当多个记录被发送到同一分区时，生产者将尝试将记录一起批量处理为更少的请求， 
#这有助于提升客户端和服务器上的性能，此配置控制默认批量大小（以字节为单位），默认值为16384
spring.kafka.producer.batch-size=16384
 
#以逗号分隔的主机：端口对列表，用于建立与Kafka集群的初始连接
spring.kafka.producer.bootstrap-servers
 
#生产者可用于缓冲等待发送到服务器的记录的内存总字节数，默认值为33554432
spring.kafka.producer.buffer-memory=33554432
 
#ID在发出请求时传递给服务器，用于服务器端日志记录
spring.kafka.producer.client-id
 
#生产者生成的所有数据的压缩类型，此配置接受标准压缩编解码器（'gzip'，'snappy'，'lz4'），
#它还接受'uncompressed'以及'producer'，分别表示没有压缩以及保留生产者设置的原始压缩编解码器，
#默认值为producer
spring.kafka.producer.compression-type=producer
 
#key的Serializer类，实现类实现了接口org.apache.kafka.common.serialization.Serializer
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
 
#值的Serializer类，实现类实现了接口org.apache.kafka.common.serialization.Serializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
 
#如果该值大于零时，表示启用重试失败的发送次数
spring.kafka.producer.retries
</code></pre>
<h3 id="consumer的配置参数">consumer的配置参数</h3>
<pre><code>#当'enable.auto.commit'为true时，该配置表示消费者offset自动提交给Kafka的频率（以毫秒为单位），默认值为5000。
spring.kafka.consumer.auto-commit-interval;
 
#当Kafka中没有初始偏移量或者服务器上不再存在当前偏移量时该怎么办，默认值为latest，表示自动将偏移重置为最新的偏移量
#可选的值为latest, earliest, none
spring.kafka.consumer.auto-offset-reset=latest;
 
#以逗号分隔的主机：端口对列表，用于建立与Kafka群集的初始连接。
spring.kafka.consumer.bootstrap-servers;
 
#ID在发出请求时传递给服务器;用于服务器端日志记录。
spring.kafka.consumer.client-id;
 
#如果为true，则消费者的偏移量将在后台定期提交，默认值为true(2.3后默认为false)
spring.kafka.consumer.enable-auto-commit=true;
 
#如果没有足够的数据立即满足“fetch.min.bytes”给出的要求，服务器在回答获取请求之前将阻塞的最长时间（以毫秒为单位）
#默认值为500
spring.kafka.consumer.fetch-max-wait;
 
#服务器应以字节为单位返回获取请求的最小数据量，默认值为1，对应的kafka的参数为fetch.min.bytes。
spring.kafka.consumer.fetch-min-size;
 
#用于标识此使用者所属的使用者组的唯一字符串。
spring.kafka.consumer.group-id;
 
#心跳与消费者协调员之间的预期时间（以毫秒为单位），默认值为3000
spring.kafka.consumer.heartbeat-interval;
 
#密钥的反序列化器类，实现类实现了接口org.apache.kafka.common.serialization.Deserializer
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
 
#值的反序列化器类，实现类实现了接口org.apache.kafka.common.serialization.Deserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
 
#一次调用poll()操作时返回的最大记录数，默认值为500
spring.kafka.consumer.max-poll-records;
</code></pre>
<h3 id="listener的配置参数">listener的配置参数</h3>
<pre><code>#侦听器的AckMode
#当enable.auto.commit的值设置为false时，该值会生效；为true时不会生效
spring.kafka.listener.ack-mode;
 
#在侦听器容器中运行的线程数
spring.kafka.listener.concurrency;
 
#轮询消费者时使用的超时（以毫秒为单位）
spring.kafka.listener.poll-timeout;
 
#当ackMode为“COUNT”或“COUNT_TIME”时，偏移提交之间的记录数
spring.kafka.listener.ack-count;
 
#当ackMode为“TIME”或“COUNT_TIME”时，偏移提交之间的时间（以毫秒为单位）
spring.kafka.listener.ack-time;
</code></pre>
]]></content>
    </entry>
</feed>