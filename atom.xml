<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://philosopherzb.github.io</id>
    <title>Philosopher</title>
    <updated>2023-03-16T06:56:54.205Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://philosopherzb.github.io"/>
    <link rel="self" href="https://philosopherzb.github.io/atom.xml"/>
    <subtitle>WORLD AS CODE</subtitle>
    <logo>https://philosopherzb.github.io/images/avatar.png</logo>
    <icon>https://philosopherzb.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, Philosopher</rights>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之Broker请求流程]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-qing-qiu-liu-cheng/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-qing-qiu-liu-cheng/">
        </link>
        <updated>2022-07-02T06:17:03.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka broker请求流程。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/nature-21474001_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="broker请求流程详解">Broker请求流程详解</h2>
<h3 id="reactor-模型">Reactor 模型</h3>
<p>在介绍kafka的broker通信前，有必要简述一下reactor(响应式)模型。Reactor核心是基于事件驱动（IO多路复用），整体架构流程图如下所示(源自Doug Lea)：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316001.png" alt="img" loading="lazy"></figure>
<ul>
<li>整个模型中所有的输入都由一个Reactor进行接受，随后通过一个dispatch loop（即acceptor）轮询将事件推送至不同的工作线程处理器进行相关逻辑处理。</li>
<li>在这个架构中，Acceptor 线程只是用来进行请求分发，所以非常轻量级，因此会有很高的吞吐量。而那些工作线程则可以根据实际系统负载情况动态的调节系统负载能力，从而达到请求处理的平衡性。</li>
<li>多核处理时，还可以针对reactor进行池化处理(pool)，来提高IO效率。在此过程中，每个Reactor都有自己的选择器，线程处理器及分发轮询器，由主acceptor将数据分发给其他reactors。如下图所示：</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316002.png" alt="img" loading="lazy"></figure>
<h3 id="broker-处理流程">Broker 处理流程</h3>
<p>在kafka中，集群中的每个broker都面临着巨量的网络请求，这个时候如果使用单线程或者多线程进行网络连接处理，那吞吐量将难以达到期望值。此时，上文所说的reactor模型便有了应用场景。</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316003.png" alt="img" loading="lazy"></figure>
<p>在Kafka的架构中，会有很多客户端向Broker端发送请求，Kafka 的 Broker 端有个 SocketServer 组件，主要负责和客户端建立连接，并将相关请求通过Acceptor转发给对应的处理器线程池；而网络连接池(RequestHandlerPool)则主要负责真实的逻辑处理。</p>
<p>SocketServer 组件是 Kafka 超高并发网络通信层中最重要的子模块。它包含 Acceptor 线程、Processor 线程和 RequestChannel 等对象，都是网络通信的重要组成部分。它主要实现了 Reactor 设计模式，用来处理外部多个 Clients（这里的 Clients 可能包含 Producer、Consumer 或其他 Broker）的并发请求，并负责将处理结果封装进 Response 中，返还给 Clients。</p>
<p>其中Acceptor 线程采用轮询的方式将入站请求公平地发到所有网络线程中，网络线程池默认大小是 3个，表示每台 Broker 启动时会创建 3 个网络线程，专门处理客户端发送的请求，可以通过Broker 端参数 num.network.threads（可适当的设置为CPU核数*2，这个值过低时可能会出现因网络空闲太低而缺失副本。）来进行修改。</p>
<p>关于网络线程处理流程如下所示：</p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316004.png" alt="img" loading="lazy"></figure>
<p>当网络线程拿到请求后，会将请求放入到一个共享请求队列中。Broker 端还有个 IO 线程池，负责从该队列中取出请求，执行真正的处理。如果是 PRODUCE 生产请求，则将消息写入到底层的磁盘日志中；如果是 FETCH 请求，则从磁盘或页缓存中读取消息。</p>
<p>IO 线程池中的线程负责执行具体的请求逻辑，默认是8，表示每台 Broker 启动后自动创建 8 个 IO 线程处理请求，可以通过Broker 端参数 num.io.threads（可设置为broker磁盘个数*2）调整。</p>
<p>Purgatory组件是用来缓存延时请求（Delayed Request）的。比如设置了 acks=all 的 PRODUCE 请求，一旦设置了 acks=all，那么该请求就必须等待 ISR 中所有副本都接收了消息后才能返回，此时处理该请求的 IO 线程就必须等待其他 Broker 的写入结果。</p>
<h4 id="socketserver">SocketServer</h4>
<p>负责管理Acceptor 线程、Processor线程和 RequestChannel等对象。</p>
<p>源码地址：<a href="https://github.com/apache/kafka">点击此处跳转github-kafka源码地址</a></p>
<pre><code>// 文件地址：core/src/main/scala/kafka/network/SocketServer.scala
class SocketServer(val config: KafkaConfig,
                   val metrics: Metrics,
                   val time: Time,
                   val credentialProvider: CredentialProvider,
                   val apiVersionManager: ApiVersionManager)
  extends Logging with KafkaMetricsGroup with BrokerReconfigurable {
  // 共享队列长度，即网络阻塞之前可容纳的等待数。由broker端的queued.max.requests参数控制，默认值500
  private val maxQueuedRequests = config.queuedMaxRequests

  private val nodeId = config.brokerId

  private val logContext = new LogContext(s&quot;[SocketServer listenerType=${apiVersionManager.listenerType}, nodeId=$nodeId] &quot;)

  this.logIdent = logContext.logPrefix

  private val memoryPoolSensor = metrics.sensor(&quot;MemoryPoolUtilization&quot;)
  private val memoryPoolDepletedPercentMetricName = metrics.metricName(&quot;MemoryPoolAvgDepletedPercent&quot;, MetricsGroup)
  private val memoryPoolDepletedTimeMetricName = metrics.metricName(&quot;MemoryPoolDepletedTimeTotal&quot;, MetricsGroup)
  memoryPoolSensor.add(new Meter(TimeUnit.MILLISECONDS, memoryPoolDepletedPercentMetricName, memoryPoolDepletedTimeMetricName))
  private val memoryPool = if (config.queuedMaxBytes &gt; 0) new SimpleMemoryPool(config.queuedMaxBytes, config.socketRequestMaxBytes, false, memoryPoolSensor) else MemoryPool.NONE
 
   // data-plane
  // 处理数据面请求的 processor线程池 
  private val dataPlaneProcessors = new ConcurrentHashMap[Int, Processor]()
  // 处理数据面请求的 acceptor线程池，一个监听器对应一个acceptor线程
  private[network] val dataPlaneAcceptors = new ConcurrentHashMap[EndPoint, Acceptor]()
  // 处理数据面的requestChannel对象
  val dataPlaneRequestChannel = new RequestChannel(maxQueuedRequests, DataPlaneMetricPrefix, time, apiVersionManager.newRequestMetrics)
  
  // control-plane
  // 处理控制面请求的 processor，只一个
  private var controlPlaneProcessorOpt : Option[Processor] = None
  // 处理控制面请求的 acceptor，只一个
  private[network] var controlPlaneAcceptorOpt : Option[Acceptor] = None
  // 处理控制面的requestChannel对象
  val controlPlaneRequestChannelOpt: Option[RequestChannel] = config.controlPlaneListenerName.map(_ =&gt;
    new RequestChannel(20, ControlPlaneMetricPrefix, time, apiVersionManager.newRequestMetrics))

  private var nextProcessorId = 0
  val connectionQuotas = new ConnectionQuotas(config, time, metrics)
  private var startedProcessingRequests = false
  private var stoppedProcessingRequests = false

</code></pre>
<h4 id="requestchannel">RequestChannel</h4>
<p>负责管理Processor，并作为传输request和response的中转站。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/network/RequestChannel.scala
class RequestChannel(val queueSize: Int,
                     val metricNamePrefix: String,
                     time: Time,
                     val metrics: RequestChannel.Metrics) extends KafkaMetricsGroup {
  import RequestChannel._
  // 共享请求阻塞队列
  private val requestQueue = new ArrayBlockingQueue[BaseRequest](queueSize)
  // processor 线程池
  private val processors = new ConcurrentHashMap[Int, Processor]()
  val requestQueueSizeMetricName = metricNamePrefix.concat(RequestQueueSizeMetric)
  val responseQueueSizeMetricName = metricNamePrefix.concat(ResponseQueueSizeMetric)

  newGauge(requestQueueSizeMetricName, () =&gt; requestQueue.size)

  newGauge(responseQueueSizeMetricName, () =&gt; {
    processors.values.asScala.foldLeft(0) {(total, processor) =&gt;
      total + processor.responseQueueSize
    }
  })
  // 添加processor线程 
  def addProcessor(processor: Processor): Unit = {
    if (processors.putIfAbsent(processor.id, processor) != null)
      warn(s&quot;Unexpected processor with processorId ${processor.id}&quot;)

    newGauge(responseQueueSizeMetricName, () =&gt; processor.responseQueueSize,
      Map(ProcessorMetricTag -&gt; processor.id.toString))
  }
  // 删除processor线程 
  def removeProcessor(processorId: Int): Unit = {
    processors.remove(processorId)
    removeMetric(responseQueueSizeMetricName, Map(ProcessorMetricTag -&gt; processorId.toString))
  }

  /** Send a request to be handled, potentially blocking until there is room in the queue for the request */
  // 发送请求到队列中，如果空间不足将会处于阻塞状态
  def sendRequest(request: RequestChannel.Request): Unit = {
    requestQueue.put(request)
  }
  // 关闭连接
  def closeConnection(
    request: RequestChannel.Request,
    errorCounts: java.util.Map[Errors, Integer]
  ): Unit = {
    // This case is used when the request handler has encountered an error, but the client
    // does not expect a response (e.g. when produce request has acks set to 0)
    updateErrorMetrics(request.header.apiKey, errorCounts.asScala)
    sendResponse(new RequestChannel.CloseConnectionResponse(request))
  }

  def sendResponse(
    request: RequestChannel.Request,
    response: AbstractResponse,
    onComplete: Option[Send =&gt; Unit]
  ): Unit = {
    updateErrorMetrics(request.header.apiKey, response.errorCounts.asScala)
    sendResponse(new RequestChannel.SendResponse(
      request,
      request.buildResponseSend(response),
      request.responseNode(response),
      onComplete
    ))
  }

  def sendNoOpResponse(request: RequestChannel.Request): Unit = {
    sendResponse(new network.RequestChannel.NoOpResponse(request))
  }

  def startThrottling(request: RequestChannel.Request): Unit = {
    sendResponse(new RequestChannel.StartThrottlingResponse(request))
  }

  def endThrottling(request: RequestChannel.Request): Unit = {
    sendResponse(new EndThrottlingResponse(request))
  }

  /** Send a response back to the socket server to be sent over the network */
  // 将响应结果发回socket服务，并通过网络传输
  private[network] def sendResponse(response: RequestChannel.Response): Unit = {
    if (isTraceEnabled) {
      val requestHeader = response.request.headerForLoggingOrThrottling()
      val message = response match {
        case sendResponse: SendResponse =&gt;
          s&quot;Sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} of ${sendResponse.responseSend.size} bytes.&quot;
        case _: NoOpResponse =&gt;
          s&quot;Not sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} as it's not required.&quot;
        case _: CloseConnectionResponse =&gt;
          s&quot;Closing connection for client ${requestHeader.clientId} due to error during ${requestHeader.apiKey}.&quot;
        case _: StartThrottlingResponse =&gt;
          s&quot;Notifying channel throttling has started for client ${requestHeader.clientId} for ${requestHeader.apiKey}&quot;
        case _: EndThrottlingResponse =&gt;
          s&quot;Notifying channel throttling has ended for client ${requestHeader.clientId} for ${requestHeader.apiKey}&quot;
      }
      trace(message)
    }

    response match {
      // We should only send one of the following per request
      case _: SendResponse | _: NoOpResponse | _: CloseConnectionResponse =&gt;
        val request = response.request
        val timeNanos = time.nanoseconds()
        request.responseCompleteTimeNanos = timeNanos
        if (request.apiLocalCompleteTimeNanos == -1L)
          request.apiLocalCompleteTimeNanos = timeNanos
      // For a given request, these may happen in addition to one in the previous section, skip updating the metrics
      case _: StartThrottlingResponse | _: EndThrottlingResponse =&gt; ()
    }

    val processor = processors.get(response.processor)
    // The processor may be null if it was shutdown. In this case, the connections
    // are closed, so the response is dropped.
    if (processor != null) {
      processor.enqueueResponse(response)
    }
  }

  /** Get the next request or block until specified time has elapsed */
  // 在过期时间之前一直阻塞获取下一个请求
  def receiveRequest(timeout: Long): RequestChannel.BaseRequest =
    requestQueue.poll(timeout, TimeUnit.MILLISECONDS)

  /** Get the next request or block until there is one */
  // 一直阻塞获取下一个请求
  def receiveRequest(): RequestChannel.BaseRequest =
    requestQueue.take()

  def updateErrorMetrics(apiKey: ApiKeys, errors: collection.Map[Errors, Integer]): Unit = {
    errors.forKeyValue { (error, count) =&gt;
      metrics(apiKey.name).markErrorMeter(error, count)
    }
  }

  def clear(): Unit = {
    requestQueue.clear()
  }

  def shutdown(): Unit = {
    clear()
    metrics.close()
  }

  def sendShutdownRequest(): Unit = requestQueue.put(ShutdownRequest)

}

</code></pre>
<h4 id="acceptor-线程">Acceptor 线程</h4>
<p>Reactor模型中，有一种重要的Dispatcher角色，主要用来接受外部请求，并进行分发操作；这个角色也被称为Acceptor。在kafka中的，每个broker端的每个SocketServer实例只会创建一个Acceptor线程，这个Acceptor线程主要用来创建连接，并分发请求给Processor线程处理。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/network/SocketServer.scala
private[kafka] class Acceptor(val endPoint: EndPoint,
                              val sendBufferSize: Int,
                              val recvBufferSize: Int,
                              nodeId: Int,
                              connectionQuotas: ConnectionQuotas,
                              metricPrefix: String,
                              time: Time,
                              logPrefix: String = &quot;&quot;) extends AbstractServerThread(connectionQuotas) with KafkaMetricsGroup {

  this.logIdent = logPrefix
  // 创建nio selector对象，用来检查一个或多个NIO Channel的状态是否处于可读、可写
  private val nioSelector = NSelector.open()
  // broker 端创建对应的socketServer channel实例，并注册到selector上。
  val serverChannel = openServerSocket(endPoint.host, endPoint.port)
  // 创建processors 线程池
  private val processors = new ArrayBuffer[Processor]()
  // processors启动标志
  private val processorsStarted = new AtomicBoolean
  // 阻塞状态记录器
  private val blockedPercentMeter = newMeter(s&quot;${metricPrefix}AcceptorBlockedPercent&quot;,
    &quot;blocked time&quot;, TimeUnit.NANOSECONDS, Map(ListenerMetricTag -&gt; endPoint.listenerName.value))
  private var currentProcessorIndex = 0
  private[network] val throttledSockets = new mutable.PriorityQueue[DelayedCloseSocket]()

  // 延迟关闭socket
  private[network] case class DelayedCloseSocket(socket: SocketChannel, endThrottleTimeMs: Long) extends Ordered[DelayedCloseSocket] {
    override def compare(that: DelayedCloseSocket): Int = endThrottleTimeMs compare that.endThrottleTimeMs
  }
  
    /**
   * Accept loop that checks for new connection attempts
   * 循环检查是否有新连接进入，如果有的话就给其分配处理器
   */
  def run(): Unit = {
    // 注册  OP_ACCEPT
    serverChannel.register(nioSelector, SelectionKey.OP_ACCEPT)
    // 等待 Acceptor启动完成
    startupComplete()
    try {
      while (isRunning) {
        try {
          // 循环检查是否有可用的新连接  
          acceptNewConnections()
          // 关闭连接
          closeThrottledConnections()
        }
        catch {
          // We catch all the throwables to prevent the acceptor thread from exiting on exceptions due
          // to a select operation on a specific channel or a bad request. We don't want
          // the broker to stop responding to requests from other clients in these scenarios.
          case e: ControlThrowable =&gt; throw e
          case e: Throwable =&gt; error(&quot;Error occurred&quot;, e)
        }
      }
    } finally {
      debug(&quot;Closing server socket, selector, and any throttled sockets.&quot;)
      CoreUtils.swallow(serverChannel.close(), this, Level.ERROR)
      CoreUtils.swallow(nioSelector.close(), this, Level.ERROR)
      throttledSockets.foreach(throttledSocket =&gt; closeSocket(throttledSocket.socket))
      throttledSockets.clear()
      shutdownComplete()
    }
  }
  
    /**
   * Listen for new connections and assign accepted connections to processors using round-robin.
   * 轮询监听并分配连接给处理器
   */
  private def acceptNewConnections(): Unit = {
    // 每500毫秒获取一次就绪的IO事件  
    val ready = nioSelector.select(500)
    // 如果存在就绪的IO事件
    if (ready &gt; 0) {
      // 获取selector中的key  
      val keys = nioSelector.selectedKeys()
      val iter = keys.iterator()
      while (iter.hasNext &amp;&amp; isRunning) {
        try {
          val key = iter.next
          iter.remove()

          // 如果key可接受数据
          if (key.isAcceptable) {
            // 针对key创建socket连接  
            accept(key).foreach { socketChannel =&gt;
              // Assign the channel to the next processor (using round-robin) to which the
              // channel can be added without blocking. If newConnections queue is full on
              // all processors, block until the last one is able to accept a connection.
              var retriesLeft = synchronized(processors.length)
              var processor: Processor = null
              do {
                retriesLeft -= 1
                // 指定处理线程processor
                processor = synchronized {
                  // adjust the index (if necessary) and retrieve the processor atomically for
                  // correct behaviour in case the number of processors is reduced dynamically
                  currentProcessorIndex = currentProcessorIndex % processors.length
                  processors(currentProcessorIndex)
                }
                // 递增当前处理器索引
                currentProcessorIndex += 1
              } while (!assignNewConnection(socketChannel, processor, retriesLeft == 0))
            }
          } else
            throw new IllegalStateException(&quot;Unrecognized key state for acceptor thread.&quot;)
        } catch {
          case e: Throwable =&gt; error(&quot;Error while accepting connection&quot;, e)
        }
      }
    }
  }

</code></pre>
<h4 id="processor-线程">Processor 线程</h4>
<p>Processor线程负责核心的逻辑处理。</p>
<p>核心参数说明：</p>
<ul>
<li>newConnections 队列: 是一个阻塞队列，主要用来保存要创建的新连接信息，也就是SocketChannel 对象，其队列长度大小为20(源码中直接硬编码了)。每当 Processor 线程接收到新的连接请求时，都会将对应的 SocketChannel 对象放入队列，等到后面创建连接时，从该队列中获取 SocketChannel，然后注册新的连接。</li>
<li>inflightResponse 队列：是一个临时的 Response 队列， 当 Processor 线程将 Repsonse 返回给 Client 之后，要将 Response 放入该队列。它存在的意义：由于有些 Response 回调逻辑要在 Response 被发送回 Request 发送方后，才能执行，因此需要暂存到临时队列。</li>
<li>ResponseQueue 队列：它主要是存放需要返回给Request 发送方的所有 Response 对象。通过源码得知：每个 Processor 线程都会维护自己的 Response 队列。</li>
</ul>
<pre><code>  // 文件地址：core/src/main/scala/kafka/network/SocketServer.scala
  private val newConnections = new ArrayBlockingQueue[SocketChannel](connectionQueueSize)
  private val inflightResponses = mutable.Map[String, RequestChannel.Response]()
  private val responseQueue = new LinkedBlockingDeque[RequestChannel.Response]()
  
  override def run(): Unit = {
    // 等待processor线程启动完成  
    startupComplete()
    try {
      while (isRunning) {
        try {
          // setup any new connections that have been queued up
          // 配置新的已就绪连接（将newConnections 里的channel取出来注册到selector中）；
          // 为了确保及时处理现有通道的流量和连接关闭的通知，每次迭代处理的连接数是有限的。
          configureNewConnections()
          // register any new responses for writing
          // selector将response通过网络发出，并将其存入inflightResponses 用作后续调用
          processNewResponses()
          // 执行nio poll，获取socketChannel上已就绪的IO事件
          poll()
          // 将接收到的请求放入request阻塞队列中
          processCompletedReceives()
          // 为了inflightResponses 中成功发送的response执行回调逻辑
          processCompletedSends()
          // 将断开连接的response从inflightResponses 中移除
          processDisconnected()
          // 关闭超过配额限制的连接
          closeExcessConnections()
        } catch {
          // We catch all the throwables here to prevent the processor thread from exiting. We do this because
          // letting a processor exit might cause a bigger impact on the broker. This behavior might need to be
          // reviewed if we see an exception that needs the entire broker to stop. Usually the exceptions thrown would
          // be either associated with a specific socket channel or a bad request. These exceptions are caught and
          // processed by the individual methods above which close the failing channel and continue processing other
          // channels. So this catch block should only ever see ControlThrowables.
          case e: Throwable =&gt; processException(&quot;Processor got uncaught exception.&quot;, e)
        }
      }
    } finally {
      debug(s&quot;Closing selector - processor $id&quot;)
      CoreUtils.swallow(closeAll(), this, Level.ERROR)
      shutdownComplete()
    }
  }

</code></pre>
<h4 id="kafkarequesthandler-核心逻辑处理器">KafkaRequestHandler 核心逻辑处理器</h4>
<p>KafkaRequestHandler 是kafka中真正的逻辑处理代码。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaRequestHandler.scala
// IO线程池
class KafkaRequestHandlerPool(val brokerId: Int,
                              val requestChannel: RequestChannel,
                              val apis: ApiRequestHandler,// 具体逻辑处理器
                              time: Time,
                              numThreads: Int,// 线程数
                              requestHandlerAvgIdleMetricName: String,
                              logAndThreadNamePrefix : String) extends Logging with KafkaMetricsGroup {
   // 可动态扩展
  private val threadPoolSize: AtomicInteger = new AtomicInteger(numThreads)
  /* a meter to track the average free capacity of the request handlers */
  private val aggregateIdleMeter = newMeter(requestHandlerAvgIdleMetricName, &quot;percent&quot;, TimeUnit.NANOSECONDS)

  // 线程数组
  this.logIdent = &quot;[&quot; + logAndThreadNamePrefix + &quot; Kafka Request Handler on Broker &quot; + brokerId + &quot;], &quot;
  val runnables = new mutable.ArrayBuffer[KafkaRequestHandler](numThreads)
  for (i &lt;- 0 until numThreads) {
    createHandler(i)
  }
  
  // 创建IO处理线程，即KafkaRequestHandler
  def createHandler(id: Int): Unit = synchronized {
    runnables += new KafkaRequestHandler(id, brokerId, aggregateIdleMeter, threadPoolSize, requestChannel, apis, time)
    KafkaThread.daemon(logAndThreadNamePrefix + &quot;-kafka-request-handler-&quot; + id, runnables(id)).start()
  }
  
// IO线程  
class KafkaRequestHandler(id: Int,
                          brokerId: Int,
                          val aggregateIdleMeter: Meter,
                          val totalHandlerThreads: AtomicInteger,
                          val requestChannel: RequestChannel,
                          apis: ApiRequestHandler,
                          time: Time) extends Runnable with Logging {
  this.logIdent = s&quot;[Kafka Request Handler $id on Broker $brokerId], &quot;
  private val shutdownComplete = new CountDownLatch(1)
  private val requestLocal = RequestLocal.withThreadConfinedCaching
  @volatile private var stopped = false

  def run(): Unit = {
    while (!stopped) {
      // We use a single meter for aggregate idle percentage for the thread pool.
      // Since meter is calculated as total_recorded_value / time_window and
      // time_window is independent of the number of threads, each recorded idle
      // time should be discounted by # threads.
      val startSelectTime = time.nanoseconds

      // 从requestChannel获取请求
      val req = requestChannel.receiveRequest(300)
      val endTime = time.nanoseconds
      // 统计线程空闲时间
      val idleTime = endTime - startSelectTime
      // 更新线程空闲百分比指标
      aggregateIdleMeter.mark(idleTime / totalHandlerThreads.get)

      req match {
        // 关闭请求  
        case RequestChannel.ShutdownRequest =&gt;
          debug(s&quot;Kafka request handler $id on broker $brokerId received shut down command&quot;)
          completeShutdown()
          return

        // 正常请求
        case request: RequestChannel.Request =&gt;
          try {
            request.requestDequeueTimeNanos = endTime
            trace(s&quot;Kafka request handler $id on broker $brokerId handling request $request&quot;)
            // 真正开始处理相关逻辑
            apis.handle(request, requestLocal)
          } catch {
            case e: FatalExitError =&gt;
              completeShutdown()
              Exit.exit(e.statusCode)
            case e: Throwable =&gt; error(&quot;Exception when handling request&quot;, e)
          } finally {
            request.releaseBuffer()
          }

        case null =&gt; // continue
      }
    }
    completeShutdown()
  }

</code></pre>
<h4 id="完整处理流程">完整处理流程</h4>
<ol>
<li>SocketServer 中的Acceptor 线程接受clients的请求。</li>
<li>Acceptor 线程会创建 NIO Selector 对象及 ServerSocketChannel 实例，并将其与 OP_ACCEPT 事件注册到 Selector 多路复用器上。</li>
<li>与此同时，Acceptor 线程还会创建默认大小为3的 Processor 线程池，（可通过broker端参数：num.network.threads进行修改） 。</li>
<li>Acceptor线程开始轮询监听，如果有新的请求对象 SocketChannel 进入，便会将其放入到连接队列中（newConnections），同时会触发一个processor线程进行处理。</li>
<li>Processor 线程向 SocketChannel 注册 OP_READ/OP_WRITE 事件，同时，轮询获取已就绪的IO事件。</li>
<li>Processor 线程会根据Channel中获取已经完成的 Receive 对象，构建 Request 对象，并将其存入到 Requestchannel 的 RequestQueue 请求队列中 。</li>
<li>KafkaRequestHandler 线程循环地从请求队列中获取 Request 实例，然后交由KafkaApis 的 handle 方法，执行真正的请求处理逻辑，并最终将数据存储到磁盘中。</li>
<li>待处理完请求后，KafkaRequestHandler 线程会将 Response 对象放入 Processor 线程的 Response 队列。</li>
<li>Processor 线程通过 Request 中的 ProcessorID 不停地从 Response 队列中来定位并取出 Response 对象，并返还给 Request 发送方。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之可用性与持久性保证]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-ke-yong-xing-yu-chi-jiu-xing-bao-zheng/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-ke-yong-xing-yu-chi-jiu-xing-bao-zheng/">
        </link>
        <updated>2022-06-18T08:44:35.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka可用性与持久化保证。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/lake-71208_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="kafka体系架构">kafka体系架构</h2>
<h3 id="总览">总览</h3>
<p>标准的kafka集群中一般包含多个broker，若干个producer，若干个consumer，以及一个zookeeper集群。</p>
<p>Kafka 中的message以topic为单位进行归类，producer负责将消息发送到特定的topic（发送到 Kafka 集群中的每一条消息都要指定一个topic），而consumer负责订阅topic并进行消费。</p>
<p>topic只是一个逻辑上的概念，每个主题都会被划分为多个partition，partition是实际存在的存储介质，消息在单个partition中有序。</p>
<p>架构图如下所示：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315004.png" alt="img" loading="lazy"></figure>
<h3 id="消息传递语义message-delivery-semantics">消息传递语义（Message Delivery Semantics）</h3>
<p>关于producer与consumer之间消息传递保证，kafka提供了如下三种语义（需要注意的是，此过程分为发布消息的持久化保证及消费消息的保证）：</p>
<ul>
<li>At most once：消息可能会丢失，但绝不会重传。</li>
<li>At least once：消息绝不会丢失，但可能会重传。</li>
<li>Exactly once：每条消息会传递一次且仅有一次，这正是用户真正所需要的。</li>
</ul>
<h4 id="发送消息持久化保证">发送消息持久化保证</h4>
<p>针对发送消息的持久化保证：在 kafka-0.11.0.0 之前，如果生产者没有收到表明消息已提交的响应，那么消息将会被重发，这仅提供了At least once传递语义，因为如果原始请求实际上已经成功，则在重新发送期间消息可能会再次写入日志。</p>
<p>在 kafka-0.11.0.0 之后，Kafka 生产者开始支持幂等交付选项，以保证重新发送不会导致日志中出现重复条目。 为实现这个目标，broker会为每个生产者分配一个 ID，并使用生产者与每条消息一起发送的序列号对消息进行去重操作。</p>
<p>同样从 kafka-0.11.0.0 开始，生产者支持使用类似事务(transaction-like)的语义将消息发送到多个主题分区的能力；即，所有消息都已成功写入，或者都没有（原子性）。</p>
<h4 id="消费消息的保证">消费消息的保证</h4>
<p>关于consumer读取消息时，该如何处理消息和更新位置的几个场景如下：</p>
<ol>
<li>consumer可以读取消息，然后将其位置保存在日志中，最后处理消息。在这种情况下，消费者进程可能会在保存其位置之后但在保存其消息处理的输出之前崩溃，而接管处理的进程却会从保存的位置开始，即使该位置之前的一些消息尚未处理。 这对应于“At most once”语义，因为在消费者失败的情况下，消息可能不会被处理。</li>
<li>consumer可以读取消息，处理消息，最后保存它的位置。 在这种情况下，消费者进程可能会在处理消息之后但在保存其位置之前崩溃；这样的话，当一个新进程接管它收到的前几条消息时，这些消息实际已经被处理了。这对应于消费者失败情况下的“At least once”语义。 在许多情况下，消息有一个主键，因此更新是幂等的（两次接收相同的消息只会用另一个自身的副本覆盖记录）。</li>
<li>关于如何保证“Exactly once”，当从 Kafka 主题消费并生产到另一个主题时（如在 Kafka Streams 应用程序中），可以利用上面提到的 kafka-0.11.0.0 中新的事务生产者功能。消费者的位置作为消息存储在主题中，与此同时，在和接收处理数据的输出主题相同的事务中将偏移量写入 Kafka。 如果传输中止，消费者的位置将恢复到其旧值，并且输出主题的生成数据将不会对其他消费者可见，当然，这具体取决于他们的“隔离级别”。 在默认的“read_uncommitted”隔离级别中，所有消息对消费者都是可见的，即使它们是中止事务的一部分，但在“read_committed”中，消费者将只返回来自已提交事务的消息（以及任何非事务消息）。</li>
<li>写入外部系统时，限制在于需要将消费者的位置与实际存储为输出的内容相协调。实现这一点的经典方法是在存储消费者位置和输出之间引入两阶段提交。但是通过让消费者将其偏移量与其输出存储在同一位置可以让其中的处理更简单与更通用化。这种方式是比较友好的，因为消费者可能想要写入的许多输出系统不支持两阶段提交。举个例子，Kafka Connect 连接器，它将所读取的数据和数据的 offset 一起写入到 HDFS，以保证数据和 offset 都被更新，或者两者都不被更新。</li>
</ol>
<h3 id="副本replication">副本（Replication）</h3>
<p>kafka为每个主题的分区提供了备份副本功能（可在服务端进行相关配置）；当集群中的某个服务宕机时，副本能够自动进行故障转移，以保证数据的可用性（不丢失，提供容灾能力）。</p>
<p>创建副本的单位是 topic 的 partition ，正常情况下，每个分区都有一个 leader 和零或多个 followers 。总的副本数包含 leader及followers，被称为副本因子。所有的读写操作都由 leader 处理。一般 partition 的数量都比 broker 的数量多的多，各分区的 leader 均匀的分布在 brokers 中。所有的 followers 节点都同步 leader 节点的日志，日志中的消息和偏移量都和 leader 中的一致。（当然，在任何给定时间，leader 节点的日志末尾时可能有几个消息尚未被备份完成）。</p>
<p>如下图所示，Kafka 集群中有4个 broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个 leader 副本和2个 follower 副本。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315005.png" alt="img" loading="lazy"></figure>
<h3 id="选举算法leader-election">选举算法（Leader Election）</h3>
<p>kafka分区的核心便是副本日志（replicated log），这同样也是分布式系统中最重要的基础元素之一（容灾）。</p>
<p>副本日志按照一系列有序的值（通常是编号为 0、1、2、…) 进行建模。有很多方法可以实现这一点，但最简单和最快的方法是由 leader 节点选择需要提供的有序的值，只要 leader 节点还存活，所有的 follower 只需要拷贝数据并按照 leader 节点的顺序排序。</p>
<p>理想情况中，如果leader永远存活，那么也就不需要follower了；然而实际生产环境中依旧可能出现机器宕机的场景（随着系统越大，机器越多，概率也将越高）；因此，为了应对这种情形，需要从存活的followers中选择一个新的leader，但是followers本身数据可能会落后于leader或者crash掉，在这种情况下，有必要选择数据最全的follower（up-to-date follower）作为新的leader。</p>
<p>在这个过程中，有一个基本原则：一条数据如果明确返回给客户端为已提交（committed），则当leader crash掉后出现的新leader必须拥有刚刚已提交过的所有消息。这时候便需要做出权衡：如果leader在表明一条消息已提交（committed）前等待更多的follower进行确认，那么leader宕机后便有更多的follower可以作为新的leader，但与此同时吞吐率也会有所下降。</p>
<p>一种非常常见的leader election算法便是“Majority Vote”。在这种模式下，假设拥有2f+1个副本（包含leader和follower），如果需要确保f+1个副本收到消息，且在这f+1个副本中选择一个作为新的leader，那么fail的副本不能超过f个。这是因为在f+1个副本中，任意一个都拥有最新的所有备份数据。</p>
<p>“Majority Vote”有一个非常好的特性：那就是系统的延迟只取决于最快的服务器；当然其劣势便是需要更多的副本来保证数据的完整性（所能允许fail的副本相对太少）。例如：如果要容忍1个follower宕机，那便需要3个及以上副本；如果要容忍2个follower宕机，那便需要5个及以上副本；也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降，且这种方式对于资源也过于浪费，毕竟生产资源很珍贵。一般情况下，这种算法用于Zookeeper之类的共享集群配置的系统中而很少在需要存储大量数据的系统中使用。</p>
<p>实际上，Leader Election算法非常多，比如Zookeeper的Zab, Raft和Viewstamped Replication。而Kafka所使用的Leader Election算法更像微软的PacificA算法。</p>
<p>kafka采用的选举算法与“Majority Vote”有所不同，它会在zookeeper中动态的维护一组ISR，这些副本基本与leader保持一致（未同步的副本将会被踢出ISR），一般情况下只有ISR中的副本才有机会成为新的leader。</p>
<p>在这种模式下，对于f+1个副本，一个分区能在保证不丢失已经committed的消息的前提下容忍f个副本发生故障。在大多数应用场景中，这种模式是非常有利的。而在实际中，为了冗余f个副本发生故障，Majority Vote和ISR在commit前需要等待的Replica数量是一样的（例如在一次故障恢复中，“Majority Vote”的 quorum 需要三个备份节点和一次确认，而ISR 需要两个备份节点和一次确认），但是ISR需要的总副本数几乎是Majority Vote的一半。</p>
<p>关于kafka另一个重要的特性设计便是其并不需要崩溃节点在拥有完好无损的数据下进行恢复，简而言之，kafka允许丢失部分数据（数据不一致性）。</p>
<p>关于分布式系统中可用性与一致性之间的权衡，往往取决于实际的业务场景需求（如资金类业务需要强一致性，用户活动跟踪只需保证可用性等）。</p>
<p>当kafka中的发生了all replica die时，它提供了两种恢复性方案（可用性与一致性抉择，不止kafka有此等困境，其余框架一样会遇到这种艰难的选择）：</p>
<ol>
<li>
<p>等待 ISR 中的副本恢复并选择此副本作为新的leader（希望它仍然拥有所有数据）。</p>
</li>
<li>
<p>选择第一个恢复过来的副本作为leader（不一定在 ISR 中）。</p>
<p>第一种方案中，如果等待 ISR 中副本恢复，那么只要这些副本一直处于宕机状态，kafka将始终不可用。如果ISR副本被破坏或它们的数据丢失，kafka便彻底宕机了。</p>
<p>第二种方案中，如果一个非同步副本从宕机状态中恢复并且允许它成为leader，那么它的数据将成为新的可信任来源，即使它不能保证拥有每条已提交的消息。</p>
<p>默认情况下，从 kafka-0.11.0.0 版本开始，Kafka 选择第一个策略并倾向于等待一致的副本。可以使用配置属性 unclean.leader.election.enable 更改此行为，以支持正常运行时间优于一致性的用例。</p>
</li>
</ol>
<h3 id="节点存活与ack机制可用性与持久性">节点存活与ack机制（可用性与持久性）</h3>
<p>kafka为节点定义了一个 “in sync” 状态，区别于 “alive” 和 “failed” 。Leader 会追踪所有 “in sync” 的节点。如果有节点挂掉了，或是写超时，或是心跳超时，leader 就会把它从同步副本列表（in sync replicas，缩写为ISR）中移除。同步超时和写超时的时间由 replica.lag.time.max.ms 配置确定。</p>
<p>kafka定义节点处于“in sync”状态的的条件如下：</p>
<ul>
<li>节点必须维护和 ZooKeeper 的会话连接，Zookeeper 可以通过心跳机制来检查每个节点的连接。</li>
<li>如果是 follower 节点，它必须能及时的同步 leader 的写操作，并且延时不能太久。</li>
</ul>
<p>分布式系统中，kafka只尝试处理 “fail/recover” 模式的故障，即节点突然停止工作，然后又恢复（节点可能不知道自己曾经挂掉）的状况。Kafka 没有处理所谓的 “Byzantine” 故障，即一个节点出现了随意响应和恶意响应（可能由于 bug 或 非法操作导致）。</p>
<p>当分区上的所有ISR都将消息保存至log中时，该消息可以被视为已提交，只有已提交的消息才会被消费者所消费。这意味着消费者不必担心当leader fail时，可能会看到丢失的消息。</p>
<p>另一方面，producer可以选择等待或不提交消息，这取决于他们对延迟和持久性之间的权衡，此功能由producer中的ACK机制实现。请注意，主题具有ISR的“最小数量”设置，当生产者请求确认消息已写入完整的ISR集时，会检查该设置。如果生产者请求不那么严格的确认，则可以提交和消费消息，即使同步副本的数量低于最小值（例如，它可以低至仅leader）。</p>
<p>在任何时候，只要至少有一个同步中的节点存活，kafka中的消息就不会丢失；这保证了在短暂的故障转移后，kafka仍具有可用性，不过在网络分区的场景中，可能无法保持可用。</p>
<p>关于ack机制如下：</p>
<ul>
<li>acks = 0，如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障）。此配置可以获得最高的吞吐量。</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315006.png" alt="img" loading="lazy"></figure>
<ul>
<li>acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应；一旦消息无法写入leader分区副本(比如网络原因、leader节点崩溃),生产者会收到一个错误响应，当生产者接收到该错误响应之后，为了避免数据丢失，会尝试重新发送数据（前提是配置了重试次数），这种方式的吞吐量取决于使用的是异步发送还是同步发送。在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录依旧会丢失。</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315007.png" alt="img" loading="lazy"></figure>
<ul>
<li>acks = all 这意味着leader将等待完整的同步副本（ISR）集以确认记录（此时如果ISR同步副本的个数小于min.insync.replicas的值，消息将不会被写入.），这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，相当于acks = -1的设置。此配置最安全，但延迟相对来说也最高。</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315008.png" alt="img" loading="lazy"></figure>
<p>关于最小同步副本配置：</p>
<ul>
<li>Kafka的Broker端提供了一个参数min.insync.replicas，它明确指定了数据要被同步到多少个副本才算真正的成功写入，此参数一般与acks=all配合使用，可以获得更好的持久性保证。该值默认为1，生产环境设定为一个大于1的值可以提升消息的持久性（例如复制因子为3则可以设置该参数为2）。 因为如果同步副本的数量低于该配置值，则生产者会收到错误响应，从而确保消息不丢失。</li>
<li>当min.insync.replicas=2且acks=all时，如果此时ISR列表只有[1,2],3被踢出ISR列表，只需要保证两个副本同步了，生产者就会收到成功响应。</li>
</ul>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315009.png" alt="img" loading="lazy"></figure>
<ul>
<li>当min.insync.replicas=2，如果此时ISR列表只有[1],2和3被踢出ISR列表，那么当acks=all时，则不能成功写入数；当acks=0或者acks=1可以成功写入数据。</li>
</ul>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315010.png" alt="img" loading="lazy"></figure>
<ul>
<li>如果acks=all且min.insync.replicas=2，此时ISR列表为[1,2,3]，这种情况下kafka还是会等到所有的同步副本都同步了消息，才会向生产者发送成功响应的ack。因为min.insync.replicas=2只是一个最低限制，即同步副本少于该配置值，才会抛异常，而acks=all，是需要保证所有的ISR列表的副本都同步了才可以发送成功响应。</li>
</ul>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315011.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[消息队列与kafka]]></title>
        <id>https://philosopherzb.github.io/post/xiao-xi-dui-lie-yu-kafka/</id>
        <link href="https://philosopherzb.github.io/post/xiao-xi-dui-lie-yu-kafka/">
        </link>
        <updated>2022-06-11T05:32:03.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述消息队列以及kafka整体架构。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/mountain-547363_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="消息队列message-queue">消息队列(message queue)</h2>
<h3 id="简介">简介</h3>
<p>消息队列是一种进程间通信或同一进程的不同线程间的通信方式；针对如今微服务或云架构，则指代服务间的异步通信方式。</p>
<p>在现代云架构或微服务架构中，应用程序被分解为多个规模较小且易于开发、部署和维护的构建块。消息队列可以为这些分布式应用程序提供通信与协调；同时，消息队列也可以显著的简化分离应用编码，并提高性能、可靠性和可扩展性等。</p>
<h3 id="消息投递模式">消息投递模式</h3>
<h4 id="端到端point-to-point">端到端(Point-to-point)</h4>
<p>在端到端消息模式中，一条消息将会一直存储在队列中，直到被消费者所接收；且，发送者必须要知道消费者的一些关键信息，例如：将要发送消息的队列名或者特定的队列管理器名。</p>
<p>此模式中，消息队列将会提供一个临时存储消息的轻量级缓冲区，并允许微服务连接到队列以发送和接收消息的终端节点；这种情况下，消息的规模一般较小，如请求，恢复，错误消息及明文消息等；同时一条消息只能由一个接收者处理一次。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314008.png" alt="img" loading="lazy"></figure>
<h4 id="发布订阅publishsubscribe">发布订阅(Publish/Subscribe)</h4>
<p>发布订阅模式中，一条消息由发送者推送至特定的主题(topic)，随后所有订阅了该topic的消费者都将收到同一条广播消息。</p>
<p>消息主题的订阅者通常会执行不同的功能，并可以同时对消息执行不同的操作。发布者无需知道谁在使用广播的信息，而订阅者也无需知道消息来自哪里。这种消息收发模式与端到端稍有不同，在端到端中，发送消息的组件通常知道发送的目的地。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314009.png" alt="img" loading="lazy"></figure>
<h3 id="消息队列优势">消息队列优势</h3>
<ul>
<li>解耦：在分布式系统中，同一份数据可能要分发给不同的程序进行处理；例如订单生成后，要扣库存，加积分；此时如果由订单服务直接调用库存服务及积分服务的接口，将会让订单服务变得十分臃肿，且不利于后续扩展（比如再加一个支付服务）。这时便可以加入消息队列，采用发布订阅模式，订单服务只需将消息发送至指定topic即可，其他服务选择性订阅。这样便将系统业务进行了解耦操作，增加了系统的可扩展性。</li>
<li>异步通信：消息队列天然支持异步操作（所有的消息都会存储在队列中，消费者可以延迟处理），针对分布式系统中的异步操作，可采用消息队列进行处理。</li>
<li>削峰填谷：针对高并发大数据的场景，消息队列可以有效地做到削峰填谷。数据推送高峰时，将由消息队列暂存所有的数据，等到数据推送低谷时，由消费者逐步消费所有的数据。必要时，也可以额外的增加消费者进行数据处理。</li>
<li>缓冲：消息队列通过一个缓冲层来帮助任务最高效率的执行；写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。</li>
<li>跨平台：消息队列支持跨平台消息处理，只需要发送者及消费者约定数据格式即可。例如，由Java发送消息，go消费消息。</li>
<li>灵活性与可扩展性：在高流量期间，可以通过动态的扩展机器来接受更多的消息，防止高并发冲击服务，导致服务挂掉。</li>
</ul>
<h2 id="kafka">kafka</h2>
<h3 id="简介-2">简介</h3>
<p>Kafka 是由 Linkedin 公司开发的，它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的消息流平台，它同时也是一款开源的基于发布订阅模式的消息引擎系统；</p>
<p>Kafka由服务端及客户端组成，且服务端和客户端可以通过高性能的TCP网络协议进行通讯。关于kafka的部署环境，无论本地还是云环境中的裸机，虚拟机或者容器都可以支持。</p>
<h3 id="基本术语">基本术语</h3>
<ul>
<li>
<p>生产者(Producer)：向kafka中的主题(topic)发布消息事件的客户端应用程序被称为生产者。</p>
</li>
<li>
<p>消费者(Consumer)：订阅了kafka消息事件所在的主题(topic)的客户端应用程序称为消费者。</p>
</li>
<li>
<p>主题(Topic)：用于存储同一类消息事件；打个简单的比喻：主题类似于文件系统中的文件夹，而消息事件则类似于文件夹中的文件。</p>
</li>
<li>
<p>消息(Message)：kafka中的数据单元被称为消息，也可以叫记录(record)。</p>
</li>
<li>
<p>偏移量(Offset)：是一种元数据，且是一个不断递增的整数值；当消费者处理完消息后，将会提交offset+1到broker中。</p>
</li>
<li>
<p>broker： 一个独立的 Kafka 服务器就被称为 broker，broker 接收来自生产者的消息，并为消息设置偏移量，同时持久化消息到磁盘。</p>
</li>
<li>
<p>分区(Partition)：在一个主题中可以存在一个或多个分区，每个分区中消息有序；同一主题中的多个分区可以位于不同机器上，方便后续扩容操作。</p>
</li>
<li>
<p>副本(Replica)：消息的备份称为副本，在创建主题时可以指定副本数量。</p>
</li>
<li>
<p>重平衡(Rebalance)：当消费组(多个消费者将会形成一个消费组)中的某个消费者实例挂掉或者新增一个消费者实例时，其他消费者实例可以自动地重新分配订阅主题，这个过程叫重平衡。</p>
</li>
<li>
<p>AR与ISR：AR即可分配副本 Assigned Replicas；而所有与leader副本保持一定同步状态的副本（包含leader副本）构成ISR（In-Sync Replicas）；ISR集合是AR集合中的一个子集。</p>
</li>
<li>
<p>ISR的伸缩：leader 副本负责维护和跟踪 ISR 集合中所有 follower 副本的滞后状态，当 follower 副本落后太多或失效时，leader 副本会把它从 ISR 集合中剔除。如果 OSR（Out-Sync Replicas） 集合中有 follower 副本“追上”了 leader 副本，那么 leader 副本会把它从 OSR 集合转移至 ISR 集合。默认情况下，当 leader 副本发生故障时，只有在 ISR 集合中的副本才有资格被选举为新的 leader，而在 OSR 集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变）。</p>
<p>replica.lag.time.max.ms ： 这个参数的含义是 Follower 副本能够落后 Leader 副本的最长时间间隔，默认10s。</p>
<p>unclean.leader.election.enable：是否允许 Unclean 领导者选举。开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。</p>
</li>
<li>
<p>HW与LW：HW（High Watermark）俗称高水位，它标识了一个特定的偏移量(offset)，消费者只能拉取此偏移量之前的数据；LW（Low Watermark）俗称低水位，它同样标识了一个特定的偏移量(offset)，一般为AR集合中最小的LSO值；LW值的增长与副本的拉取或删除请求息息相关。</p>
</li>
<li>
<p>LSO：LSO 是LogStartOffset的缩写，一般情况下，日志文件的起始偏移量 logStartOffset 等于第一个日志分段的 baseOffset，但这并不是绝对的，logStartOffset 的值可以通过 DeleteRecordsRequest 请求(比如使用 KafkaAdminClient 的 deleteRecords()方法、使用 kafka-delete-records.sh 脚本、日志的清理和截断等操作）进行修改。</p>
</li>
<li>
<p>LEO：LEO是 LogEndOffset 的缩写，它标识当前日志文件中下一条待写入消息的 offset，如下图中 offset 为9的位置即为当前日志文件的 LEO，LEO 的大小相当于当前日志分区中最后一条消息的 offset 值加1。分区 ISR 集合中的每个副本都会维护自身的 LEO，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息</p>
</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314010.png" alt="img" loading="lazy"></figure>
<ul>
<li>发送者，broker集群，消费者基本协作结构图</li>
</ul>
<p><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314011.png" alt="img" loading="lazy"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314011.png" alt="" loading="lazy"></p>
<ul>
<li>主题剖析结构图</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315001.png" alt="img" loading="lazy"></figure>
<ul>
<li>一个两节点的 kafka 集群支持的 2 个消费组的四个分区 (P0-P3)。消费者 A 有两个消费者实例，消费者 B 有四个消费者实例。</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315002.png" alt="img" loading="lazy"></figure>
<h3 id="基本特性">基本特性</h3>
<ul>
<li>高吞吐、低延迟：收发消息快是kafka最重要的特性之一，即使在非常廉价的机器上，kafka也可以轻松达到每秒几十万条消息的传输速率，且此过程中最低延迟仅有几毫秒；一般可以用来做实时日志聚合。</li>
<li>持久化：kafka中的所有消息都会被其持久化存储在文件系统中；在这个过程中，kafka采用顺序写磁盘及直接写页缓存的机制提高IO效率（顺序写磁盘的速度接近于随机写内存的速度：<a href="https://queue.acm.org/detail.cfm?id=1563874">点击此处跳转文章页面</a>）。</li>
<li>水平扩容(Scale out)：kafka提供分区机制以达到动态扩容的效果，即增加分区数（不支持减少分区操作）；</li>
<li>分区容错(Partition-tolerance)：即使集群中的某个节点挂掉，kafka仍然可以提供可用性（可用性与一致性间的冲突，kafka目前采用的是ISR机制进行了部分妥协）。</li>
<li>核心API：Producer API，Consumer API，Streams API，Connect API，Admin API</li>
</ul>
<h3 id="生产者producer">生产者(Producer)</h3>
<h4 id="简介-3">简介</h4>
<p>生产者无需经过路由便可将消息发送至主分区所在的服务器上；为了实现这个功能，所有的kafka服务器节点都能够响应这样的元数据请求：哪些服务器存活，主题的主分区位于哪台服务器上。</p>
<p>对于生产者而言，只需要执行发送请求，消息将会自动根据路由规则分配到不同的分区上。</p>
<p>消息路由规则：如果指定了 partition，则直接使用；如果未指定 partition 但指定了 key，则通过对 key 的 value 进行hash 选出一个 partition（形如：Math.abs(key.hashCode()) % partitions.size()）； 如果partition 和 key 都未指定，便会轮询选出一个 partition。</p>
<h4 id="整体流程架构">整体流程架构</h4>
<p>整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。</p>
<p>在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。</p>
<p>Sender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。</p>
<p>RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315003.png" alt="img" loading="lazy"></figure>
<h5 id="序列化器">序列化器</h5>
<p>生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka；在消费者侧同样需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换成相应的对象。网络传输过程中，序列化是必须的操作。自定义时需要实现org.apache.kafka.common.serialization.Serializer</p>
<pre><code>package org.apache.kafka.common.serialization;

import org.apache.kafka.common.header.Headers;

import java.io.Closeable;
import java.util.Map;

/**
 * 一个用于转换对象为字节的接口
 *
 * 实现此接口的类应具有不带参数的构造函数
 * 
 * @param &lt;T&gt; Type to be serialized from.
 */
public interface Serializer&lt;T&gt; extends Closeable {

    /**
     * 配置当前类，此方法一般是在创建 KafkaProducer 实例的时候调用的，主要用来确定编码类型。
     * @param configs configs in key/value pairs
     * @param isKey whether is for key or value
     */
    default void configure(Map&lt;String, ?&gt; configs, boolean isKey) {
        // intentionally left blank
    }

    /**
     * 转换数据为字节数组（执行序列化操作）
     * 可以进行编解码，如果 Kafka 客户端提供的几种序列化器都无法满足应用需求，
     * 则可以选择使用如 Avro、JSON、Thrift、ProtoBuf 和 Protostuff 等通用的序列化工具来实现，
     * 或者使用自定义类型的序列化器来实现。  
     *
     * @param topic topic associated with data
     * @param data typed data
     * @return serialized bytes
     */
    byte[] serialize(String topic, T data);

    /**
     * 转换数据为字节数组（执行序列化操作）
     * 可以进行编解码，如果 Kafka 客户端提供的几种序列化器都无法满足应用需求，
     * 则可以选择使用如 Avro、JSON、Thrift、ProtoBuf 和 Protostuff 等通用的序列化工具来实现，
     * 或者使用自定义类型的序列化器来实现。  
     *
     * @param topic topic associated with data
     * @param headers headers associated with the record
     * @param data typed data
     * @return serialized bytes
     */
    default byte[] serialize(String topic, Headers headers, T data) {
        return serialize(topic, data);
    }

    /**
     * 关闭序列化器
     * 注意：此方法必须是幂等的，因为它可能会被多次调用。
     */
    @Override
    default void close() {
        // intentionally left blank
    }
}

</code></pre>
<h5 id="分区器">分区器</h5>
<p>分区器的作用就是为消息分配partition。如果消息 ProducerRecord 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值；或者依赖轮询分配partition（可参考消息路由规则）。Kafka 中提供的默认分区器是 org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了 org.apache.kafka.clients.producer.Partitioner 接口。</p>
<pre><code>package org.apache.kafka.clients.producer.internals;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.utils.Utils;

import java.util.Map;

/**
 * 默认的分区策略器
 * &lt;ul&gt;
 * &lt;li&gt;如果记录中指定了分区，则直接使用
 * &lt;li&gt;如果未制定分区，但存在key，则基于key的hash值选择一个分区（拥有相同 key 的消息会被写入同一个分区，前提是分区数不会变更）
 * &lt;li&gt;如果分区和key都不存在，则选择在批处理已满时更改的粘性分区（轮询操作）。
 * 
 * See KIP-480 for details about sticky partitioning.
 */
public class DefaultPartitioner implements Partitioner {

    private final StickyPartitionCache stickyPartitionCache = new StickyPartitionCache();

    public void configure(Map&lt;String, ?&gt; configs) {}

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param key The key to partition on (or null if no key)
     * @param keyBytes serialized key to partition on (or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        return partition(topic, key, keyBytes, value, valueBytes, cluster, cluster.partitionsForTopic(topic).size());
    }

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param numPartitions The number of partitions of the given {@code topic}
     * @param key The key to partition on (or null if no key)
     * @param keyBytes serialized key to partition on (or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster,
                         int numPartitions) {
        if (keyBytes == null) {
            return stickyPartitionCache.partition(topic, cluster);
        }
        // 基于key 的hash值选择一个翻去
        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
    }

    public void close() {}
  
    /**
     * 轮询选择分区
     */
    public void onNewBatch(String topic, Cluster cluster, int prevPartition) {
        stickyPartitionCache.nextPartition(topic, cluster, prevPartition);
    }
}

</code></pre>
<pre><code>package org.apache.kafka.clients.producer;

import org.apache.kafka.common.Configurable;
import org.apache.kafka.common.Cluster;

import java.io.Closeable;

/**
 * 分区器接口，自定义分区器时，需要实现此接口。
 */
public interface Partitioner extends Configurable, Closeable {

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param key The key to partition on (or null if no key)
     * @param keyBytes The serialized key to partition on( or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes The serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);

    /**
     * 关闭分区器时执行相关操作
     */
    public void close();


    /**
     * 通知分区器有一个已创建的新批处理数据。当时用粘性分区器时，次方法会自动为新批处理数据选择一个粘性分区。
     * @param topic The topic name
     * @param cluster The current cluster metadata
     * @param prevPartition The partition previously selected for the record that triggered a new batch
     */
    default public void onNewBatch(String topic, Cluster cluster, int prevPartition) {
    }
}

</code></pre>
<h5 id="拦截器">拦截器</h5>
<p>生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。消费者拦截器主要在消费到消息或在提交消费位移时进行一些定制化的操作。自定义时需要实现org.apache.kafka.clients.producer. ProducerInterceptor</p>
<pre><code>package org.apache.kafka.clients.producer;

import org.apache.kafka.common.Configurable;

/**
 * 一个插件接口，允许拦截（并可能改变）生产者发布到kafka集群中的记记录
*/
public interface ProducerInterceptor&lt;K, V&gt; extends Configurable {
    /**
     * 将消息序列化及分区（如果分区未指定）前对消息进行相关定制化处理。
     * @param record the record from client or the record returned by the previous interceptor in the chain of interceptors.
     * @return producer record to send to topic/partition
     */
    public ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; record);

    /**
     * 当发送到服务器的记录已被确认，或者在发送到服务器之前发送记录失败时调用此方法。
     * 此方法通常在调用用户callback之前调用
     * 此方法通常运行在 Producer 的 background I/O线程中，因此这个方法中的实现代码逻辑越简单越好，否则会影响消息的发送速度。
     * 
     * This method will generally execute in the background I/O thread, so the implementation should be reasonably fast.
     * Otherwise, sending of messages from other threads could be delayed.
     *
     * @param metadata The metadata for the record that was sent (i.e. the partition and offset).
     *                 If an error occurred, metadata will contain only valid topic and maybe
     *                 partition. If partition is not given in ProducerRecord and an error occurs
     *                 before partition gets assigned, then partition will be set to RecordMetadata.NO_PARTITION.
     *                 The metadata may be null if the client passed null record to
     *                 {@link org.apache.kafka.clients.producer.KafkaProducer#send(ProducerRecord)}.
     * @param exception The exception thrown during processing of this record. Null if no error occurred.
     */
    public void onAcknowledgement(RecordMetadata metadata, Exception exception);

    /**
     * 用于关闭拦截器时执行逻辑，例如清理相关资源等
     */
    public void close();
}

</code></pre>
<h5 id="处理顺序">处理顺序</h5>
<p>拦截器-&gt;序列化器-&gt;分区器；KafkaProducer 在将消息序列化和计算分区之前会调用生产者拦截器的 onSend() 方法来对消息进行相应的定制化操作。然后生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka。最后可能会被发往分区器为消息分配分区。</p>
<h3 id="消费者consumer">消费者(Consumer)</h3>
<h4 id="简介-4">简介</h4>
<p>kafka consumer 订阅感兴趣的主题，同时向其所在的主分区发送fetch请求，获取需要进行消费的消息；需要注意的是，此过程中，consumer的每个请求都需要在partition中指定offset，从而消费从offset处开始的message。因此，consumer对offset的控制便尤为重要，可以依此来进行回退重消费操作。</p>
<h4 id="关于offset">关于offset</h4>
<p>大多数消息系统都在 broker 上保存被消费消息的元数据。也就是说，当消息被传递给 consumer，broker 要么立即在本地记录该事件，要么等待 consumer 的确认后再记录。这是一种相当直接的选择，而且事实上对于单机服务器来说，也没其它地方能够存储这些状态信息。</p>
<p>由于大多数消息系统用于存储的数据结构规模都很小，所以这也是一个很实用的选择，因为只要 broker 知道哪些消息被消费了，就可以在本地立即进行删除，一直保持较小的数据量。</p>
<p>然而，此过程中要一直保持broker与consumer的数据一致性不是一件容易的事；例如consumer已消费了数据，但是回执给broker时出现网络错误导致broker没有删除本地记录，这时消息便有可能会重复消费；并且这种记录操作对于broker而言也是一个不小的性能负担（首先对其加锁，确保该消息只被发送一次，然后将其永久的标记为 consumed，以便将其移除）。</p>
<p>为了应对上述问题，Kafka 使用了完全不同的方式来解决消息丢失问题。</p>
<p>首先Kafka 的 topic 被分割成了一组完全有序的 partition，其中每一个 partition 在任意给定的时间内只能被每个订阅了这个 topic 的 consumer group中的一个 consumer 消费。这意味着 partition 中 每一个 consumer 的位置仅仅是一个数字，即下一条要消费的消息的 offset。这使得被消费的消息的状态信息相当少，每个 partition 只需要一个数字。这个状态信息还可以作为周期性的 checkpoint。这以非常低的代价实现了和消息确认机制等同的效果。</p>
<p>这种方式还有一个附加的好处，那就是consumer 可以回退到之前的 offset 来再次消费之前的数据，这个操作违反了队列的基本原则，但事实证明对大多数 consumer 来说这是一个必不可少的特性。 例如，如果 consumer 的代码有 bug，并且在 bug 被发现前已经有一部分数据被消费了，那么 consumer 可以在 bug 修复后通过回退到之前的 offset 来再次消费这些数据。需要注意的是，对于重复消费的消息要保持幂等操作。</p>
<h4 id="push-vs-pull">Push vs Pull</h4>
<p>关于消息的推送拉取，kafka采用的是：Producer push数据到broker，Consumer从broker pull数据。针对消费者而言，无论是pull-based还是push-based都有各自的优缺点。</p>
<p>pull-based：数据传输速率由Consumer控制，可防止服务因大量数据冲击而宕机，同时也简化了broker的设计；且，此模式中，Producer可以达到最大化量产消息。当然，其缺点就是如果 broker 中没有数据，consumer 可能会在一个紧密的循环中结束轮询，实际上却是在忙于等待数据的到达。为了避免 busy-waiting，kafka在 pull 请求中加入参数，使得 consumer 在一个“long pull”中阻塞等待，直到数据到来（还可以选择等待给定字节长度的数据来确保传输长度）。</p>
<p>push-based：消息能最快的被指定消费者所消费，但是相应的broker设计将会更复杂，且数据传输速率过大时，容易冲击消费者服务，导致其宕机无法提供服务。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka安装&使用&配置]]></title>
        <id>https://philosopherzb.github.io/post/kafka-an-zhuang-andshi-yong-andpei-zhi/</id>
        <link href="https://philosopherzb.github.io/post/kafka-an-zhuang-andshi-yong-andpei-zhi/">
        </link>
        <updated>2022-06-04T08:42:31.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka安装，使用以及spring-kafka配置信息。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/lake-6278825_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="win10下部署kafka">Win10下部署kafka</h2>
<p>主要涉及Java，zookeeper，kafka，步骤如下。</p>
<h3 id="java安装">Java安装</h3>
<p>JDK下载：<a href="https://www.oracle.com/java/technologies/javase-downloads.html">点此此处跳转官网下载页面</a></p>
<p>安装过程比较简单，基本都是下一步即可，唯一需要注意的是环境变量的设置；安装完后效果如下图所示（注意：Java安装路径不要太深（目录过多），同时文件夹命名不要存在空格，否则将导致kafka部署失败）</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314001.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314002.png" alt="img" loading="lazy"></figure>
<h3 id="zookeeper安装">zookeeper安装</h3>
<p>下载地址：<a href="https://zookeeper.apache.org/releases.html">点击此处跳转官网下载页面</a></p>
<p>找一个稳定的版本下载即可：</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314003.png" alt="img" loading="lazy"></figure>
<ol>
<li>下载后解压到一个目录：eg: D:\Program\zookeeper\zookeeper-3.4.14</li>
<li>在zookeeper-3.4.14目录下，新建两个文件夹，并命名(eg: data,log)，(路径：D:\Program\zookeeper\zookeeper-3.4.14\data,D:\Program\zookeeper\zookeeper-3.4.14\log)</li>
<li>进入Zookeeper设置目录，eg: D:\Program\zookeeper\zookeeper-3.4.14\conf；复制“zoo_sample.cfg”副本并将副本重命名为“zoo.cfg”；在任意文本编辑器（eg：记事本）中打开zoo.cfg；找到并编辑dataDir=D:\Program\zookeeper\zookeeper-3.4.14\data；dataLogDir=D:\Program\zookeeper\zookeeper-3.4.14\log</li>
<li>进入D:\Program\zookeeper\zookeeper-3.4.14\bin目录，双击zkServer.cmd即可运行zk。</li>
</ol>
<h3 id="kafka安装">kafka安装</h3>
<p>下载地址：<a href="http://kafka.apache.org/downloads.html">点击此处跳转官网下载页面</a></p>
<p>2.8之后kafka支持不依赖zookeeper启动：<a href="https://kafka.apache.org/quickstart">点击此处跳转详细说明页面</a></p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314004.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>下载后解压缩。eg: D:\Program\kafka_2.13-2.7.0</p>
</li>
<li>
<p>建立一个空文件夹 logs. eg: D:\Program\kafka_2.13-2.7.0\logs</p>
</li>
<li>
<p>进入config目录，编辑 server.properties文件(eg: 用“写字板”打开)。；找到并编辑log.dirs=D:\Program\kafka_2.13-2.7.0\logs；找到并编辑zookeeper.connect=localhost:2181。表示本地运行。(Kafka会按照默认，在9092端口上运行，并连接zookeeper的默认端口：2181)</p>
</li>
<li>
<p>运行：命令行下切入D:\Program\kafka_2.13-2.7.0目录，随后执行如下命令即可（注意先开启zookeeper）：.\bin\windows\kafka-server-start.bat .\config\server.properties</p>
</li>
<li>
<p>可能存在的报错：</p>
<p>输入行太长。 命令语法不正确：目录树太深了，减少几个目录即可。</p>
<p>找不到或无法加载主类 Files\java\jdk8\lib;D:\Program：此错误由目录存在空格所导致。解决：找到D:\Program\kafka_2.13-2.7.0\bin\windows，用编辑器（eg：记事本）打开kafka-run-class.bat，查看配置是否有加上双引号</p>
</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314005.png" alt="img" loading="lazy"></figure>
<p>如果加了双引号仍然无法启动，那可以去查看java安装路径中的文件夹是否存在空格，如果存在，将空格去掉即可。</p>
<p>启动图如下：</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314006.png" alt="img" loading="lazy"></figure>
<h2 id="简单kafka命令">简单kafka命令</h2>
<p>部分kafka命令，基于win10（如果非win系统，使用bin目录下的.sh即可）</p>
<h3 id="创建topic两个副本-四个分区">创建topic（两个副本。四个分区）</h3>
<pre><code>kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic testTopic  
$ bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --partitions 3 --replication-factor 3 --topic topic_test( Kafka 版本 &gt;= 2.2 支持此方式（推荐）)  
$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
</code></pre>
<h3 id="查看topic">查看topic</h3>
<p>查看topic列表：kafka-topics.bat --list --zookeeper localhost:2181</p>
<p>查看topic详情：bin/kafka-topics.sh --zookeeper host12:2181  --describe --topic ltopicName</p>
<pre><code>$ bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092
Topic:quickstart-events  PartitionCount:1    ReplicationFactor:1 Configs:
Topic: quickstart-events Partition: 0    Leader: 0   Replicas: 0 Isr: 0
</code></pre>
<p>清除Kafka topic下所有消息：kafka-topics.sh --zookeeper zookeeper地址:端口 --delete --topic topic_name</p>
<h3 id="删除topic">删除topic</h3>
<p>linux：./bin/kafka-topics  --delete --zookeeper 【zookeeper server】  --topic 【topic name】</p>
<p>win：kafka-topics.bat --delete --zookeeper localhost:2181 --topic testTopic</p>
<p>如果kafaka启动时加载的配置文件中server.properties没有配置delete.topic.enable=true，那么此时的删除并不是真正的删除，而是把topic标记为：marked for deletion</p>
<p>彻底删除topic，可以如下操作：</p>
<ol>
<li>删除kafka存储目录（server.properties文件log.dirs配置，默认为&quot;/tmp/kafka-logs&quot;）相关topic目录</li>
<li>登录zookeeper客户端：命令：./bin/zookeeper-client</li>
<li>找到topic所在的目录：ls /brokers/topics</li>
<li>找到要删除的topic，执行命令：rmr /brokers/topics/【topic name】即可，此时topic被彻底删除。</li>
</ol>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314007.png" alt="img" loading="lazy"></figure>
<h3 id="发送消息至topic">发送消息至topic</h3>
<pre><code>$ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
This is my first event
This is my second event
</code></pre>
<h3 id="消费topic消息">消费topic消息</h3>
<p>从头开始查看kafka topic下的数据：kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic_name --from-beginning</p>
<p>按照偏移量查看topic下数据：kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic_name --offset latest --partition 0</p>
<p>--offset设置偏移量 latest代表最后 ，可以设置区间，不设置结尾的话默认为查询到latest(最后)</p>
<p>--partition 设置分区 使用偏移量查询时一定要设置分区才能查询</p>
<pre><code>$ bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
This is my first event
This is my second event
</code></pre>
<h2 id="spring-kafka属性配置">spring-kafka属性配置</h2>
<p>spring已经封装了kafka，所以在springboot项目中可以非常便捷的集成kafka，引入其依赖，如下：</p>
<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;
  &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;
  &lt;version&gt;${last.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>文档地址：<a href="https://docs.spring.io/spring-kafka/docs/current/reference/html/">点击此处跳转官网页面</a></p>
<p>属性配置：<a href="https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html#spring.kafka.admin.client-id">点击此处跳转说明页面</a></p>
<h3 id="producer的配置参数">producer的配置参数</h3>
<pre><code>#procedure要求leader在考虑完成请求之前收到的确认数，用于控制发送记录在服务端的持久化，其值可以为如下：
#acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为-1。
#acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。
#acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1的设置。
#可以设置的值为：all(-1), 0, 1
spring.kafka.producer.acks=1
 
#每当多个记录被发送到同一分区时，生产者将尝试将记录一起批量处理为更少的请求， 
#这有助于提升客户端和服务器上的性能，此配置控制默认批量大小（以字节为单位），默认值为16384
spring.kafka.producer.batch-size=16384
 
#以逗号分隔的主机：端口对列表，用于建立与Kafka集群的初始连接
spring.kafka.producer.bootstrap-servers
 
#生产者可用于缓冲等待发送到服务器的记录的内存总字节数，默认值为33554432
spring.kafka.producer.buffer-memory=33554432
 
#ID在发出请求时传递给服务器，用于服务器端日志记录
spring.kafka.producer.client-id
 
#生产者生成的所有数据的压缩类型，此配置接受标准压缩编解码器（'gzip'，'snappy'，'lz4'），
#它还接受'uncompressed'以及'producer'，分别表示没有压缩以及保留生产者设置的原始压缩编解码器，
#默认值为producer
spring.kafka.producer.compression-type=producer
 
#key的Serializer类，实现类实现了接口org.apache.kafka.common.serialization.Serializer
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
 
#值的Serializer类，实现类实现了接口org.apache.kafka.common.serialization.Serializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
 
#如果该值大于零时，表示启用重试失败的发送次数
spring.kafka.producer.retries
</code></pre>
<h3 id="consumer的配置参数">consumer的配置参数</h3>
<pre><code>#当'enable.auto.commit'为true时，该配置表示消费者offset自动提交给Kafka的频率（以毫秒为单位），默认值为5000。
spring.kafka.consumer.auto-commit-interval;
 
#当Kafka中没有初始偏移量或者服务器上不再存在当前偏移量时该怎么办，默认值为latest，表示自动将偏移重置为最新的偏移量
#可选的值为latest, earliest, none
spring.kafka.consumer.auto-offset-reset=latest;
 
#以逗号分隔的主机：端口对列表，用于建立与Kafka群集的初始连接。
spring.kafka.consumer.bootstrap-servers;
 
#ID在发出请求时传递给服务器;用于服务器端日志记录。
spring.kafka.consumer.client-id;
 
#如果为true，则消费者的偏移量将在后台定期提交，默认值为true(2.3后默认为false)
spring.kafka.consumer.enable-auto-commit=true;
 
#如果没有足够的数据立即满足“fetch.min.bytes”给出的要求，服务器在回答获取请求之前将阻塞的最长时间（以毫秒为单位）
#默认值为500
spring.kafka.consumer.fetch-max-wait;
 
#服务器应以字节为单位返回获取请求的最小数据量，默认值为1，对应的kafka的参数为fetch.min.bytes。
spring.kafka.consumer.fetch-min-size;
 
#用于标识此使用者所属的使用者组的唯一字符串。
spring.kafka.consumer.group-id;
 
#心跳与消费者协调员之间的预期时间（以毫秒为单位），默认值为3000
spring.kafka.consumer.heartbeat-interval;
 
#密钥的反序列化器类，实现类实现了接口org.apache.kafka.common.serialization.Deserializer
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
 
#值的反序列化器类，实现类实现了接口org.apache.kafka.common.serialization.Deserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
 
#一次调用poll()操作时返回的最大记录数，默认值为500
spring.kafka.consumer.max-poll-records;
</code></pre>
<h3 id="listener的配置参数">listener的配置参数</h3>
<pre><code>#侦听器的AckMode
#当enable.auto.commit的值设置为false时，该值会生效；为true时不会生效
spring.kafka.listener.ack-mode;
 
#在侦听器容器中运行的线程数
spring.kafka.listener.concurrency;
 
#轮询消费者时使用的超时（以毫秒为单位）
spring.kafka.listener.poll-timeout;
 
#当ackMode为“COUNT”或“COUNT_TIME”时，偏移提交之间的记录数
spring.kafka.listener.ack-count;
 
#当ackMode为“TIME”或“COUNT_TIME”时，偏移提交之间的时间（以毫秒为单位）
spring.kafka.listener.ack-time;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[敏感数据加密方案]]></title>
        <id>https://philosopherzb.github.io/post/min-gan-shu-ju-jia-mi-fang-an/</id>
        <link href="https://philosopherzb.github.io/post/min-gan-shu-ju-jia-mi-fang-an/">
        </link>
        <updated>2022-05-31T07:43:18.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述一种基于云服务的敏感数据加密设计方案。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/sunset-1117008_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="引言">引言</h2>
<h3 id="需求描述">需求描述</h3>
<p>为了防止敏感数据泄露，造成资产损失，故需要对相关数据进行加密处理；同时，允许部分加密数据支持模糊搜索。</p>
<h2 id="详细设计">详细设计</h2>
<h3 id="总体设计">总体设计</h3>
<p>加密服务提供针对敏感数据存储的加密能力，用于防止因外部或内部安全威胁所导致的数据泄露问题，从而提高数据安全的防护水平。</p>
<p>基本功能：</p>
<ul>
<li>基础安全保障：加解密从根本上夯实了数据安全性。对敏感字段加密后，可以有效防止数据库内容被直接盗取。且密钥以租户维度隔离，有效解决应用的水平权限隔离问题。</li>
<li>数据库内容和密钥存储管理分离：接入方只存储加密数据，不保存密钥。只需接入本产品提供的SDK即可（实现细节将由SDK处理）。这在增强安全系数的同时，也简化了开发者管理、存储密钥的成本。</li>
<li>安全与服务平滑性兼得：SDK提供智能的、丰富的api可以自动识别数据库中存量密文的版本、自动加密、解密。接入方引入SDK后，可以做到在不停服务的条件下进行密钥升级。</li>
<li>接入简单便捷：加解密方案不依赖硬件，只需要引入加解密SDK即可，对数据库无特殊要求，使用成本、改造成本低。</li>
<li>保证数据完整性传输：所有接口都将使用非对称加密RSA进行签名校验，保证了数据的完整性。</li>
</ul>
<h3 id="整体业务流程">整体业务流程</h3>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310009.png" alt="img" loading="lazy"></figure>
<h4 id="接入方使用">接入方使用</h4>
<p>引入加解密SDK，初始化client后，直接调用对应的函数即可。</p>
<h4 id="加解密云服务">加解密云服务</h4>
<p>1、允许用户输入应用级别的RSA公私钥，应用公私钥功能：应用使用私钥加签数据，云服务使用公钥验签数据。此值支持变更。</p>
<p>2、自动生成云服务RSA公私钥，云服务公私钥功能：云服务使用私钥加签数据，应用使用公钥验签数据。此值不支持变更。</p>
<p>3、自动生成AES密钥（外部获取此值时，将会被RSA加密），此密钥不展示。此值支持变更。</p>
<p>4、自动生成伪随机码（日期+雪花算法生成），用于验伪操作，客户端需持有此参数并传递。此值支持变更。</p>
<p>5、自动生成appid，用于唯一标识一个应用（同一个客户允许存在多个appid），云服务前缀+雪花算法生成。</p>
<p>6、appId加version为组合唯一索引；其中version默认为0，每次变更AES密钥时，version+1。</p>
<p>7、允许设置过期时间（默认90天），最大有效期（默认120天，必须大于过期时间）；单位：天。主要用于客户端本地缓存的有效时长。</p>
<p>8、允许设置滑动窗口（默认4）大小及压缩长度（默认3）；用于加密search类型的数据。</p>
<h3 id="整体技术方案">整体技术方案</h3>
<h4 id="关于算法">关于算法</h4>
<p>对称加密与非对称加密混合使用：</p>
<ol>
<li>云服务自动生成出一对秘钥pub/pri。将私钥保密，将公钥公开。</li>
<li>接入方请求云服务时，拿到云服务的公钥pub。</li>
<li>云服务通过AES计算出一个对称加密的秘钥X。 然后使用应用公钥将X进行加密。</li>
<li>接入方发送加签请求到云服务获取加密后的秘钥X，云服务对请求进行验签，通过了方才返回秘钥X。</li>
<li>接入方得到加密后的秘钥X，便可以使用应用私钥解密，得到AES秘钥。</li>
<li>之后便可以使用AES秘钥进行敏感数据的加解密操作。</li>
</ol>
<p>注意：接入方不需要额外的开发，只需引入云服务SDK，并调用对应的接口即可。</p>
<h5 id="rsa">RSA</h5>
<p>将用于签名（保证数据完整性）及加密（保证数据安全性）</p>
<table>
<thead>
<tr>
<th>开放平台签名算法名称</th>
<th>标准签名算法名称</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>RSA2</td>
<td>SHA256WithRSA</td>
<td>强制要求 RSA 密钥的长度至少为 2048</td>
</tr>
<tr>
<td>RSA</td>
<td>SHA1WithRSA</td>
<td>对 RSA 密钥的长度不限制，推荐使用 2048 位以上</td>
</tr>
</tbody>
</table>
<p>由于计算能力的飞速发展，从安全性角度考虑，云服务推荐使用 SHA256WithRSA 的签名算法。该算法在摘要算法上比 SHA1WithRSA 有更强的安全能力。</p>
<p>注意避免公私钥混用：不同签名算法的签名密钥是隔离的。由于同时提供了两套签名算法，若选择了特定的签名算法，请保证使用对应的私钥签名，同时使用对应的云服务公钥进行验签。</p>
<h5 id="aes">AES</h5>
<p>将用于加解密敏感数据，此数据传输时，会被RSA加密保护，算法特性如下：</p>
<ol>
<li>加密算法使用“AES/CBC/PKCS5Padding”</li>
<li>加密用的初始向量会直接编码到加密数据中, 因此相同数据内容多次加密的结果不同</li>
<li>不同应用使用不同的秘钥，不同应用间加密数据无法解密（以appId作为区分）</li>
<li>支持秘钥升级, 密文中会包含加密密文用的秘钥版本，更新后支持老版本密文解密（appId + version 获取唯一AES秘钥）</li>
<li>传输过程AES秘钥将会被应用RSA公钥加密，返回时需要使用应用RSA私钥解密。</li>
</ol>
<h4 id="加密类型">加密类型</h4>
<table>
<thead>
<tr>
<th>类型</th>
<th>phone</th>
<th>id</th>
<th>simple</th>
<th>search</th>
</tr>
</thead>
<tbody>
<tr>
<td>描述</td>
<td>手机号</td>
<td>有规律的数字，例如身份证</td>
<td>普通文本类型</td>
<td>支持模糊搜索的文本类型</td>
</tr>
</tbody>
</table>
<p>1、phone，id为有规律的数字, 其加密算法只支持使用尾号后4位搜索。如13912345678的手机号加密后可用5678搜索。</p>
<p>2、接入方如果存在phone，id类的加密类型，建议增加一列检索串（创建索引），用于匹配搜索（List<code>&lt;DO&gt;</code> objects =  SELECT * FROM table WHERE phone=‘encryptedData’）；如果不想增加额外的检索串，也可以在原有的加密手机号字段上建立前缀索引，可缩短一定的模糊匹配查询时间；当然也可以在加密手机号字段上直接建立普通索引，因为加密后的索引串会放在整个加密字符的最前方（List<code>&lt;DO&gt;</code> objects =  SELECT * FROM table WHERE phone like ‘encryptedData%’）。关于如何截取检索串可在附录中查看。</p>
<p>3、simple类型只进行加密，不支持模糊搜索。</p>
<p>4、search类型支持模糊搜索，基本实现原理是根据4位英文字符（半角），2个中文字符（全角）为一个检索条件。将一个字段拆分为多个。</p>
<p>比如：test123，使用4个字符为一组的加密方式。切割结果为：[test, est1, st12, t123]，第一组 test，第二组est1，第三组st12，第四组 t123… 依次类推，如果需要检索 所有包含 检索条件4个字符的数据 比如：test，加密字符后通过key like “%partial%” 查库。</p>
<p>因为密文检索开启后 密文长度会膨胀几倍以上，如果没有强需求建议不开启。</p>
<p>使用这种方式存在一定的代价：</p>
<ul>
<li>支持模糊查询加密方式，产出的密文比较长；</li>
<li>支持的模糊查询子句长度必须大于等于4个英文/数字，或者2个汉字。不支持过短的查询(出于安全考虑)；</li>
<li>如果数据本身长度不够4个数字或2个汉字, 则此时输入全文即可搜索.  如: 原始数据为&quot;安&quot;的, 使用&quot;安&quot;调用平台搜索接口即可获取搜索用文本；</li>
<li>返回的结果列表中有可能有多余的结果，需要增加筛选的逻辑：对记录先解密，再筛选；</li>
</ul>
<h4 id="加密格式">加密格式</h4>
<p>注意：其中 Index 是检索信息，同一份数据多次加密后的结果都不会变。</p>
<p>手机号(phone)和身份证(id)格式，其中手机号(phone)分隔符：SEP=$；身份证(id)分隔符：SEP=#</p>
<table>
<thead>
<tr>
<th>SEP</th>
<th>Index</th>
<th>SEP</th>
<th>EncryptedData</th>
<th>SEP</th>
<th>Version</th>
<th>SEP</th>
<th>SEP</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>search类文本，SEP=~</p>
<table>
<thead>
<tr>
<th>SEP</th>
<th>EncryptedData</th>
<th>SEP</th>
<th>Index</th>
<th>SEP</th>
<th>Version</th>
<th>SEP</th>
<th>SEP</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>simple类文本，SEP=~</p>
<table>
<thead>
<tr>
<th>SEP</th>
<th>EncryptedData</th>
<th>SEP</th>
<th>Version</th>
<th>SEP</th>
</tr>
</thead>
<tbody></tbody>
</table>
<h4 id="加密样例">加密样例</h4>
<p>加密算法支持模糊搜索(可选)，模糊加密的数据比一般加密更长一些，大概长度扩大 10-15 倍。</p>
<p>加密数据的示例如下：</p>
<p>phone(手机号)加密方式，加密类型：phone</p>
<table>
<thead>
<tr>
<th>手机号</th>
<th>13012345678</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>$mMHmK1rfa+OZ23eYhO1AUQ==$q6cFTUyW6SuJD2NReTUVBQ==$1$$</td>
</tr>
<tr>
<td>后四位手机号</td>
<td>5678</td>
</tr>
<tr>
<td>5678检索串</td>
<td>mMHmK1rfa+OZ23eYhO1AUQ==</td>
</tr>
</tbody>
</table>
<p>身份证加密方式，加密类型：id</p>
<table>
<thead>
<tr>
<th>身份证</th>
<th>200300190002039898</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>#uJfxXOF0EJ+6V8H51L9rdg==#ao/gDujB17PTObfSjeAOvl/YxmScgYuxQPl2wokwlec=#1##</td>
</tr>
<tr>
<td>后四位身份证</td>
<td>9898</td>
</tr>
<tr>
<td>9898检索串</td>
<td>uJfxXOF0EJ+6V8H51L9rdg==</td>
</tr>
</tbody>
</table>
<p>普通文本加密方式，加密类型：simple</p>
<table>
<thead>
<tr>
<th>地址</th>
<th>浙江省杭州市滨江区星耀城</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>~thzORLtQtSKYQZW2UFvM64bMN5NhDPb8PSRZ5KNLtl3ZJ+sIYfzMN5gxPy7+ba4o~1~</td>
</tr>
</tbody>
</table>
<p>支持搜索的文本加密方式，加密类型：search</p>
<table>
<thead>
<tr>
<th>域名</th>
<th>www.test.hostname.com</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>~J/7ro/3scxjvrPXFB5+OxxOZdv45iqXbkP4doNfcEAo=~Q9bpt3Na0G8oOwOe1i9y+Eanjg7Tfr5U3vcYuMyC2iZi7ciYfvC+bNP6pS29uUPpo4L3spYx~1~~</td>
</tr>
<tr>
<td>需要查询的字符</td>
<td>test.hostname</td>
</tr>
<tr>
<td>test.hostname检索串</td>
<td>1i9y+Eanjg7Tfr5U3vcYuMyC2iZi7ciYfvC+bNP6</td>
</tr>
</tbody>
</table>
<h4 id="加密长度">加密长度</h4>
<p>加密后由于增加了密文信息, 将比原文要长, 各种常见字段加密后的数据长度参考表如下:</p>
<p>注: 当发生加密秘钥version字段变更时, 密文长度可能会发生变化. 建议设计字段时在表中字段上加10个左右字符。</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>密文长度（支持搜索）</th>
<th>密文长度（不支持搜索）（使用simple类型加密）</th>
</tr>
</thead>
<tbody>
<tr>
<td>phone（手机号）</td>
<td>54</td>
<td>28</td>
</tr>
<tr>
<td>id（身份证为例）</td>
<td>74</td>
<td>48</td>
</tr>
<tr>
<td>文本数据（5字符，全汉字）</td>
<td>46</td>
<td>28</td>
</tr>
<tr>
<td>文本数据（10字符，全汉字）</td>
<td>86</td>
<td>48</td>
</tr>
<tr>
<td>文本数据（20字符，全汉字）</td>
<td>170</td>
<td>92</td>
</tr>
<tr>
<td>文本数据（40字符，全汉字）</td>
<td>334</td>
<td>176</td>
</tr>
</tbody>
</table>
<h2 id="数据库设计">数据库设计</h2>
<h3 id="数据库ddl">数据库DDL</h3>
<pre><code>-- DROP TABLE IF EXISTS `tb_app_config_aes`;
CREATE TABLE `tb_app_config_aes`
(
  `id`             bigint(20)  NOT NULL AUTO_INCREMENT COMMENT '自增主键',
  `app_id`         varchar(64) NOT NULL COMMENT '应用id，用于唯一标识应用；生成规则：云服务前缀+雪花算法生成',
  `aes_key`        char(24)    NOT NULL COMMENT 'aes秘钥,固定长度',
  `secret_version` bigint(16)  NOT NULL COMMENT '版本号，用于升级aesKey',
  `is_delete`      tinyint(1)  NOT NULL DEFAULT 0 COMMENT '删除标识，0-未删除，1-已删除',
  `create_time`    datetime    NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `modify_time`    datetime    NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`),
  unique key `unique_index_app_id_secret_version` (`app_id`, `secret_version`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8 COMMENT ='AES秘钥配置表';
  
-- DROP TABLE IF EXISTS `tb_app_config_rsa`;
CREATE TABLE `tb_app_config_rsa`
(
  `id`                         bigint(20)    NOT NULL AUTO_INCREMENT COMMENT '自增主键',
  `app_id`                     varchar(64)   NOT NULL COMMENT '应用id，用于唯一标识应用；生成规则：云服务前缀+雪花算法生成',
  `tenant_id`                  varchar(128)  NOT NULL COMMENT '租户id',
  `tenant_name`                varchar(255)  NOT NULL COMMENT '租户名称',
  `app_pub_key`                varchar(512)  NOT NULL COMMENT '应用公钥，用于验签接口，同时加密AES秘钥',
  `das_pub_key`                varchar(512)  NOT NULL COMMENT '云服务公钥，用于接入方验签',
  `das_pri_key`                varchar(2048) NOT NULL COMMENT '云服务私钥，云服务使用此私钥进行加签',
  `encrypt_slide_size`         int(3)        NOT NULL DEFAULT 4 COMMENT '滑动窗口大小',
  `encrypt_index_compress_len` int(3)        NOT NULL DEFAULT 3 COMMENT '密文滑窗压缩长度',
  `random_num`                 varchar(64)   NOT NULL COMMENT '伪随机码，生成规则：日期+雪花算法生成',
  `invalid_time`               int(4)        NOT NULL DEFAULT 90 COMMENT '过期时间，单位：天，默认90天',
  `max_invalid_time`           int(4)        NOT NULL DEFAULT 120 COMMENT '最大有效期，单位：天，默认120天；必须大于invalidTime',
  `is_delete`                  tinyint(1)    NOT NULL DEFAULT 0 COMMENT '删除标识，0-未删除，1-已删除',
  `create_time`                datetime      NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `modify_time`                datetime      NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`),
  unique key `unique_index_app_id` (`app_id`),
  unique key `unique_index_tenant_id` (`tenant_id`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8 COMMENT ='RSA秘钥配置表';

</code></pre>
<h2 id="附录">附录</h2>
<h3 id="rsa公私钥生成代码-java版">RSA公私钥生成代码-java版</h3>
<pre><code>import java.security.KeyPair;
import java.security.KeyPairGenerator;
import java.security.NoSuchAlgorithmException;
import java.security.PrivateKey;
import java.security.PublicKey;
import java.security.SecureRandom;
import java.util.Base64;
 
/**
 * @author philosopherZB
 * @date 2022/1/13
 */
public class GenerateKeyUtils {
    private static KeyPair KEY_PAIR_INSTANCE = null;
    private static String ALGORITHM = &quot;RSA&quot;;
    private static Integer KEY_LENGTH = 2048;
 
    /**
     * 生成默认公钥--algorithm=RSA; keyLength=2048
     *
     * @return publicKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genDefaultPublicKey() throws NoSuchAlgorithmException {
        return genPublicKey(ALGORITHM, KEY_LENGTH);
 
    }
 
    /**
     * 生成默认私钥--algorithm=RSA; keyLength=2048
     *
     * @return privateKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genDefaultPrivateKey() throws NoSuchAlgorithmException {
        return genPrivateKey(ALGORITHM, KEY_LENGTH);
    }
 
    /**
     * @param algorithm 标准的算法名: https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html
     * @param keyLength 指定长度
     * @return publicKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genPublicKey(String algorithm, int keyLength) throws NoSuchAlgorithmException {
        PublicKey publicKey = getKeyPairInstance(algorithm, keyLength).getPublic();
        return Base64.getEncoder().encodeToString(publicKey.getEncoded());
 
    }
 
    /**
     * @param algorithm 标准的算法名: https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html
     * @param keyLength 指定长度
     * @return privateKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genPrivateKey(String algorithm, int keyLength) throws NoSuchAlgorithmException {
        PrivateKey privateKey = getKeyPairInstance(algorithm, keyLength).getPrivate();
        return Base64.getEncoder().encodeToString(privateKey.getEncoded());
    }
 
    /**
     * getKeyPairInstance
     *
     * @param algorithm 标准的算法名: https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html
     * @param keyLength 指定长度
     * @return KeyPair
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    private static KeyPair getKeyPairInstance(String algorithm, int keyLength) throws NoSuchAlgorithmException {
        if (KEY_PAIR_INSTANCE == null) {
            KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(algorithm);
            SecureRandom secureRandom = new SecureRandom();
            keyPairGenerator.initialize(keyLength, secureRandom);
            KEY_PAIR_INSTANCE = keyPairGenerator.generateKeyPair();
        }
 
        return KEY_PAIR_INSTANCE;
    }
 
    public static void main(String[] args) throws NoSuchAlgorithmException {
        System.out.println(&quot;publicKey: &quot; + genPublicKey(&quot;RSA&quot;, 2048));
        System.out.println(&quot;privateKey: &quot; + genPrivateKey(&quot;RSA&quot;, 2048));
 
        System.out.println(&quot;default publicKey: &quot; + genDefaultPublicKey());
        System.out.println(&quot;default privateKey: &quot; + genDefaultPrivateKey());
    }
}

</code></pre>
<h3 id="检索串提取代码示例-java版">检索串提取代码示例-java版</h3>
<pre><code>public static String extractIndex(String encryptedData) {
    if (encryptedData == null || encryptedData.length() &lt; 4) {
        return null;
    }
    char sepInData = encryptedData.charAt(0);
    if (encryptedData.charAt(encryptedData.length() - 2) != sepInData) {
        return null;
    }
    String[] parts = StringUtils.split(encryptedData, sepInData);
    if (sepInData == '$' || sepInData == '#') {
        return parts[0];
    } else {
        return parts[1];
    }
}

</code></pre>
<h3 id="sdk使用样例-java版">SDK使用样例-java版</h3>
<pre><code>public static void main(String[] args) {
    String privateKey = &quot;MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDGbO1R5OQ0Ff27V+k1FuPBnKMqwC3c0HFnAFu0ZUWH5FTyfkMahhsRntg7/uLk2QM1v0rx7kEdd/ExhOs5z68nO8xbyX/YbC+DX/NH3IsvNRrDU+xfpZdnuMT3bjtlGsasTIOK8S3DlHfiGpO4HPgeY7mBfduBaPStmJFLMM4Xpg3piD4r8mmq+2dAIhq6vX8GpwVqap0XLk8TcYY5h8WQ0FTbTSbRUNN/+YHmlwbEJVa+NR8qaUoo75/WHeVgjNlZ8SAfdjMt1oVWmibSiKYDr1JBVZrPD4CjBy6UR9jDrTxTrwmGsvHUzE3ZThPJZvJEWVXHbMCTs4KReUds/UcPAgMBAAECggEAXSsAM5e53wsEXFbm1VquDlax9nzODASDes3cQZPblfcMO+A1OdsGErv25BTGDJYo/6+WTQqF4IRU5991Y2u03kMhrWdrc/84QANpg7B2WfAhZN2e+zoRYU5MjbFgihSMfJJgoXik+FRaBfxcp/JSPlKs47RowNa7LFeawSdlXYxy+eXCouYomGe0aCPACKfHRyBWDRCf+eK/UwXRcmiyHi8hXOa/IfsaBsZNLtHR3rFod4x+hZUoIJAuXpupc2KW4qsK8ImW9BDcx2p2EO6iggw6MZMZkIo7NRHf3aaaLfpVsebjHm5lBF95ptVUP4CFpfoNevn3y09D4nhGjBP7AQKBgQDnZIcX+XJDykYES10jClxKndibhLYN9zNp7FOiin1Sfd5tUux+VXNEocWSMIqz8wuuI6kyrigwa0E2Kis3QANLizsaOu39jCW5kAHFm19txG3IgntPix95qADl9fQAO1Y3Nbh+zL43FH8f0s066L3V9bTvmZ6rN9vf8GF/xHrfLwKBgQDbhuUop370RhFbgN1CIFM6m6gio7pft7NW6YLyn4yXa/vbpw7c7B+c5TA1ru9maaXlF+Sff5WtLVK10czghDOdI0B5qrm62+TEhOEk7C6dgNsCdaHuDzqA2MYEmq4HJhoXrwM+xvzrlCwNZQ1AIRQWKdceKK9YgEnZQXQU9TEeIQKBgQCfPELrcLH9jLlaQzK45mxUvQNPIqjWO4OaJRP5CyzrE8t5mFM/LTbByEHaNKV+6IblM41AXzExAN5DlAlhYB/kYNAvYNZeYY+kf0F4509ojoCuN3z8ZFUot0DG/9cGQc829zUbrXJJHUXOdJbfL0NUdl4pdKIIWcxp81ZlQqT76QKBgHMlCzfKux1XTy1mpydTGzSnhoY8yLoB+dBBhQzLwQt/eUhaFMKuG1rJIANYcXuPOJO0d5dtbU27cyGpHMQ6s3PdlKj8cpTfV9v4MruSIlU8zCM7Hidm13HTwfGSTGu1gYQgqRwZdXn/ayfPdCbJ8uY5JftMrcRG7fVFjqSbgxrhAoGAVPNPW/QOVarHZDCVJi0XtOhl1g6oR51FGkOZ13SEuu8271v0PxcP55Hib6WvCPDOFB75oPaa1HmQs/WL41RCFMlELH/DhwEEkd2hLFmou4AiiB+M2DQODxagyINOx0RGd7FZuhiIWQTocqXj2PV71gkn5q4pIGKLsk1aklLTKXY=&quot;;
    String dasPubKey = &quot;MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAiPNXFw6tQyfpNi63OoR5E+VR+KS4Sy1+1EFNiHTIc/f5AXXZtmo8CVTgAM8X7a2GHLMwRqjH1JZ/2Da9HHn8zHdEtSAbObxMtTKPrnQOG5NrTuA3hfRb/4N00iZZ2KZMr5fTXJ4824VMr2/fQZySwDd0bOPTmrNnlHLu6ErFvfJwjQqbhWVC1VhRkGzvT81O2SM+ALuTnbgoFGqFyaUE9YUP57COA/Hw4Yz+GmQkHxs9ELPvikFSGdBdptDvHQ2dTprskRW8UU/v0XjVED8jeayiqKxJFn2Yejq43eqkH+SV6c9R1jE39qhMyEX7hVvzSMcyvONSo5Za4R5zLqap4wIDAQAZ&quot;;
    // 初始化一次即可（单例模式）
    SecurityClient securityClient =
            new DefaultSecurityClient(&quot;http://127.0.0.1:8999&quot;, &quot;cloud20220209651316091208540160&quot;, privateKey, &quot;20220209651322386368110592&quot;, dasPubKey);
    try {
        // 加密类型: TYPE_PHONE-手机号; TYPE_ID-id; TYPE_SIMPLE-普通加密；TYPE_SEARCH-支持搜索的加密
        // version 为AES版本号，可从云服务中查看
        String encryptData = securityClient.encrypt(&quot;www.test.hostname.com&quot;, SecurityConstants.TYPE_SEARCH, 1L);
        System.out.println(&quot;encryptData: &quot; + encryptData);

        boolean isEncryptData = securityClient.isEncryptData(encryptData, SecurityConstants.TYPE_SEARCH);
        System.out.println(&quot;isEncryptData: &quot; + isEncryptData);

        String decryptData = securityClient.decrypt(encryptData, SecurityConstants.TYPE_SEARCH, 1L);
        System.out.println(&quot;decryptData: &quot; + decryptData);

        String searchData = securityClient.search(&quot;test.hostname&quot;, SecurityConstants.TYPE_SEARCH, 1L);
        System.out.println(&quot;searchData: &quot; + searchData);
    } catch (DasSecurityException e) {
        e.printStackTrace();
    }
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[分布式事务详解]]></title>
        <id>https://philosopherzb.github.io/post/fen-bu-shi-shi-wu-xiang-jie/</id>
        <link href="https://philosopherzb.github.io/post/fen-bu-shi-shi-wu-xiang-jie/">
        </link>
        <updated>2022-05-21T06:09:37.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述单事务到分布式事务的转变以及对应分布式事务解决方案。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/iceland-1768744_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="事务演变">事务演变</h2>
<h3 id="概要">概要</h3>
<p>随着业务的不断发展以及业务复杂度的提升，传统单体服务逐渐暴露出了一些问题，如开发效率低、可维护性差、架构扩展难、部署不灵活等。因此，走上分布式微服务是一条必然的道路。</p>
<p>在分布式中，每个服务便是一个独立的进程，各个服务间可单独迭代，互不影响，甚至可以用不同的语言开发，只需事先定义好对接规范即可。</p>
<p>分布式很好地解决了单体服务的一些问题，但随之而来的便是一系列分布式难题。本篇将要讲述的便是分布式事务这个问题。</p>
<h3 id="本地事务">本地事务</h3>
<p>本地事务，一般也被称之为数据库事务。它主要功能是将多条SQL语句当做一个整体来处理，保证这一个整体要么执行成功，要么执行失败。</p>
<p>比较典型的例子就是转账服务了：A转账100给B，那么对应有A-100和B+100两条语句，事务必须保证这两条语句的整体性，否则就会出现A-100，但B没有增加100的场景。</p>
<p>上述例子体现了事务的一个重要特性：原子性。事实上，事务具备四个基本特性：原子性，一致性，隔离性，永久性；通常也简称为ACID特性。</p>
<ul>
<li>Atomicity(原子性)：一个事务便是一个整体，是不可再被分隔的最小独立单元；在一个事务中的操作，要么同时成功，要么同时失败，不会存在部分成功，部分失败的场景。</li>
<li>Consistency(一致性)：在事务开始前以及结束后，数据库的完整性没有被破坏。完整性包括约束、级联、触发器及其任意组合。</li>
<li>Isolation(隔离性)：数据库允许事务并发执行读写，为了防止出现多个事务并发写导致数据不一致，便有了隔离性。</li>
<li>Durability(持久性)：事务提交后，数据将会被永久地保存，即使系统故障也不会丢失。</li>
</ul>
<p>隔离级别：</p>
<table>
<thead>
<tr>
<th>Isolation Level</th>
<th>Dirty Reads</th>
<th>Non-Repeatable Reads</th>
<th>Phantom Reads</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read uncommitted</td>
<td>允许</td>
<td>允许</td>
<td>允许</td>
</tr>
<tr>
<td>Read committed(Sql server, Oracle)</td>
<td>不允许</td>
<td>允许</td>
<td>允许</td>
</tr>
<tr>
<td>Repeatable reads(Mysql)</td>
<td>不允许</td>
<td>不允许</td>
<td>允许</td>
</tr>
<tr>
<td>Serializable</td>
<td>不允许</td>
<td>不允许</td>
<td>不允许</td>
</tr>
</tbody>
</table>
<ul>
<li>Dirty reads：A事务可以读到B事务还未提交的数据。</li>
<li>Non-repeatable read：A事务读取一行数据，B事务后续修改了这行数据，A事务再次读取这行数据，结果得到的数据不同。</li>
<li>Phantom reads：A事务通过SELECT ... WHERE得到一些行，B事务插入新行或者删除已有的行使得这些行满足A事务的WHERE条件，A事务再次SELECT ... WHERE结果比上一次多/少了一些行。</li>
</ul>
<p>注意1：mysql默认使用RR隔离级别，这是由于binlog的格式问题（statement-记录修改的SQL语句,row-记录每行实际数据的变更,mixed-前面两种混合）所导致的，在5.0之前binlog只有statement一种格式，而主从复制时，这会导致数据的不一致。</p>
<p>注意2：其他数据库选用RC隔离级别，是由于RR隔离级别增加了间隙锁，会增加发生死锁的概率；同时，条件列未命中索引时，会锁全表，RC只会锁行。</p>
<p>注意3：RC隔离级别下，主从复制需要采用binlog的row格式，基于行的复制，这样不会出现主从不一致问题。</p>
<h3 id="分布式事务">分布式事务</h3>
<p>随着业务量的增加，单体服务所能承载的数据量越来越多，系统的运行速度逐渐下降，这时候便需要进行服务独立化（微服务）。</p>
<p>比如跨行转账，或者本行跨服务转账等都是比较典型的分布式场景；这里的每个服务都有自己独立的数据库，为了避免服务不可用，网络连接异常等情造成的数据不一致，分布式事务便应运而生了。</p>
<p>分布式事务的本质是为了多服务之间事务的正确执行，是对本地事务的一个扩展，但它只遵循部分ACID规范。所以，有必要介绍下分布式事务中的CAP定理与BASE理论。</p>
<h4 id="cap定理">CAP定理</h4>
<p>在理论计算机科学中，CAP定理(CAP theorem)，又被称为<a href="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/">布鲁尔定理(Brewer's theorem)</a>。</p>
<ul>
<li>Consistency(一致性)：此处指的是强一致性；它要求分布式系统中一个服务的写操作成功后，其他服务的读操作都是刚刚写入的数据（最新数据）。</li>
<li>Availability(可用性)：非故障服务节点收到请求后必须给予响应（非最新数据）。</li>
<li>Partition-tolerance(分区容错性)：分布式系统在遇到任何网络分区故障时，仍然可以对外提供服务，除非整个分布式系统宕机。（ Gilbert and Lync describe partitions: the network will be allowed to lose arbitrarily many messages sent from one node to another）</li>
</ul>
<p>网络分区：在分布式系统中，不同的节点分布在不同的子网络中，由于一些特殊的原因，这些子节点之间出现了网络不通的状态，但他们的内部子网络是正常的。从而导致了整个系统的环境被切分成了若干个孤立的区域，这就是分区。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310001.png" alt="img" loading="lazy"></figure>
<h4 id="cap定理验证">CAP定理验证</h4>
<p>事实上，在分布式系统中，CAP三者是无法同时存在的，且由于分布式的缘故，分区容灾是必须存在的。</p>
<p>为了验证CAP定理，假设网络中存在两个节点N1和N2，它们之间网络互通，其中N1上存在服务A和独立数据库D1，N2上存在服务B和独立数据库D2。此时，A和B是分布式系统中的一部分，同时对外提供相关服务。</p>
<ul>
<li>一致性要求A，B服务中的数据是一致的，即D1=D2。</li>
<li>可用性要求不管是请求A还是B都可以得到确认的响应。</li>
<li>分区容灾则是指出现网络故障或其他异常场景时，都不会影响A，B之间的正常运行，除非服务本身宕机。</li>
</ul>
<p>在分布式系统中，网络通信时最常见的，就比如上述N1与N2之间；现在假设N1与N2因网络故障而无法通信，为了支持这种网络异常，即要满足分区容灾，那么如果还要同时满足一致性与可用性是否可行了？</p>
<p>可以简单模拟一下：当N1与N2断开网络时，一个请求抵达服务A，更新了D1上的一条数据；此时为了一致性要求，需要将该条更新数据同步至服务B上的数据库D2，但由于网络故障，两个服务无法通信，此时便面临两种选择。</p>
<ul>
<li>牺牲一致性，保证可用性；当下次请求服务B时响应旧的数据。</li>
<li>牺牲可用性，保证一致性；阻塞等待网络恢复，将数据更新至服务B。</li>
</ul>
<h4 id="cap抉择">CAP抉择</h4>
<p>CAP三者究竟该如何选择，才能满足业务需求，这是每个分布式系统架构中都需要考虑。</p>
<ul>
<li>CA(一致性与可用性)：如果选择CA，那么就需要一种非常严格的强一致性协议来进行保障；且，不允许出现网络故障或节点错误，否则整个系统将会不可用。显然，CA不是很适合分布式系统。</li>
<li>CP(一致性与分区容灾)：CP保证了数据的一致性，但由于P的存在，某些时候一个请求将会被无限延长。</li>
<li>AP(可用性与分区容灾)：AP保证了可用性，但却丢失了数据的一致性，在某些场景（银行转账服务）中是难以忍受的。</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310002.png" alt="img" loading="lazy"></figure>
<h4 id="base理论">BASE理论</h4>
<p>由上述CAP定理中可以知道，分布式系统是无法同时满足三者的，只能从中取二。由于分布式系统中网络环境的不可信，分区容灾是必选的；其次可用性保证了用户体验，除开一些需要强一致性的场景(支付，转账等)，应该先选；但如果没有一致性，分布式系统也就失去了存在的意义。</p>
<p>针对此种场景，可以对CAP中的强一致性做一定的让步，有些时候，我们只关注数据的最终一致性即可。于是，BASE理论便诞生了。</p>
<ul>
<li>Basically Available(基本可用)：相对高可用而言，基本可用要求系统即使出现了重大故障，仍然能够提供一些基本型的服务。例如响应时间上的损失以及功能上的损失(熔断，降级)。</li>
<li>Soft state(软状态)：软状态是相对原子性(硬状态，即多个节点间的数据完全一致)而言的。该状态允许数据存在中间状态，并认为该状态并不影响系统的整体可用性，即允许系统在多个不同节点的数据同步存在一定的延时。（注意：对于软状态,我们允许中间状态存在，但不可能一直是中间状态，必须要有个期限，系统保证在没有后续更新的前提下,在这个期限后,系统最终返回上一次更新操作的值,从而达到数据的最终一致性,这个容忍期限（不一致窗口的时间）取决于通信延迟，系统负载，数据复制方案设计，复制副本个数等，DNS是一个典型的最终一致性系统。）</li>
<li>Eventually consisten(最终一致性)：相对强一致性而言，最终一致性仅仅要求数据在经过一段合理的延时后（软状态），最终抵达一致即可。</li>
</ul>
<p>关于最终一致性的变种模型，一般在实践中，五种模式会组合使用：</p>
<ul>
<li>因果一致性(Causal consistency)：如果节点A在更新完某个数据后通知了节点B，那么节点B之后对该数据的访问和修改都是基于A更新后的值。与此同时，和节点A没有因果关系的节点C则没有这样的限制。</li>
<li>读己所写(Read your writes)：一个节点总能访问自己更新过的最新值，是因果一致性的特定形式。</li>
<li>会话一致性(session consistency)：系统保证在一个有效的会话中实现读己所写(Read your writes)。</li>
<li>单调读一致性(Monotonic read consistency)：节点A从系统中读到一个数据项D的某个值V后，节点A后续对数据项D的访问都不会返回比值V更旧的值。</li>
<li>单调写一致性(Monotonic write consistency)：系统需保证来自同一个节点的写操作被顺序执行。</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310003.png" alt="img" loading="lazy"></figure>
<h2 id="解决方案">解决方案</h2>
<h3 id="概要-2">概要</h3>
<p>因为业务的多样性与复杂性，再加上分布式事务解决方案也并未银弹，所以，在实际开发中，应该以本身业务作为作出发点，选择最适合自己的方案。</p>
<h3 id="二阶段提交协议">二阶段提交协议</h3>
<p>在分布式系统中，为了保证多节点间事务的正确执行，便需要一个协调者来管理参与事务的所有节点，确保操作结果符合执行预期。其中，XA便是一个典型的分布式事务处理协议。</p>
<p>XA协议是由X/Open组织提出的分布式事务处理规范，它主要定义了(全局)事务管理器TM和(局部)资源管理器RM之间的接口；是通过二阶段提交协议来保证强一致性的。</p>
<ul>
<li>第一阶段(prepare)：事务管理者TM(协调者)向资源管理者RM(参与者)发送prepare请求，RM(参与者)进行预提交并将结果响应给TM。</li>
<li>第二阶段(commit/rollback)：如果所有RM(参与者)预提交结果都为成功，则TM(协调者)向所有RM(参与者)发送commit请求，否则发送rollback请求。</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310004.png" alt="img" loading="lazy"></figure>
<p>二阶段提交协议虽然为强一致性提出了一套解决方案，但需要注意的是，其中也有几个不可忽略的缺点。</p>
<ul>
<li>同步阻塞：TM(协调者)控制着所有RM(参与者)的操作(准备与实际提交)，这个过程是同步的，TM(协调者)必须等待所有的RM(参与者)返回操作结果才能进行下一步操作；如果在这个过程中有其他请求进来，将会被阻塞。</li>
<li>单点故障：无论是TM(协调者)还是RM(参与者)发生故障，整个流程都将会陷入无限期等待中。</li>
<li>数据不一致：极端情况下，prepare成功，但commit出现部分提交，部分宕机，便会导致数据不一致。</li>
</ul>
<h3 id="三阶段提交协议">三阶段提交协议</h3>
<p>三阶段提交协议是二阶段提交协议的改良版本，它增加了超时机制用来解决同步阻塞问题；实际上是将第一阶段prepare拆分为canCommit和preCommit两个阶段。</p>
<p>preCommit阶段，TM(协调者)在发送真正的commit请求之前，会再次检查各个RM(参与者)的状态，以确保它们的状态一致。</p>
<p>当然，某些极端场景下，同样会出现数据不一致；如：第三阶段的commit出现机器宕机。</p>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310005.png" alt="img" loading="lazy"></figure>
<h3 id="tcc">TCC</h3>
<p>在二阶段提交协议，资源管理者RM负责准备、提交与回滚，而事务管理者TM则负责协调所有RM具体进行哪些操作；资源管理者RM有很多种实现方式，其中TCC(Try-Confirm-Cancel)便是资源管理者的一种服务化的实现。</p>
<ul>
<li>Try阶段：尝试执行，完成业务检查（一致性），预留业务资源（隔离性）。</li>
<li>Confirm阶段：真正的执行具体业务操作，保证幂等。</li>
<li>Cancel阶段：回滚操作，释放try阶段预留的业务资源，保证幂等。</li>
</ul>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310006.png" alt="img" loading="lazy"></figure>
<p>关于TCC理论及设计可参考：<a href="https://www.sofastack.tech/blog/seata-tcc-theory-design-realization/">点击此处跳转文章页面</a></p>
<p>关于TCC使用场景可参考：<a href="https://www.sofastack.tech/blog/seata-tcc-applicable-models-scenarios/">点击此处跳转文章页面</a></p>
<h3 id="saga">Saga</h3>
<p>saga会将一个长事务(long lived transaction)拆分为多个短事务，然后saga自己进行调度管理。saga有点类似于tcc，但是没有try阶段。</p>
<p>当流程出现异常导致部分事务执行失败时，saga会进行补偿操作。此时saga有两种选择：backward recovery 和 forward recovery。</p>
<ul>
<li>逆向恢复(backward recovery)：即回滚操作，将之前所有成功的节点进行回滚，以达到数据一致的目的。</li>
<li>正向恢复(forward recovery)：根据save-point尽最大努力不断重试，以达到数据一致的目的。</li>
</ul>
<p>sagas论文地址：<a href="https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf">点击此处跳转论文页面</a></p>
<h3 id="重试补偿模式">重试补偿模式</h3>
<p>在实际业务开发中，可以针对某些特定的业务添加额外的消息表记录失败信息，后续定时地进行补偿操作。例如京东订单成功后会发送邮件信息，此时可简单的拆分为订单服务与邮件服务，时序图如下：</p>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310007.png" alt="img" loading="lazy"></figure>
<h2 id="开源分布式事务框架">开源分布式事务框架</h2>
<h3 id="rocketmq">RocketMQ</h3>
<p>从4.3版本开始提供基于2PC(二阶段提交协议)加补偿机制的分布式事务消息，其中补偿机制主要针对2PC过程超时或失败的场景。可参考：<a href="http://rocketmq.apache.org/rocketmq/the-design-of-transactional-message/">点击此处跳转页面</a></p>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310008.png" alt="img" loading="lazy"></figure>
<ol>
<li>生产者发送半消息(half message)至MQ服务端。</li>
<li>如果发送半消息成功，则执行本地事务。</li>
<li>基于本地事务的执行结果发送commit或rollback消息到MQ服务端。</li>
<li>如果commit/rollback消息丢失或者生产者在执行本地事务时处于pended状态，MQ服务端将会发送检查消息到同组(same group)的每个生产者获取事务消息的状态(CommitTransaction,RollbackTransaction,Unknown)。</li>
<li>生产者基于本地事务状态恢复commit/rollback消息。</li>
<li>已提交的消息会被MQ服务端分发给消费者，而回滚类消息则会被丢弃。</li>
</ol>
<p>注意1：半消息(half message)是无法被消费的，因为它位于一个独立的内部half-topic中，对消费者是不可见的；commit过程相当于从half-topic中取出消息进行重新投递。</p>
<p>注意2：使用事务消息功能将会降低rocketMQ的执行效率，因为写过程被放大了。</p>
<h3 id="tcc-transaction">TCC-Transaction</h3>
<p>基于TCC模式开发的分布式事务框架，具体可参考：<a href="https://github.com/changmingxie/tcc-transaction">点击此处跳转官网地址</a></p>
<h3 id="servicecomb-saga">ServiceComb-Saga</h3>
<p>servicecomb-saga是servicecomb中的一部分，比之tcc而言少了一个try操作，更加轻量化，具体可参考：<a href="https://github.com/apache/servicecomb-saga-actuator">点击此处跳转官网地址</a></p>
<h3 id="seata">Seata</h3>
<p>阿里开源的分布式事务解决方案，致力于提供高性能和简单易用，包括AT，TCC，SAGA以及XA事务模式。具体可参考：<a href="http://seata.io/zh-cn/">点击此处跳转官网地址</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bash条件判断详解]]></title>
        <id>https://philosopherzb.github.io/post/bash-tiao-jian-pan-duan-xiang-jie/</id>
        <link href="https://philosopherzb.github.io/post/bash-tiao-jian-pan-duan-xiang-jie/">
        </link>
        <updated>2022-05-13T09:20:29.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述bash条件判断规则语法。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/rocks-1757593_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="条件判断">条件判断</h2>
<h3 id="if结构">if结构</h3>
<p>if是最常用的条件判断结构，只有符合给定条件时，才会执行指定的命令。它的语法如下。</p>
<p>注意：如果写在同一行，关键字之间需要使用分号隔离。分号是 Bash 的命令分隔符</p>
<pre><code>if commands; then
  commands
[elif commands; then
  commands...]
[else
  commands]
fi

# 例子 判断环境变量$USER是否等于root
if test $USER = &quot;root&quot;; then
  echo &quot;Hello root.&quot;
else
  echo &quot;You are not root.&quot;
fi

# 单行书写例子
if true; then echo 'hello world'; fi

</code></pre>
<p>if后面可以跟任意数量的命令。这时，所有命令都会执行，但是判断真伪只看最后一个命令，即使前面所有命令都失败，只要最后一个命令返回0，就会执行then的部分。</p>
<pre><code>if false; i=3; true; then echo 'hello world'; fi
echo ${i}

</code></pre>
<h3 id="test命令">test命令</h3>
<p>if结构的判断条件，一般使用test命令，有三种形式。</p>
<pre><code># 写法一
test expression
# 写法二
[ expression ]
# 写法三
[[ expression ]]

</code></pre>
<p>三种形式是等价的，但是第三种形式还支持正则判断，前两种不支持。</p>
<p>expression是一个表达式。这个表达式为真，test命令执行成功（返回值为0）；表达式为伪，test命令执行失败（返回值为1）。</p>
<p>注意：第二种和第三种写法，[ ]与内部的表达式之间必须有空格。</p>
<pre><code>read i
# 写法一
if test ${i} == 1 ; then
  echo &quot;input param: ${i}&quot;
fi
# 写法二
if [ ${i} == 1 ] ; then
   echo &quot;input param: ${i}&quot;
fi
# 写法三
if [[ ${i} == 1 ]] ; then
   echo &quot;input param: ${i}&quot;
fi

</code></pre>
<h2 id="判断表达式">判断表达式</h2>
<p>[[ ]]使用在条件判断中，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt; 和 &gt; 操作符能够正常存在于 [[ ]] 条件判断结构中，但是如果出现在 [ ] 结构中的话，会报错。</p>
<p>执行的时候，需要用bash test.sh；因为[[]]是bash脚本中的命令（bash是sh的增强版本）。</p>
<p>注意：[[  ]]中操作符与变量之间需要有空格，否则会被当做一个变量处理。</p>
<h3 id="文件判断">文件判断</h3>
<ul>
<li>[ -a file ]：如果 file 存在，则为true。</li>
<li>[ -b file ]：如果 file 存在并且是一个块（设备）文件，则为true。</li>
<li>[ -c file ]：如果 file 存在并且是一个字符（设备）文件，则为true。</li>
<li>[ -d file ]：如果 file 存在并且是一个目录，则为true。</li>
<li>[ -e file ]：如果 file 存在，则为true。</li>
<li>[ -f file ]：如果 file 存在并且是一个普通文件，则为true。</li>
<li>[ -g file ]：如果 file 存在并且设置了组 ID，则为true。</li>
<li>[ -G file ]：如果 file 存在并且属于有效的组 ID，则为true。</li>
<li>[ -h file ]：如果 file 存在并且是符号链接，则为true。</li>
<li>[ -k file ]：如果 file 存在并且设置了它的“sticky bit”，则为true。</li>
<li>[ -L file ]：如果 file 存在并且是一个符号链接，则为true。</li>
<li>[ -N file ]：如果 file 存在并且自上次读取后已被修改，则为true。</li>
<li>[ -O file ]：如果 file 存在并且属于有效的用户 ID，则为true。</li>
<li>[ -p file ]：如果 file 存在并且是一个命名管道，则为true。</li>
<li>[ -r file ]：如果 file 存在并且可读（当前用户有可读权限），则为true。</li>
<li>[ -s file ]：如果 file 存在且其长度大于零，则为true。</li>
<li>[ -S file ]：如果 file 存在且是一个网络 socket，则为true。</li>
<li>[ -t fd ]：如果 fd 是一个文件描述符，并且重定向到终端，则为true。 这可以用来判断是否重定向了标准输入／输出错误。</li>
<li>[ -u file ]：如果 file 存在并且设置了 setuid 位，则为true。</li>
<li>[ -w file ]：如果 file 存在并且可写（当前用户拥有可写权限），则为true。</li>
<li>[ -x file ]：如果 file 存在并且可执行（有效用户有执行／搜索权限），则为true。</li>
<li>[ file1 -nt file2 ]：如果 FILE1 比 FILE2 的更新时间最近，或者 FILE1 存在而 FILE2 不存在，则为true。</li>
<li>[ file1 -ot file2 ]：如果 FILE1 比 FILE2 的更新时间更旧，或者 FILE2 存在而 FILE1 不存在，则为true。</li>
<li>[ FILE1 -ef FILE2 ]：如果 FILE1 和 FILE2 引用相同的设备和 inode 编号，则为true。</li>
</ul>
<pre><code>file=/opt/shellScriptDir/test1.sh
if [[ -e ${file} &amp;&amp; -a ${file} ]]; then
    echo &quot;${file} exist&quot;
    if [[ -f ${file} ]]; then
        echo &quot;${file} is normal file&quot;
    fi
    if [[ -d ${file} ]]; then
        echo &quot;${file} is directory&quot;
    fi
    if [[ -r ${file} ]]; then
        echo &quot;${file} is readable&quot;
    fi
    if [[ -w ${file} ]]; then
        echo &quot;${file} is writable&quot;
    fi
    if [[ -x ${file} ]]; then
        echo &quot;${file} is executable/searchable&quot;
    fi
else
    echo &quot;${file} not exist&quot;
fi

</code></pre>
<p>注意：上述判断中，如果使用的是单个中括号[]时，$file需要用双引号括起来，否则判断将会失误。因为当$file为空时，-e会判断为真，如果放在双引号中，返回的是空字符串，[ -e &quot;&quot; ]会判断为伪。</p>
<pre><code># 下面的例子会输出 not exist
file=
if [ -e &quot;${file}&quot; ]; then
    echo &quot;${file} exist&quot;
else
    echo &quot;${file} not exist&quot;
fi

# 下面的例子会输出 exist
file=
if [ -e ${file} ]; then
    echo &quot;${file} exist&quot;
else
    echo &quot;${file} not exist&quot;
fi

</code></pre>
<h3 id="字符串判断">字符串判断</h3>
<ul>
<li>[ string ]：如果string不为空（长度大于0），则判断为真。</li>
<li>[ -n string ]：如果字符串string的长度大于零，则判断为真。</li>
<li>[ -z string ]：如果字符串string的长度为零，则判断为真。</li>
<li>[ string1 = string2 ]：如果string1和string2相同，则判断为真。</li>
<li>[ string1 == string2 ] 等同于[ string1 = string2 ]。</li>
<li>[ string1 != string2 ]：如果string1和string2不相同，则判断为真。</li>
<li>[ string1 '&gt;' string2 ]：如果按照字典顺序string1排列在string2之后，则判断为真。</li>
<li>[ string1 '&lt;' string2 ]：如果按照字典顺序string1排列在string2之前，则判断为真。</li>
</ul>
<p>注意，test命令内部的&gt;和&lt;，必须用引号引起来（或者是用反斜杠转义，或者使用双中括号）。否则，它们会被 shell 解释为重定向操作符。</p>
<p>字符串判断时，变量要放在双引号之中，比如[ -n &quot;$COUNT&quot; ]，否则变量替换成字符串以后，test命令可能会报错，提示参数过多。另外，如果不放在双引号之中，变量为空时，命令会变成[ -n ]，这时会判断为真。如果放在双引号之中，[ -n &quot;&quot; ]就判断为伪。</p>
<p>如果不想使用双引号，也可以使用双括号。</p>
<pre><code>str=fwfw
if [[ -z ${str} ]]; then
    echo &quot;${str} length =0&quot;
elif [[ -n ${str} ]]; then
    echo &quot;${str} length &gt;0&quot;
    if [[ ${str} = &quot;fwfw&quot; ]]; then
        echo &quot;${str} exist&quot;
    fi
fi

</code></pre>
<h3 id="整数判断">整数判断</h3>
<ul>
<li>[ integer1 -eq integer2 ]：如果integer1等于integer2，则为true。</li>
<li>[ integer1 -ne integer2 ]：如果integer1不等于integer2，则为true。</li>
<li>[ integer1 -le integer2 ]：如果integer1小于或等于integer2，则为true。</li>
<li>[ integer1 -lt integer2 ]：如果integer1小于integer2，则为true。</li>
<li>[ integer1 -ge integer2 ]：如果integer1大于或等于integer2，则为true。</li>
<li>[ integer1 -gt integer2 ]：如果integer1大于integer2，则为true。</li>
</ul>
<pre><code>a=10
b=20
if [ ${a} -lt ${b} ]
then
    echo &quot;${a} &lt; ${b}&quot;
else
    echo &quot;${a} &gt;= ${b}&quot;
fi

# 使用双中括号，效果一致
if [[ ${a} &lt; ${b} ]]
then
    echo &quot;${a} &lt; ${b}&quot;
else
    echo &quot;${a} &gt;= ${b}&quot;
fi

</code></pre>
<h3 id="正则判断">正则判断</h3>
<p>[[ expression ]]这种判断形式，支持正则表达式。</p>
<pre><code># regex是一个正则表示式，=~是正则比较运算符。
[[ string =~ regex ]]
</code></pre>
<pre><code>read input
if [[ ${input} =~ [0-9] ]]; then
    echo &quot;match num value = ${input}&quot;
fi

</code></pre>
<h3 id="逻辑运算">逻辑运算</h3>
<ul>
<li>AND运算：符号&amp;&amp;，也可使用参数-a。</li>
<li>OR运算：符号||，也可使用参数-o。</li>
<li>NOT运算：符号!。</li>
</ul>
<pre><code>## 使用双中括号，可以直接在命令内容拼接逻辑运算符
read input
if [[ ${input} =~ [0-9] &amp;&amp; ${input} == 3 ]]; then
    echo &quot;match num value = ${input}&quot;
fi

</code></pre>
<p>&amp;&amp; 和 || 也被称作命令控制操作符，可以用来聚合多个逻辑运算命令。</p>
<pre><code>file=/opt/shellScriptDir/temp
[[ -d  ${file} ]] || echo &quot;${file} not exist&quot;
</code></pre>
<h3 id="算术判断">算术判断</h3>
<p>bash提供了(( ... ))作为算术条件，用于进行算术运算的判断。</p>
<p>注意，算术判断不需要使用test命令，而是直接使用((...))结构。这个结构的返回值，决定了判断的真伪。</p>
<p>如果算术计算的结果是非零值，则表示判断成立。这一点跟命令的返回值正好相反，需要小心。</p>
<pre><code># 输出match num value true
if [[ 0 ]]; then
    echo &quot;match num value true&quot;
else
    echo &quot;value false&quot;
fi

# 输出value false
if (( 0 )); then
    echo &quot;match num value true&quot;
else
    echo &quot;value false&quot;
fi

</code></pre>
<p>(( ... ))可以用作变量赋值，赋值完成后将会返回变量的值。</p>
<pre><code># 输出match num value: 1
if (( var=1 )); then
    echo &quot;match num value: ${var}&quot;
else
    echo &quot;value false&quot;
fi

</code></pre>
<h3 id="case结构判断">case结构判断</h3>
<p>case结构用于多值判断，可以为每个值指定对应的命令，跟包含多个elif的if结构等价，但是语义更好。</p>
<pre><code># 语法格式
# expression是一个表达式，pattern是表达式的值或者一个模式，可以有多条，
# 用来匹配多个值，每条以两个分号（;）结尾。
case expression in
  pattern )
    commands ;;
  pattern )
    commands ;;
  ...
esac

</code></pre>
<pre><code># 简单实例
echo &quot;input value[1-3]: &quot;
read input
case ${input} in
    1) echo &quot;read value is 1&quot;;;
    2) echo &quot;read value is 2&quot;;;
    3) echo &quot;read value is 3&quot;;;
    *) echo &quot;read value is not match: ${input}&quot;;;
esac

</code></pre>
<p>case的匹配模式可以使用各种通配符，类似于下方所示：</p>
<ul>
<li>a)：匹配a。</li>
<li>a|b)：匹配a或b。</li>
<li>[[:alpha:]])：匹配单个字母。</li>
<li>???)：匹配3个字符的单词。</li>
<li>*.txt)：匹配.txt结尾。</li>
<li>*)：匹配任意输入，通过作为case结构的最后一个模式。</li>
</ul>
<pre><code># 匹配数值，单个字符，两个字符
echo &quot;input value[number or character]: &quot;
read input
case ${input} in
    [0-9]) echo &quot;read number value is: ${input}&quot;;;
    [[:lower:]] | [[:upper:]]) echo &quot;read character value is: ${input}&quot;;;
    ??) echo &quot;read double input value is: ${input}&quot;;;
    *) echo &quot;read value is not match: ${input}&quot;;;
esac

</code></pre>
<p>Bash 4.0之前，case结构只能匹配一个条件，然后就会退出case结构。Bash 4.0之后，允许匹配多个条件，这时可以用;;&amp;终止每个条件块。</p>
<pre><code>echo &quot;input character value: &quot;
read input
case ${input} in
  [[:upper:]])    echo &quot;'${input}' is upper case.&quot; ;;&amp;
  [[:lower:]])    echo &quot;'${input}' is lower case.&quot; ;;&amp;
  [[:alpha:]])    echo &quot;'${input}' is alphabetic.&quot; ;;&amp;
  [[:digit:]])    echo &quot;'${input}' is a digit.&quot; ;;&amp;
  [[:graph:]])    echo &quot;'${input}' is a visible character.&quot; ;;&amp;
  [[:punct:]])    echo &quot;'${input}' is a punctuation symbol.&quot; ;;&amp;
  [[:space:]])    echo &quot;'${input}' is a whitespace character.&quot; ;;&amp;
  [[:xdigit:]])   echo &quot;'${input}' is a hexadecimal digit.&quot; ;;&amp;
esac

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shell基本知识]]></title>
        <id>https://philosopherzb.github.io/post/shell-ji-ben-zhi-shi/</id>
        <link href="https://philosopherzb.github.io/post/shell-ji-ben-zhi-shi/">
        </link>
        <updated>2022-04-30T09:03:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述shell基本知识。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/nature-2147400_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="简介">简介</h2>
<h3 id="简介-2">简介</h3>
<p>Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。</p>
<p>Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。</p>
<p>Shell 脚本（shell script），是一种为 shell 编写的脚本程序。业界所说的 shell 通常都是指 shell 脚本（故此处也沿用该说明），需要注意的是：shell 和 shell script 是两个不同的概念。</p>
<p>查看安装的shell信息，两种查看方式任选一种键入回车即可得到相关信息。</p>
<pre><code>ls -l /bin/*sh
cat /etc/shells

</code></pre>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230307001.png" alt="img" loading="lazy"></figure>
<h2 id="基本语法bash">基本语法（Bash）</h2>
<h3 id="变量">变量</h3>
<h4 id="基础变量">基础变量</h4>
<p>创建变量时，需遵循如下规则：</p>
<ul>
<li>命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。</li>
<li>不允许出现空格及标点符号。</li>
<li>不能使用bash里的关键字（可用help命令查看保留关键字）。</li>
</ul>
<p>变量声明的语法如下（注意：等号两边不能存在空格，读取时使用$符）：</p>
<pre><code>variable=vlaue
</code></pre>
<pre><code># 简单例子
# 变量 a 赋值为字符串 hello
a=hello
# $a 等效于 ${a}，加花括号是为了帮助解释器识别变量的边界
# 如：echo $a_world 将不会输出内容，因为变量a_world不存在
# 但是可以使用echo ${a}_world，将会输出hello_world
echo ${a}

# 变量值包含空格，就必须放在引号里面        
b=&quot;world shell&quot;
echo ${b}

# 变量值可以引用其他变量的值
c=&quot;${a} ${b} !&quot;
echo ${c}

# 变量值可以使用转义字符
d=&quot;\t content \n&quot;
echo ${d}

# 变量值可以是命令的执行结果
e=$(ls -l /bin/*sh)
echo ${e}

# 变量值可以是数学运算的结果
f=$((4 * 5))
echo ${f}

# 变量重复赋值，后面的赋值将会覆盖前面的赋值
foo=1
foo=2
echo ${foo}

# 变量的值是变量，如果需要输出，可以使用${!varname}愈发
myvar=USER
echo ${!myvar}

</code></pre>
<h4 id="特殊变量">特殊变量</h4>
<p>Bash（是shell的一个增强版本）提供了一些特殊变量，特殊变量的值由shell预定义，用户不用进行赋值。</p>
<ul>
<li>$?: 表示上一个命令的退出码，用来判断上一个命令是否执行成功，0表示成功，非0表示失败。</li>
<li>$$: 表示当前shell的进程id，也可以用来命名临时文件</li>
<li>$_: 表示上个命令的最后一个参数</li>
<li>$!: 表示最近一个后台执行的异步命令的进程id</li>
<li>$0: 表示当前shell的名称（在命令直接执行时）或者脚本名（在脚本中执行时）</li>
<li>$-: 表示当前shell的启动参数</li>
<li>$@ $#: 表示脚本中的参数数量（两者不同之处：在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 &quot; * &quot; 等价于 &quot;1 2 3&quot;（传递了一个参数），而 &quot;@&quot; 等价于 &quot;1&quot; &quot;2&quot; &quot;3&quot;（传递了三个参数）。）</li>
</ul>
<pre><code># $? 例子
ls -l /bin/bash
echo $?
ls -l notexistfile
echo $?
# 输出结果
-rwxr-xr-x 1 root root 1183448 Jun 18  2020 /bin/bash
0
ls: cannot access 'notexistfile': No such file or directory
2

</code></pre>
<h4 id="变量默认值">变量默认值</h4>
<p>Bash 提供四个特殊语法，跟变量的默认值有关，目的是保证变量不为空。</p>
<ul>
<li>${varname:-word} 语法含义：如果变量varname存在且不为空，则返回它的值，否则返回word。它的目的是返回一个默认值，比如${count:-0}表示变量count不存在时返回0。</li>
<li>${varname:=word} 语法含义：如果变量varname存在且不为空，则返回它的值，否则将它设为word，并且返回word。它的目的是设置变量的默认值，比如${count:=0}表示变量count不存在时返回0，且将count设为0。</li>
<li>${varname:+word} 语法含义：如果变量名存在且不为空，则返回word，否则返回空值。它的目的是测试变量是否存在，比如${count:+1}表示变量count存在时返回1（表示true），否则返回空值。</li>
<li>${varname:?message} 语法含义：如果变量varname存在且不为空，则返回它的值，否则打印出varname: message，并中断脚本的执行。如果省略了message，则输出默认的信息“parameter null or not set.”。它的目的是防止变量未定义，比如${count:?&quot;undefined!&quot;}表示变量count未定义时就中断执行，抛出错误，返回给定的报错信息undefined!。</li>
</ul>
<pre><code>echo ${a:-0}
echo ${a:=word}
echo ${a:+1}
echo ${b:?&quot;undefined!&quot;}
# 输出结果
0
word
1
./test1.sh: line 5: b: undefined!

</code></pre>
<h4 id="变量命令">变量命令</h4>
<p>declare命令可以声明一些特殊类型的变量，为变量设置一些限制，比如声明只读类型的变量和整数类型的变量。</p>
<p>declare语法格式：declare OPTION VARIABLE=value，命令的主要参数（OPTION）如下：</p>
<ul>
<li>-a：声明数组变量。</li>
<li>-f：输出所有函数定义。</li>
<li>-F：输出所有函数名。</li>
<li>-i：声明整数变量。</li>
<li>-l：声明变量为小写字母。</li>
<li>-p：查看变量信息。</li>
<li>-r：声明只读变量。</li>
<li>-u：声明变量为大写字母。</li>
<li>-x：该变量输出为环境变量。</li>
</ul>
<p>readonly命令等同于declare -r，用来声明只读变量，不能改变变量值，也不能unset变量。</p>
<p>let命令声明变量时，可以直接执行算术表达式。</p>
<h4 id="数组变量">数组变量</h4>
<p>数组中可以存放多个值。Bash Shell 只支持一维数组（不支持多维数组），初始化时不需要定义数组大小。</p>
<p>与大部分编程语言类似，数组元素的下标由 0 开始。</p>
<p>Shell 数组用括号来表示，元素用&quot;空格&quot;符号分割开，语法格式如下：</p>
<pre><code>array_name=(value1 value2 ... valuen)
</code></pre>
<pre><code># 数组例子
array_test=(&quot;hello&quot; &quot;world&quot; &quot;shell&quot;)
echo ${array_test[2]}
# 获取所有元素
echo ${array_test[@]}
echo ${array_test[*]}

</code></pre>
<h4 id="字符串变量进阶例子">字符串变量&lt;进阶例子&gt;</h4>
<pre><code>var=&quot;opt/temp/test.sh&quot;
# 字符串长度
echo ${#var}

# 截取字符串 ${varname:offset:length}: 从位置offset开始（从0开始计算），长度为length
echo ${var:0:3}
# 如果省略length，则从位置offset开始，一直返回到字符串的结尾。
echo ${var:0}
# 如果offset为负值，表示从字符串的末尾开始算起。
# 注意，负数前面必须有一个空格（如果不想写空格，可以填0），以防止与${variable:-word}的变量的设置默认值语法混淆。
# 这时，如果还指定length，则length不能小于零。
echo ${var: -16:3}
echo ${var: -13}
echo ${var:0-16:3}
echo ${var:0-13}

# 输出大写
echo ${var^^}
# 输出小写
echo ${var,,}

# 匹配删除字符串
# 匹配模式pattern可以使用*、?、[]等通配符。

# ${variable#pattern}
# 如果 pattern 匹配变量 variable 的开头，删除最短匹配（非贪婪匹配）的部分，返回剩余部分
# #*/ 表示从左边开始删除第一个 / 号及左边的所有字符，即删除opt/，结果为：temp/test.sh
echo ${var#*/}

# ${variable##pattern}
# 如果 pattern 匹配变量 variable 的开头，删除最长匹配（贪婪匹配）的部分，返回剩余部分
# ##*/ 表示从左边开始删除最后（最右边）一个 / 号及左边的所有字符，即删除opt/temp/，结果为：test.sh
echo ${var##*/}

# ${variable%pattern}
# 如果 pattern 匹配变量 variable 的结尾，删除最短匹配（非贪婪匹配）的部分，返回剩余部分
# %/* 表示从右边开始，删除第一个 / 号及右边的字符，即删除/test.sh，结果：opt/temp
echo ${var%/*}

# ${variable%%pattern}
# 如果 pattern 匹配变量 variable 的结尾，删除最长匹配（贪婪匹配）的部分，返回剩余部分
# %%/* 表示从右边开始，删除最后（最左边）一个 / 号及右边的字符，即删除/temp/test.sh，结果：opt
echo ${var%%/*}

</code></pre>
<h3 id="算术运算符">算术运算符</h3>
<h4 id="算术表达式">算术表达式</h4>
<p>((...))语法可以进行整数的算术运算。该表达式可以忽略内部的空格，在其内部可以使用()改变运算顺序。输出结果时，需要在前面加上$符。</p>
<pre><code>a=2
echo $(( ${a} + 2 ))
# 可以不使用$或者${}（花括号是为了确定边界）引用变量
echo $(( + 2))

# 赋值
i=$((a + 2))
echo &quot;i= ${i}&quot;

</code></pre>
<p>expr命令同样支持算是运算，其可以使用变量替换。</p>
<p>注意：表达式与运算符之间需要有空格，否则会被当做字符串输出。</p>
<pre><code>a=2
expr ${a} + 2

# 赋值，两种方式都可进行赋值，``符号位于esc键下方，并非单引号
b=$(expr ${a} + 2)
c=`expr ${a} + 2`
echo &quot;b= ${b}&quot;
echo &quot;c= ${c}&quot;

</code></pre>
<p>((...))语法支持的算术运算符如下。</p>
<ul>
<li>+：加法</li>
<li>-：减法</li>
<li>*：乘法</li>
<li>/：除法（整除）</li>
<li>%：余数</li>
<li>**：指数</li>
<li>++：自增运算（作为前缀是先运算后返回值，作为后缀是先返回值后运算）</li>
<li>--：自减运算（作为前缀是先运算后返回值，作为后缀是先返回值后运算）</li>
</ul>
<h4 id="进制数值">进制数值</h4>
<p>Bash 的数值默认都是十进制，但是在算术表达式中，也可以使用其他进制。</p>
<ul>
<li>number：没有任何特殊表示法的数字是十进制数（以10为底）。</li>
<li>0number：八进制数。</li>
<li>0xnumber：十六进制数。</li>
<li>base#number：base进制的数。</li>
</ul>
<pre><code>echo $((016))
echo $((0xee))
echo $((2#00000011))

</code></pre>
<h4 id="位运算">位运算</h4>
<p>Bash 支持二进制位运算符</p>
<ul>
<li>&lt;&lt;：位左移运算，把一个数字的所有位向左移动指定的位。</li>
<li>&gt;&gt;：位右移运算，把一个数字的所有位向右移动指定的位。</li>
<li>&amp;：位的“与”运算，对两个数字的所有位执行一个AND操作。</li>
<li>|：位的“或”运算，对两个数字的所有位执行一个OR操作。</li>
<li>~：位的“否”运算，对一个数字的所有位取反。</li>
<li>!：逻辑“否”运算</li>
<li>^：位的异或运算（exclusive or），对两个数字的所有位执行一个异或操作。</li>
</ul>
<h4 id="逻辑运算">逻辑运算</h4>
<p>Bash 支持逻辑运算符</p>
<ul>
<li>&lt;/-lt：小于</li>
<li>&gt;/-gt：大于</li>
<li>&lt;=/-le：小于或相等</li>
<li>&gt;=/ge：大于或相等</li>
<li>==/-eq：相等</li>
<li>!=/-ne：不相等</li>
<li>&amp;&amp;：逻辑与</li>
<li>||：逻辑或</li>
<li>expr1?expr2:expr3：三元条件运算符。若表达式expr1的计算结果为非零值（算术真），则执行表达式expr2，否则执行表达式expr3。</li>
</ul>
<h4 id="赋值与求值运算">赋值与求值运算</h4>
<p>逗号,在$((...))内部是求值运算符，执行前后两个表达式，并返回后一个表达式的值。</p>
<pre><code>echo $((foo = 1 + 2, 3 * 4))
echo ${foo}
# 输出
12
3

</code></pre>
<p>赋值运算如下：</p>
<ul>
<li>parameter = value：简单赋值。</li>
<li>parameter += value：等价于parameter = parameter + value。</li>
<li>parameter -= value：等价于parameter = parameter – value。</li>
<li>parameter *= value：等价于parameter = parameter * value。</li>
<li>parameter /= value：等价于parameter = parameter / value。</li>
<li>parameter %= value：等价于parameter = parameter % value。</li>
<li>parameter &lt;&lt;= value：等价于parameter = parameter &lt;&lt; value。</li>
<li>parameter &gt;&gt;= value：等价于parameter = parameter &gt;&gt; value。</li>
<li>parameter &amp;= value：等价于parameter = parameter &amp; value。</li>
<li>parameter |= value：等价于parameter = parameter | value。</li>
<li>parameter ^= value：等价于parameter = parameter ^ value。</li>
</ul>
<pre><code>echo $((foo = 3))
echo $((foo *= 2))
# 输出
3
6

</code></pre>
<h2 id="流程控制">流程控制</h2>
<h3 id="条件判断">条件判断</h3>
<p>1、简单if语句</p>
<pre><code># 简单if语句
if condition
then
    command
fi

</code></pre>
<p>2、if else语句</p>
<pre><code># if else语句
if condition
then
    command
else
    command
fi

</code></pre>
<p>3、if else-if else语句</p>
<pre><code># if else-if else语句
if condition1
then
    command1
elif condition2 
then 
    command2
else
    commandN
fi

</code></pre>
<pre><code># 例子
a=10
b=20
# [[ ]]使用在条件判断中，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt; 和 &gt; 操作符能够正常存在于 [[ ]] 条件判断结构中，但是如果出现在 [ ] 结构中的话，会报错。
# 执行的时候，需要用bash test.sh；因为[[]]是bash脚本中的命令（bash是sh的增强版本）。
if [[ ${a} &lt; ${b} ]]
then
    echo &quot;a &lt; b&quot;
else
    echo &quot;a &gt;= b&quot;
fi

</code></pre>
<h3 id="循环语句">循环语句</h3>
<p>Bash 提供三种循环语法for、while和until。</p>
<p>while循环有一个判断条件，只要符合条件，就不断循环执行指定的语句。关键字do可以跟while不在同一行，这时两者之间不需要使用分号分隔。</p>
<pre><code>while condition; do
  commands
done

</code></pre>
<p>until循环与while循环恰好相反，只要不符合判断条件（判断条件失败），就不断循环执行指定的语句。一旦符合判断条件，就退出循环。关键字do可以与until不写在同一行，这时两者之间不需要分号分隔。</p>
<pre><code>until condition; do
  commands
done

</code></pre>
<p>for...in循环用于遍历列表中的每一项。关键词do可以跟for写在同一行，两者使用分号分隔。</p>
<pre><code>for variable in list
do
  commands
done

</code></pre>
<p>for循环支持C语言的循环语法。</p>
<pre><code># expression1用来初始化循环条件，expression2用来决定循环结束的条件，
# expression3在每次循环迭代的末尾执行，用于更新值。
# 注意，循环条件放在双重圆括号之中。另外，圆括号之中使用变量，不必加上美元符号$。
for (( expression1; expression2; expression3 )); do
  commands
done

# 等同于下述表达式
(( expression1 ))
while (( expression2 )); do
  commands
  (( expression3 ))
done

# 例子
for (( i=0; i&lt;5; i=i+1 )); do
  echo $i
done

</code></pre>
<p>Bash 提供了两个内部命令break和continue，用来在循环内部跳出循环。</p>
<p>break命令立即终止循环，程序继续执行循环块之后的语句，即不再执行剩下的循环。</p>
<p>continue命令立即终止本轮循环，开始执行下一轮循环。</p>
<pre><code># break
a=(first second third)
for i in ${a[*]}; do
    echo ${i}
    if [[ ${i} == &quot;first&quot; ]]; then break; fi
done

# continue
a=(first second third)
for i in ${a[*]}; do
    if [[ ${i} == &quot;second&quot; ]]; then continue; fi
    echo ${i}
done

</code></pre>
<h2 id="函数">函数</h2>
<p>函数（function）是可以重复使用的代码片段，有利于代码的复用。它与别名（alias）的区别是，别名只适合封装简单的单个命令，函数则可以封装复杂的多行命令。</p>
<p>函数总是在当前 Shell 执行，这是跟脚本的一个重大区别，Bash 会新建一个子 Shell 执行脚本。如果函数与脚本同名，函数会优先执行。但是，函数的优先级不如别名，即如果函数与别名同名，那么别名优先执行。</p>
<pre><code># 第一种
fn() {
  # codes
}
# 第二种
function fn() {
  # codes
}

</code></pre>
<p>$1~$9：函数的第一个到第9个的参数。如果函数的参数多于9个，那么第10个参数可以用${10}的形式引用，以此类推</p>
<pre><code>printParam(){
    echo &quot;first param is $1&quot;
}
printParam wqrwq

function log_msg(){
    echo &quot;[$(date '+%F %T')]: $@&quot;
}
log_msg this is sample log message

</code></pre>
<p>函数体内支持局部变量的声明。同时，也支持修改全局变量。</p>
<pre><code>function fn(){
    local str=paramValue
    echo &quot;local: str=${str}&quot;
}
fn
echo &quot;global: str=${str}&quot;

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[python3基本知识]]></title>
        <id>https://philosopherzb.github.io/post/python3-ji-ben-zhi-shi/</id>
        <link href="https://philosopherzb.github.io/post/python3-ji-ben-zhi-shi/">
        </link>
        <updated>2022-04-16T07:52:59.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述python3脚本知识。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/mountains-209956_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="简介">简介</h2>
<h3 id="简介-2">简介</h3>
<p>Python 是一个高层次的结合了解释性、编译性、互动性和面向对象的脚本语言（计算机编程语言）。</p>
<p>Python 的设计具有很强的可读性，相比其他语言经常使用英文关键字，其他语言的一些标点符号，它具有比其他语言更有特色的语法结构。</p>
<p>注意：python3与python2具备较大的差别，他们之间并不完全兼容。</p>
<p>Python的优点：</p>
<ol>
<li>简单和明确，做一件事只有一种方法。</li>
<li>学习曲线低，跟其他很多语言相比，Python更容易上手。</li>
<li>开放源代码，拥有强大的社区和生态圈。</li>
<li>解释型语言，天生具有平台可移植性。</li>
<li>支持两种主流的编程范式（面向对象编程和函数式编程）都提供了支持。</li>
<li>可扩展性和可嵌入性，可以调用C/C++代码，也可以在C/C++中调用Python。</li>
<li>代码规范程度高，可读性强，适合有代码洁癖和强迫症的人群。</li>
</ol>
<p>Python的缺点主要集中在以下几点。</p>
<ol>
<li>执行效率稍低，因此计算密集型任务可以由C/C++编写。</li>
<li>代码无法加密，但是现在很多公司都不销售卖软件而是销售服务，这个问题会被淡化。</li>
<li>在开发时可以选择的框架太多（如Web框架就有100多个），有选择的地方就有错误。</li>
</ol>
<h2 id="基本语法">基本语法</h2>
<h3 id="变量">变量</h3>
<h4 id="基础变量">基础变量</h4>
<p>创建变量时，需遵循以下规则：</p>
<ul>
<li>第一个字符必须是字母表中字母或下划线 _ 。</li>
<li>标识符的其他的部分由字母、数字和下划线组成。</li>
<li>标识符对大小写敏感。</li>
<li>不能使用python保留字。</li>
<li>python3支持中文作为变量名，非 ASCII 标识符也是允许的。（不建议使用中文作为变量名）</li>
</ul>
<pre><code># 查看关键字
import keyword
var = keyword.kwlist
print(var)

</code></pre>
<h4 id="注释与编码">注释与编码</h4>
<p>单行注释使用#号，多行注释可以使用'''或者&quot;&quot;&quot;，python3编码默认为utf-8（可以修改）。</p>
<pre><code># 单行注释信息
&quot;&quot;&quot;
多行注释
&quot;&quot;&quot;

# 修改编码
# -*- coding: GBK -*-

</code></pre>
<h4 id="语法结构">语法结构</h4>
<p>python对于代码块需要使用缩进来表示，缩进的空格数是可变的，但是同一个代码块中的缩进需要保持一致，一般使用4个空格。</p>
<p>为了方便后期维护，在代码之间可以适当的使用空行进行代码功能隔离。</p>
<pre><code>a = &quot;12&quot;
if n := len(a) &gt; 1:
    print(&quot;True&quot;)
else:
    print(&quot;False&quot;)

</code></pre>
<p>python中关于一条语句过长需要换行的写法如下（使用反斜杠\）：</p>
<pre><code>var = &quot;var1&quot; \
    &quot;var2&quot; \
    &quot;var3&quot;
print(var)

</code></pre>
<p>python支持多行语句并做一行处理</p>
<pre><code>import sys; x = 'runoob'; sys.stdout.write(x + '\n')
</code></pre>
<h2 id="基本数据类型">基本数据类型</h2>
<p>Python3有六个标准的数据类型：Number，String，List，Tuple，Set，Dictionary。</p>
<p>其中不可变数据：Number，String，Tuple</p>
<p>可变数据：List，Set，Dictionary</p>
<h3 id="number数字">Number（数字）</h3>
<p>python3支持int(长整型，相当于python2的Long)，float，bool(python2没有布尔值；1等于True，0等于False，可用于算术运算)，complex(复数)。</p>
<pre><code># 可以使用type或者isinstance函数判断变量类型。
# type()不会认为子类是一种父类类型。即type事先不知道变量类型。
# isinstance()会认为子类是一种父类类型。即isinstance事先知道变量类型。
a, b, c, d = 1, 2.2, False, 4+1j
print(type(a), type(b), type(c), type(d))
print(isinstance(a, int))

# bool 算术运算
a = True
b, c, d = a + 1, a - 1, a * 10
print(a, b, c, d)

# 强制转换
a = &quot;1&quot;
print(a, int(a), bool(a), float(a))

</code></pre>
<h3 id="string字符串">String（字符串）</h3>
<p>python3使用''或者&quot;&quot;定义字符串，使用反斜杠\转义特殊字符，也可以使用r让反斜杠不转义；连接字符串用+号，重复字符串用*号。</p>
<pre><code>a = &quot;test\ncontent&quot;
b = r&quot;test\ncontent&quot;
print(a)
print(b)
print(b + b)
print(b*3)

</code></pre>
<p>字符串的截取语法：variable[头下标:尾下标]，规则为左闭右开区间；</p>
<p>从左往右，头下标从0开始（0可省略）；从右往左，尾下标从-1开始（注意：由于左闭右开原则，-1并不会取到最后一位字符，如需取最后一位字符，需将尾下标置空）。</p>
<pre><code>a = &quot;test content&quot;
print(a[:5])
print(a[0:5])
print(a[-12:-1])
print(a[-12:])

# 判断字符是否在目标字符串中
print(&quot;te&quot; in a)
print(&quot;t&quot; not in a)

# 字符串格式化
a = &quot;%s, %d&quot; % (&quot;weq&quot;, 2)
print(a)

# 使用f-string进行格式化，可以不用%s之类的转换符（python版本需要3.6及以上）
a = &quot;content&quot;
b = f&quot;test {a}&quot;
print(b)
print(f&quot;{1+2=}&quot;)

</code></pre>
<h3 id="tuple元组">Tuple（元组）</h3>
<p>元组使用()进行赋值，使用逗号分隔不同元素；()中可以存在不同类型的元素，如数字，字符串，嵌套元组等。</p>
<p>元组的索引与截取规则与字符串一致，可参考3.2节内容。</p>
<p>注意：元组中的元素是不能被修改的；只包含一个元素时，需要在元素后面添加逗号，否则括号会被当作运算符使用</p>
<pre><code>tup = (&quot;www&quot;, 12306, 99.99, 1, 2, 3)
tup2 = (&quot;test&quot;,)
print(tup)
print(tup[:2])
print(tup[-1:])
print(tup + tup2)
print(tup * 2)
print(&quot;www&quot; in tup)

# 元组截取时，支持第三个参数：步长
print(tup[0:6:2])

</code></pre>
<h3 id="list列表">List（列表）</h3>
<p>列表与元组规则基本一致，不过列表使用[]进行赋值，且其中的元素是可以被修改的。</p>
<pre><code>list1 = [&quot;www&quot;, 12306, 99.99, 1, 2, 3]
list2 = [&quot;test&quot;]
print(list1)
print(list1[:2])
print(list1[-1:])
print(list1 + list2)
print(list1 * 2)
print(&quot;www&quot; in list1)

# 截取时，支持第三个参数：步长
print(list1[0:6:2])

# 修改列表元素
list2[0] = &quot;update&quot;
print(list2)

</code></pre>
<p>借助内置函数，列表也可以被当做堆栈（后进先出）或者队列（先进先出）使用。</p>
<p>注意：列表用作队列时相对比较低效。因为在列表的末尾添加和弹出元素非常快，但是在列表的开头插入或弹出元素却很慢 (因为所有的其他元素都必须移动一位)。</p>
<pre><code># 列表作为堆栈，利用append函数添加一个元素到堆栈的顶端，利用pop函数从堆栈顶部取出一个元素
stack = [1, 2, 3]
stack.append(4)
stack.append(5)
print(stack)
stack.pop()
print(stack)

# 列表作为队列，需要借助collections.deque操作列表两端
from collections import deque

queue = deque([1, 2, 3])
queue.append(4)
queue.append(5)
print(queue)
queue.popleft()
print(queue)

</code></pre>
<p>python中的列表推导式：列表推导式提供了一个更简单的创建列表的方法。常见的用法是把某种操作应用于序列或可迭代对象的每个元素上，然后使用其结果来创建列表，或者通过满足某些特定条件元素来创建子序列。</p>
<p>列表推导式的结构是由一对方括号所包含的以下内容组成：一个表达式，后面跟一个 for 子句，然后是零个或多个 for 或 if 子句。 其结果将是一个新列表，由对表达式依据后面的 for 和 if 子句的内容进行求值计算而得出。</p>
<p>注意：如果表达式是一个元组，那么其必须加上括号。</p>
<pre><code># 简单列表推导式
t = [x * 2 for x in range(3)]
print(t)

list_x = [1, 3, 5]
list_y = [2, 4, 6]
print([x * y for x in list_x for y in list_y])

fresh_str = ['  f   ', '  r', 'w   ']
print([x.strip() for x in fresh_str])

# 组合两个列表中的不同元素，将返回一个列表，列表中的元素类型是元组
t = [(x, y) for x in [1, 2, 3] for y in [2, 1, 3] if x != y]
print(t)

# 嵌套列表推导式
list_1 = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
t = [[row[i] for row in list_1] for i in range(4)]
print(t)

</code></pre>
<h3 id="set集合">Set（集合）</h3>
<p>集合是由不重复元素组成的无序的集。它的基本用法包括成员检测和消除重复元素。集合对象也支持像 联合，交集，差集，对称差分等数学运算。</p>
<p>使用{}花括号或<a href="https://docs.python.org/zh-cn/3.9/library/stdtypes.html#set">set()</a> 函数可以用来创建集合。注意：要创建一个空集合只能用 set() 而不能用 {}，因为后者是创建一个空字典。</p>
<pre><code># 去重
sites = {&quot;www&quot;, &quot;12306&quot;, &quot;com&quot;, &quot;www&quot;}
print(sites)
print(&quot;www&quot; in sites)

# 运算
a = set(&quot;abcd&quot;)
b = set(&quot;defg&quot;)
print(a &amp; b)
print(a | b)
print(a ^ b)
# a集合包含的元素而b集合不包含
print(a - b)

# 集合支持推导式
a = {x for x in 'abcdefg' if x not in 'abc'}
print(a)

</code></pre>
<h3 id="dictionary字典">Dictionary（字典）</h3>
<p>字典（dictionary）是Python中另一个非常有用的内置数据类型。</p>
<p>列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。</p>
<p>字典是一种映射类型，字典用 { } 标识，它是一个无序的 键(key) : 值(value) 的集合。</p>
<p>键(key)必须使用不可变类型。</p>
<p>在同一个字典中，键(key)必须是唯一的。</p>
<pre><code># 字典，类比如json，或者map，是一个kv键值对的集合
dict_test = {'name': 'hello', 'age': 100}
# 获取字典的值
print(dict_test['name'])
print(dict_test.get('age'))
# 获取键值对
for k, v in dict_test.items():
    print(k, v)
  
# 使用dict函数构造字典
d = dict([('name', 'jack'), ('age', 100)])
print(d)
d2 = dict(name='tom', age=90)
print(d2)

# 字典支持推导式
a = {x: x * 2 for x in (1, 2, 3)}
print(a)

</code></pre>
<h2 id="流程控制">流程控制</h2>
<h3 id="条件判断">条件判断</h3>
<p>if 语法如下所示：</p>
<ul>
<li>每个条件后面要使用冒号 :，表示接下来是满足条件后要执行的语句块。</li>
<li>使用缩进来划分语句块，相同缩进数的语句在一起组成一个语句块。</li>
<li>在Python中没有switch – case语句。</li>
</ul>
<pre><code># 语法格式
if condition_1:
    statement_block_1
elif condition_2:
    statement_block_2
else:
    statement_block_3
  
# 简单实例
var = 10
if var &gt; 10:
    print(&quot;value(&quot; + str(var) + &quot;) grate than 10&quot;)
elif var == 10:
    print(&quot;value(&quot; + str(var) + &quot;) equals 10&quot;)
else:
    print(&quot;value(&quot; + str(var) + &quot;) less than 10&quot;)  
  
# := 赋值表达式运算符，又称海象运算符（python3.8）
a = &quot;12&quot;
if n := len(a) &gt; 1:
    print(&quot;True&quot;)
else:
    print(&quot;False&quot;)  

</code></pre>
<h3 id="循环语句">循环语句</h3>
<p>break（终止循环）及continue（跳过本次循环）用于流程控制。pass是空语句，是为了保持程序结构的完整性。</p>
<p>while语句</p>
<pre><code># 简单例子
var = 1
while var &lt; 3:
    print(&quot;value: &quot; + str(var))
    var += 1
  
# pass占位语句
while True:
    pass  

</code></pre>
<p>for语句（break（终止循环）及continue（跳过本次循环）用于流程控制）</p>
<pre><code># 简单例子
# 列表 for循环
list1 = [&quot;first&quot;, 'second', 123, 45.6]
print(len(list1))
for var in list1:
    print(var, end=&quot;; &quot;)
  
# 获取索引及其值
for i, v in enumerate(list1):
    print(i, v)   
  
# 多个list，使用zip函数聚合
list2 = ['map', 'json', 'session', 'cookie']
for q1, q2, in zip(list1, list2):
    print('param1={0}, param2={1}'.format(q1, q2))
  
# 内置的range函数可以遍历数字序列
# 简单range，
for i in range(3):
    print(i)
# 指定区间
for i in range(1, 3):
    print(i)
# 指定区间及步长
for i in range(1, 10, 2):
    print(i)
  
# 迭代器
list3 = [&quot;good&quot;, &quot;fire&quot;, &quot;hello&quot;]
it = iter(list2)
print(next(it))
for var in it:
    print(var)  

</code></pre>
<h2 id="函数">函数</h2>
<h3 id="函数-2">函数</h3>
<p>函数是组织好的，可重复使用的，用来实现单一，或相关联功能的代码段。函数能提高应用的模块性，和代码的重复利用率。</p>
<ul>
<li>函数代码块以 def 关键词开头，后接函数标识符名称和圆括号 ()。</li>
<li>任何传入参数和自变量必须放在圆括号中间，圆括号之间可以用于定义参数。</li>
<li>函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。</li>
<li>函数内容以冒号 : 起始，并且缩进。</li>
<li>return [表达式] 结束函数，选择性地返回一个值给调用方，不带表达式的 return 相当于返回 None。</li>
</ul>
<pre><code># 可变对象传递给函数，将会改变其中的值；不可变对象传递不会改变值（如果修改，则返回一个新的变量）
def test_n(var):
    var[0] = 3

test1 = [2, 3]
test_n(test1)
print(test1)

# 必需参数须以正确的顺序传入函数。调用时的数量必须和声明时的一样。
def test_1(var):
    print(var)

test_1(1)

# 关键字参数和函数调用关系紧密，函数调用使用关键字参数来确定传入的参数值。
# 使用关键字参数允许函数调用时参数的顺序与声明时不一致，因为 Python 解释器能够用参数名匹配参数值。
def test_2(var1, var2):
    print(var1, var2)

test_2(var2=2, var1=1)

# 默认参数：调用函数时，如果没有传递参数，则会使用默认参数
# 默认参数最好使用不可变数据类型，否则可能会出现超出预期的场景
def test_3(var1, var2=2):
    print(var1, var2)

test_3(1)

# 不定长参数: 当需要一个函数能处理比当初声明时更多的参数时，这些参数就叫做不定长参数
# *表示一个元组
def test_4(var1, *var2):
    print(var1)
    print(var2)

test_4(1, 10, 20, 30)

# **表示一个字典
def test_4(var1, **var2):
    print(var1)
    print(var2)

test_4(1, a=10, b=20, c=30)

# 文档字符串
# 第一行应该是对象目的的简要概述。为简洁起见，它不应显式声明对象的名称或类型，因为这些可通过其他方式获得（除非名称恰好是描述函数操作的动词）。这一行应以大写字母开头，以句点结尾。
# 如果文档字符串中有更多行，则第二行应为空白，从而在视觉上将摘要与其余描述分开。后面几行应该是一个或多个段落，描述对象的调用约定，它的副作用等。
def test_5():
    &quot;&quot;&quot;Do nothing, but document it.

    No, really, it doesn't do anything
    &quot;&quot;&quot;
    print(&quot;test&quot;)

print(test_5.__doc__)

</code></pre>
<h3 id="匿名函数">匿名函数</h3>
<p>python 使用 lambda 来创建匿名函数。</p>
<p>所谓匿名，意即不再使用 def 语句这样标准的形式定义一个函数。</p>
<ul>
<li>lambda 只是一个表达式，函数体比 def 简单很多。</li>
<li>lambda的主体是一个表达式，而不是一个代码块。仅仅能在lambda表达式中封装有限的逻辑进去。</li>
<li>lambda 函数拥有自己的命名空间，且不能访问自己参数列表之外或全局命名空间里的参数。</li>
<li>虽然lambda函数看起来只能写一行，却不等同于C或C++的内联函数，后者的目的是调用小函数时不占用栈内存从而增加运行效率。</li>
</ul>
<pre><code># lambda函数
# 语法
lambda [arg1 [,arg2,.....argn]]:expression

# 简单例子
sum_result = lambda a1, a2: a1 + a2
print(sum_result(2, 3))

</code></pre>
<h2 id="错误与异常">错误与异常</h2>
<h3 id="异常">异常</h3>
<p>即使语句或表达式在语法上是正确的，但在尝试执行时，它仍可能会引发错误。在执行时检测到的错误被称为异常，异常不一定会导致严重后果。</p>
<pre><code># 简单异常
print(3 / 0)
# 输出
Traceback (most recent call last):
  File &quot;D:/knowledge/python/pycharm/pythonProject/learn/test.py&quot;, line 123, in &lt;module&gt;
    print(3 / 0)
ZeroDivisionError: division by zero

# 处理异常
def test_6(x, y):
    try:
        x / y
    except Exception as msg:
        print(msg)

test_6(3, 0)

# try finally， finally语句中的代码始终都会执行
def test_7(x, y):
    try:
        x / y
    except ZeroDivisionError as msg:
        print(msg)
    finally:
        print(&quot;finally exec&quot;)

test_7(3, 0)

# raise抛出异常
raise Exception(&quot;raise exception&quot;)

</code></pre>
<p>一个 <a href="https://docs.python.org/zh-cn/3.9/reference/compound_stmts.html#try">try</a> 语句可能有多个 except 子句，以指定不同异常的处理程序。 最多会执行一个处理程序。 处理程序只处理相应的 try 子句中发生的异常，而不处理同一 try 语句内其他处理程序中的异常。 一个 except 子句可以将多个异常命名为带括号的元组。</p>
<pre><code># 一个except多个异常
try:
    expression
except (RuntimeError, OSError, TypeError):
    pass
  
# 多个except
# 最后的 except 子句可以省略异常名，以用作通配符。但请谨慎使用，因为以这种方式很容易掩盖真正的编程错误！
# 它还可用于打印错误消息，然后重新引发异常（同样允许调用者处理异常）   
import sys
try:
    f = open('myfile.txt')
    s = f.readline()
    i = int(s.strip())
except OSError as err:
    print(&quot;OS error: {0}&quot;.format(err))
except ValueError:
    print(&quot;Could not convert data to an integer.&quot;)
except:
    print(&quot;Unexpected error:&quot;, sys.exc_info()[0])
    raise 

</code></pre>
<p>使用with语句可以自动关闭流</p>
<pre><code># with关键字会自动关闭文件,try捕获异常输出
try:
    with open('/opt/pythonDir/temp.txt', 'w+') as f:
        f.write(&quot;test content&quot;)
    print(f.close())
except IOError as err:
    print(&quot;exception: {0}&quot;.format(err))

</code></pre>
<h2 id="面向对象">面向对象</h2>
<h3 id="python中的类">python中的类</h3>
<p>类实例化后，可以使用其属性，实际上，创建一个类之后，可以通过类名访问其属性。</p>
<pre><code># 创建类
class FirstClass:
    name = &quot;python&quot;

    def fn(self):
        print(self.name)
        return &quot;hello world&quot;


# 实例化类
first_class = FirstClass()
# 调用变量
print(&quot;name: &quot;, first_class.name)
# 调用函数
print(&quot;function: &quot;, first_class.fn())

# 定义初始化函数
class People:
    # 基本属性
    name = &quot;&quot;
    age = 0
    # 使用双下划线定义私有属性，其无法被类外部所访问
    __private_attribute = &quot;private_attribute&quot;

    def __init__(self, name, age):
        self.name = name
        self.age = age

    def speak(self):
        print(&quot;{0} speak: {1}, {2}&quot;.format(self.name, self.age, self.__private_attribute))

    def action(self):
        print(&quot;base class action&quot;)


# people = People(&quot;Jack&quot;, 100)
# people.speak()

# 继承类
class Employee(People):
    profession = &quot;&quot;

    def __init__(self, name, age, profession):
        People.__init__(self, name, age)
        self.profession = profession

    def speak(self):
        print(&quot;{0} speak: {1}, {2}&quot;.format(self.name, self.age, self.profession))

    # 重写toString方法
#   def __str__(self):
#       return &quot;Employee:{name: %s, age: %d, profession: %s}&quot; % (self.name, self.age, self.profession)

    # 重写toString方法（使用f-string格式化）
    def __str__(self):
        return f&quot;Employee:(name: {self.name}, age: {self.age}, profession: {self.profession})&quot;

teacher = Employee(&quot;Tom&quot;, 40, &quot;teacher&quot;)
teacher.speak()
teacher.action()
print(teacher.name)

json_str = json.dumps(teacher.__dict__)
print(json_str)
teacher2 = json.loads(json_str)
print(teacher2)

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lua基本知识]]></title>
        <id>https://philosopherzb.github.io/post/lua-ji-ben-zhi-shi/</id>
        <link href="https://philosopherzb.github.io/post/lua-ji-ben-zhi-shi/">
        </link>
        <updated>2022-04-02T09:10:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Lua脚本知识。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/tianjin-2185510_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="简介与安装">简介与安装</h2>
<h3 id="简介">简介</h3>
<p>Lua是一个强大，高效且轻量化的嵌入式脚本语言，由 clean C（标准 C 和 C++ 间共通的子集）实现成一个库，支持过程编程，面向对象编程，函数编程以及数据驱动编程，以此来供任何需要的程序使用。</p>
<p>同时，作为一门扩展式语言，Lua 没有 &quot;main&quot; 程序的概念：它只能 嵌入 一个宿主程序中工作，该宿主程序被称为 被嵌入程序 或者简称 宿主 。 宿主程序可以调用函数执行一小段 Lua 代码，可以读写 Lua 变量，可以注册 C 函数让 Lua 代码调用。依靠 C 函数，Lua 可以共享相同的语法框架来定制编程语言，从而适用不同的领域。</p>
<p>这也是其设计目的：为了给应用程序提供灵活的可扩展及定制化功能。</p>
<h3 id="安装">安装</h3>
<p>linux/mac安装lua，命令如下，第四行根据系统自行选择，可在<a href="http://www.lua.org/ftp">官网地址</a>中获取最新的lua下载信息。</p>
<pre><code>curl -R -O http://www.lua.org/ftp/lua-5.*.*.tar.gz
tar zxf lua-5.*.*.tar.gz
cd lua-5.*.*
make linux/macosx test
make install

</code></pre>
<p>windows下载LuaForWindows（双击安装即可）：</p>
<ul>
<li>Github下载地址：<a href="https://github.com/rjpcomputing/luaforwindows/releases">点击此处跳转下载页面</a></li>
<li>Google下载地址 : <a href="https://code.google.com/p/luaforwindows/downloads/list">点击此处跳转下载页面</a></li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230306010.png" alt="img" loading="lazy"></figure>
<h2 id="基本概念">基本概念</h2>
<h3 id="值与类型">值与类型</h3>
<p>Lua 是一门动态类型语言，这意味着变量没有类型，只有值才有类型；值可以存储在变量中，作为参数传递或者返回。</p>
<p>Lua 中有八种基本类型：nil、boolean、number、string、function、userdata、thread 和 table</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>nil</td>
<td>NIL是值nil的类型，用于与其他值区分；通常用来表示一个有意义的值不存在时的状态。如果用于条件判断，其等同于boolean中的false。</td>
</tr>
<tr>
<td>boolean</td>
<td>包含两个值：true和false</td>
</tr>
<tr>
<td>number</td>
<td>整数及实数(浮点数)：标准Lua使用64位整数及双精度(64位)浮点数；小型机器和嵌入式系统可以使用32位整数及单精度(32位)浮点数。</td>
</tr>
<tr>
<td>string</td>
<td>不可变的字节序列(字符串)，可以使用双引号(&quot;&quot;)，单引号('')，双中括号([[]])表示</td>
</tr>
<tr>
<td>function</td>
<td>由C或者Lua编写的函数</td>
</tr>
<tr>
<td>userdata</td>
<td>存储在变量中的C语言数据，用户数据类型的值是一个内存块，分为：完全用户数据：指一块由Lua管理的内存对应的对象；轻量用户数据：一个简单的C指针。Lua可使用元表(metatable)对userdata进行操作;如若需要修改用户数据中的值，只能通过C API进行处理，这保证了数据仅被宿主机所控制。</td>
</tr>
<tr>
<td>thread</td>
<td>一个独立的执行序列，主要用于实现协程(coroutine)。注意：Lua中的线程与操作系统没有任何关系，因此，它可以为那些不支持原生线程的系统提供协程支持。线程跟协程的区别：线程可以同时多个运行，而协程任意时刻只能运行一个，并且处于运行状态的协程只有被挂起（suspend）时才会暂停。</td>
</tr>
<tr>
<td>table</td>
<td>一个关联数组，是Lua中唯一的数据结构。它可被用于表示普通数组、序列、符号表、集合、记录、图、树等等。</td>
</tr>
</tbody>
</table>
<h3 id="错误处理">错误处理</h3>
<p>由于 Lua 是一门嵌入式扩展语言，其所有行为均源于宿主程序中代码对某个 Lua 库函数的调用（如果是单独使用 Lua 时，那么Lua 程序就是宿主程序）。因此，在编译或运行 Lua 代码块的过程中，无论何时发生错误，控制权都返回给宿主，由宿主负责采取恰当的措施（比如打印错误消息）。</p>
<p>在Lua中的可以通过asset函数或者error函数处理错误。</p>
<p>asset函数首先会检查第一个参数，如果没有问题则不做任何事情，否则输出第二个参数作为错误信息。使用样例：</p>
<pre><code>-- 错误处理
local function add(i, j)
    assert(type(i) == &quot;number&quot;, &quot;i 不是一个数字&quot;)
	assert(type(j) == &quot;number&quot;, &quot;j 不是一个数字&quot;)
	return i + j
end
print(add(rd, 1))
-- 输出
lua: demo.lua:3: i 不是一个数字
stack traceback:
	[C]: in function 'assert'
	demo.lua:3: in function 'add'
	demo.lua:7: in main chunk
	[C]: ?

</code></pre>
<p>error函数：error (message [, level])，显式地抛出一个错误，内容为参数message，</p>
<p>Level参数指示获得错误的位置: Level=1[默认]：为调用error位置(文件+行号)；Level=2：指出哪个调用error的函数的函数；Level=0:不添加错误位置信息。</p>
<pre><code>error('error msg..')
--输出
lua: demo.lua:1: error msg..
stack traceback:
	[C]: in function 'error'
	demo.lua:1: in main chunk
	[C]: ?

</code></pre>
<p>如果需要在 Lua 中捕获这些错误，可以使用 <a href="https://www.bookstack.cn/read/lua-5.3/2.md#pdf-pcall">pcall</a> 或<a href="https://www.bookstack.cn/read/lua-5.3/2.md#pdf-xpcall">xpcall</a>在 保护模式 下调用一个函数。</p>
<p>使用xpcall或pcall时，需要提供一个 消息处理函数 用于错误抛出时调用。该函数需接收原始的错误消息，并返回一个新的错误消息。</p>
<p>它在错误发生后栈尚未展开时调用，因此可以利用栈来收集更多的信息，比如通过探知栈来创建一组栈回溯信息。同时，该处理函数也处于保护模式下，所以该函数内发生的错误会再次触发它（递归）。如果递归太深，Lua 会终止调用并返回一个合适的消息。</p>
<h3 id="垃圾收集">垃圾收集</h3>
<p>Lua采用了自动内存管理（类似于Java）。这意味着使用者不用操心新创建的对象需要的内存如何分配出来，也不用考虑在对象不再被使用后怎样释放它们所占用的内存。</p>
<p>Lua 运行了一个 垃圾收集器 来收集所有 死对象（即在 Lua 中不可能再访问到的对象）来完成自动内存管理的工作。</p>
<p>Lua 中所有用到的内存，如：字符串、表、用户数据、函数、线程、内部结构等，都服从自动管理。</p>
<p>Lua 实现了一个增量标记-扫描收集器。它使用这两个数字来控制垃圾收集循环：垃圾收集器间歇率 和 垃圾收集器步进倍率。这两个数字都使用百分数为单位（例如：值 100 在内部表示 1 ）。</p>
<p>垃圾收集器间歇率控制着收集器需要在开启新的循环前要等待多久。增大这个值会减少收集器的积极性。当这个值比 100 小的时候，收集器在开启新的循环前不会有等待。设置这个值为 200 就会让收集器等到总内存使用量达到之前的两倍时才开始新的循环。</p>
<p>垃圾收集器步进倍率控制着收集器运作速度相对于内存分配速度的倍率。增大这个值不仅会让收集器更加积极，还会增加每个增量步骤的长度。不要把这个值设得小于 100 ，那样的话收集器就工作的太慢了以至于永远都干不完一个循环。默认值是 200 ，这表示收集器以内存分配的“两倍”速工作。</p>
<p>如果将步进倍率设为一个非常大的数字（比程序可能用到的字节数还大 10% ），收集器的行为就像一个 stop-the-world 收集器。接着若把间歇率设为 200 ，收集器的行为就和过去的 Lua 版本一样了：每次 Lua 使用的内存翻倍时，就做一次完整的收集。</p>
<p>通过在 C 中调用 lua_gc或在 Lua 中调用collectgarbage ([opt [, arg]])来控制自动内存管理。通过参数 opt 它提供了一组不同的功能：</p>
<ul>
<li>collectgarbage(&quot;collect&quot;): 做一次完整的垃圾收集循环。</li>
<li>collectgarbage(&quot;count&quot;): 以 K 字节数为单位返回 Lua 使用的总内存数。 这个值有小数部分，所以只需要乘上 1024 就能得到 Lua 使用的准确字节数（除非溢出）。</li>
<li>collectgarbage(&quot;restart&quot;): 重启垃圾收集器的自动运行。</li>
<li>collectgarbage(&quot;setpause&quot;): 将 arg 设为收集器的 间歇率。 返回 间歇率 的前一个值。</li>
<li>collectgarbage(&quot;setstepmul&quot;): 返回 步进倍率 的前一个值。</li>
<li>collectgarbage(&quot;step&quot;): 单步运行垃圾收集器。 步长&quot;大小&quot;由 arg 控制。 传入 0 时，收集器步进（不可分割的）一步。 传入非 0 值， 收集器收集相当于 Lua 分配这些多（K 字节）内存的工作。 如果收集器结束一个循环将返回 true 。</li>
<li>collectgarbage(&quot;stop&quot;): 停止垃圾收集器的运行。 在调用重启前，收集器只会因显式的调用运行。</li>
<li>collectgarbage(&quot;isrunning&quot;): 返回表示收集器是否工作的布尔值。</li>
</ul>
<pre><code>demoTable = {&quot;LUa&quot;, &quot;shell&quot;, &quot;python&quot;, &quot;Java&quot;, &quot;ruby&quot;}
print(collectgarbage(&quot;count&quot;))
demoTable = nil

print(collectgarbage(&quot;count&quot;))
print(collectgarbage(&quot;collect&quot;))
print(collectgarbage(&quot;count&quot;))
-- 输出
21.0859375
21.1123046875
0
19.498046875

</code></pre>
<h3 id="协程coroutine">协程（coroutine）</h3>
<p>Lua中的协程也被称作协同式多线程，代表了一段独立执行的线程。它与线程非常的类似：拥有独立的堆栈、局部变量、指令指针，同时又与其他协程共享全局变量。</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>coroutine.create()</td>
<td>创建一个协程。其唯一的参数是该协程的主函数。create 函数只负责新建一个协程并返回其句柄（一个 thread 类型的对象）；而不会启动该协程。</td>
</tr>
<tr>
<td>coroutine.resume()</td>
<td>执行一个协程。第一次调用coroutine.resume时，第一个参数应传入coroutine.create返回的线程对象，然后协程从其主函数的第一行开始执行。传递给coroutine.resume的其他参数将作为协程主函数的参数传入。协程启动之后，将一直运行到它终止或调用coroutine.yield。</td>
</tr>
<tr>
<td>coroutine.yield()</td>
<td>挂起协程，让出执行权。一般与resume配合使用。协程挂起时，对应的最近coroutine.resume函数会立刻返回，即使该挂起操作发生在内嵌函数调用中（即不在主函数，但在主函数直接或间接调用的函数内部）。在协程挂起的情况下，coroutine.resume也会返回 true，并加上传给coroutine.yield的参数。当下次重启同一个协程时，协程会接着从挂起点继续执行。此时，此前挂起点处对coroutine.yield的调用会返回，返回值为传给coroutine.resume的第一个参数之外的其他参数。</td>
</tr>
<tr>
<td>coroutine.status()</td>
<td>查看线程状态：dead，suspended，running</td>
</tr>
<tr>
<td>coroutine.warp()</td>
<td>与create类似，也是创建一个协程。不同之处在于，它不返回协程本身，而是返回一个函数。调用这个函数将启动该协程。传递给该函数的任何参数均当作coroutine.resume的额外参数。</td>
</tr>
<tr>
<td>coroutine.running()</td>
<td>返回一个running协程的协程号。</td>
</tr>
</tbody>
</table>
<p>协程样例及步骤解释：</p>
<p>1、函数foo加载，协程co创建。</p>
<p>2、第15行print执行，由coroutine.resume触发执行co协程。</p>
<p>2.1、接着打印第7行数据，对应输出结果第28行。</p>
<p>2.2、随后第8行调用函数foo，开始执行第2行，打印结果为第29行</p>
<p>2.3、最后第3行执行，协程被挂起，打印结果对应第30行。</p>
<p>注意：yield调用后的返回值为下次resume同一协程时的输入参数。对应第8行的r。</p>
<p>3、第16行执行，打印协程状态，为suspended（挂起）。</p>
<p>4、第18行执行，resume同一协程，输入参数“r”对应第8行的local r，也即第3行yield挂起后的返回值。</p>
<p>4.1、第9行打印，对应结果为第32行。</p>
<p>4.2、第10行执行，协程挂起，返回r，s两个值，作为下次resume时可输入的值。</p>
<p>5、第21行执行，resume同一协程，输入参数“x”，“y”，对应第10行的local r，s。</p>
<p>5.1、第11行执行，打印协程内容，对接结果为第35行。</p>
<p>5.2、协程结束，打印结果为第36行。</p>
<p>6、第22执行，显示协程已dead（死亡），对应结果为第37行。</p>
<p>7、第24行执行，resume协程，开始提示，协程已经dead，对应结果为第38行。</p>
<pre><code>function foo (a)
  print(&quot;foo&quot;, a)
  return coroutine.yield(2*a)
end

co = coroutine.create(function (a,b)
      print(&quot;co-body&quot;, a, b)
      local r = foo(a+1)
      print(&quot;co-body&quot;, r)
      local r, s = coroutine.yield(a+b, a-b)
      print(&quot;co-body&quot;, r, s)
      return b, &quot;end&quot;
end)

print(&quot;main&quot;, coroutine.resume(co, 1, 10))
print(coroutine.status(co))

print(&quot;main&quot;, coroutine.resume(co, &quot;r&quot;))
print(coroutine.status(co))

print(&quot;main&quot;, coroutine.resume(co, &quot;x&quot;, &quot;y&quot;))
print(coroutine.status(co))

print(&quot;main&quot;, coroutine.resume(co, &quot;x&quot;, &quot;y&quot;))
print(coroutine.status(co))

-- 输出
co-body	1	10
foo	2
main	true	4
suspended
co-body	r
main	true	11	-9
suspended
co-body	x	y
main	true	10	end
dead
main	false	cannot resume dead coroutine
dead

</code></pre>
<h2 id="基本语法">基本语法</h2>
<h3 id="变量">变量</h3>
<p>在Lua中的进行赋值操作，是不需要指定类型的。如下：</p>
<pre><code>-- 简单赋值
z = 1
print(z)
-- 多重赋值，同时也支持多重返回值
a,b = 3,4
print(a)
print(b)
return a,b

-- 本地（局部）变量赋值
local x,y=5,6
print(x)
print(y)
-- 交换数据
x,y = y,x
print(x)
print(y)

-- 代码块
do
    local str = &quot;world&quot;
	print(str)
end

</code></pre>
<h3 id="表达式">表达式</h3>
<p>在Lua中的表达式基本与高级语言相差无几，如下：</p>
<p>操作符</p>
<ol>
<li>算术运算符：+ - * / ^ (加减乘除幂)</li>
<li>关系运算符：&lt; &gt; &lt;= &gt;= == ~=</li>
<li>逻辑运算符：and or not</li>
<li>连接运算符：..</li>
</ol>
<p>有几个操作符需要注意：</p>
<ul>
<li>a ~= b 即 a 不等于 b</li>
<li>a ^ b 即 a 的 b 次方</li>
<li>a .. b 将 a 和 b 作为字符串连接</li>
</ul>
<p>优先级：</p>
<ol>
<li>^</li>
<li>not -(负号)</li>
<li>*/</li>
<li>+-</li>
<li>..</li>
<li>&lt; &gt; &lt;= &gt;= ~= ==</li>
<li>and</li>
<li>or</li>
</ol>
<h3 id="控制流">控制流</h3>
<p>Lua以if for while等来进行流程控制，具体如下所示：</p>
<pre><code>-- if语句
local num = &quot;21&quot;
if (tonumber(num)~=nil) then
    print(&quot;tonumber result: &quot;..tonumber(num))
else
    print(&quot;tonumber result is not number&quot;)
end

-- for语句
-- 三个数字分别表示初始值，终止值，步长
for i = 2, 10, 2 do
    print(i)
end
-- i，v分别表示数组对应的索引及值，注意：Lua中的数组是从1开始排序
demoTable = {&quot;LUa&quot;, &quot;shell&quot;, &quot;python&quot;, &quot;Java&quot;, &quot;ruby&quot;}
for i,v in ipairs(demoTable) do
    print(i)
    print(v)
end

-- while语句
local i = 0
while i &lt; 2 do
    print(i)
	i = i + 1
	if (i == 1) then break end
end

</code></pre>
]]></content>
    </entry>
</feed>