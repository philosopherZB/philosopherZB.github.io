<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://philosopherzb.github.io</id>
    <title>Philosopher</title>
    <updated>2023-03-15T08:33:35.262Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://philosopherzb.github.io"/>
    <link rel="self" href="https://philosopherzb.github.io/atom.xml"/>
    <subtitle>WORLD AS CODE</subtitle>
    <logo>https://philosopherzb.github.io/images/avatar.png</logo>
    <icon>https://philosopherzb.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, Philosopher</rights>
    <entry>
        <title type="html"><![CDATA[消息队列与kafka]]></title>
        <id>https://philosopherzb.github.io/post/xiao-xi-dui-lie-yu-kafka/</id>
        <link href="https://philosopherzb.github.io/post/xiao-xi-dui-lie-yu-kafka/">
        </link>
        <updated>2022-06-11T05:32:03.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述消息队列以及kafka整体架构。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/mountain-547363_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="消息队列message-queue">消息队列(message queue)</h2>
<h3 id="简介">简介</h3>
<p>消息队列是一种进程间通信或同一进程的不同线程间的通信方式；针对如今微服务或云架构，则指代服务间的异步通信方式。</p>
<p>在现代云架构或微服务架构中，应用程序被分解为多个规模较小且易于开发、部署和维护的构建块。消息队列可以为这些分布式应用程序提供通信与协调；同时，消息队列也可以显著的简化分离应用编码，并提高性能、可靠性和可扩展性等。</p>
<h3 id="消息投递模式">消息投递模式</h3>
<h4 id="端到端point-to-point">端到端(Point-to-point)</h4>
<p>在端到端消息模式中，一条消息将会一直存储在队列中，直到被消费者所接收；且，发送者必须要知道消费者的一些关键信息，例如：将要发送消息的队列名或者特定的队列管理器名。</p>
<p>此模式中，消息队列将会提供一个临时存储消息的轻量级缓冲区，并允许微服务连接到队列以发送和接收消息的终端节点；这种情况下，消息的规模一般较小，如请求，恢复，错误消息及明文消息等；同时一条消息只能由一个接收者处理一次。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314008.png" alt="img" loading="lazy"></figure>
<h4 id="发布订阅publishsubscribe">发布订阅(Publish/Subscribe)</h4>
<p>发布订阅模式中，一条消息由发送者推送至特定的主题(topic)，随后所有订阅了该topic的消费者都将收到同一条广播消息。</p>
<p>消息主题的订阅者通常会执行不同的功能，并可以同时对消息执行不同的操作。发布者无需知道谁在使用广播的信息，而订阅者也无需知道消息来自哪里。这种消息收发模式与端到端稍有不同，在端到端中，发送消息的组件通常知道发送的目的地。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314009.png" alt="img" loading="lazy"></figure>
<h3 id="消息队列优势">消息队列优势</h3>
<ul>
<li>解耦：在分布式系统中，同一份数据可能要分发给不同的程序进行处理；例如订单生成后，要扣库存，加积分；此时如果由订单服务直接调用库存服务及积分服务的接口，将会让订单服务变得十分臃肿，且不利于后续扩展（比如再加一个支付服务）。这时便可以加入消息队列，采用发布订阅模式，订单服务只需将消息发送至指定topic即可，其他服务选择性订阅。这样便将系统业务进行了解耦操作，增加了系统的可扩展性。</li>
<li>异步通信：消息队列天然支持异步操作（所有的消息都会存储在队列中，消费者可以延迟处理），针对分布式系统中的异步操作，可采用消息队列进行处理。</li>
<li>削峰填谷：针对高并发大数据的场景，消息队列可以有效地做到削峰填谷。数据推送高峰时，将由消息队列暂存所有的数据，等到数据推送低谷时，由消费者逐步消费所有的数据。必要时，也可以额外的增加消费者进行数据处理。</li>
<li>缓冲：消息队列通过一个缓冲层来帮助任务最高效率的执行；写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。</li>
<li>跨平台：消息队列支持跨平台消息处理，只需要发送者及消费者约定数据格式即可。例如，由Java发送消息，go消费消息。</li>
<li>灵活性与可扩展性：在高流量期间，可以通过动态的扩展机器来接受更多的消息，防止高并发冲击服务，导致服务挂掉。</li>
</ul>
<h2 id="kafka">kafka</h2>
<h3 id="简介-2">简介</h3>
<p>Kafka 是由 Linkedin 公司开发的，它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的消息流平台，它同时也是一款开源的基于发布订阅模式的消息引擎系统；</p>
<p>Kafka由服务端及客户端组成，且服务端和客户端可以通过高性能的TCP网络协议进行通讯。关于kafka的部署环境，无论本地还是云环境中的裸机，虚拟机或者容器都可以支持。</p>
<h3 id="基本术语">基本术语</h3>
<ul>
<li>
<p>生产者(Producer)：向kafka中的主题(topic)发布消息事件的客户端应用程序被称为生产者。</p>
</li>
<li>
<p>消费者(Consumer)：订阅了kafka消息事件所在的主题(topic)的客户端应用程序称为消费者。</p>
</li>
<li>
<p>主题(Topic)：用于存储同一类消息事件；打个简单的比喻：主题类似于文件系统中的文件夹，而消息事件则类似于文件夹中的文件。</p>
</li>
<li>
<p>消息(Message)：kafka中的数据单元被称为消息，也可以叫记录(record)。</p>
</li>
<li>
<p>偏移量(Offset)：是一种元数据，且是一个不断递增的整数值；当消费者处理完消息后，将会提交offset+1到broker中。</p>
</li>
<li>
<p>broker： 一个独立的 Kafka 服务器就被称为 broker，broker 接收来自生产者的消息，并为消息设置偏移量，同时持久化消息到磁盘。</p>
</li>
<li>
<p>分区(Partition)：在一个主题中可以存在一个或多个分区，每个分区中消息有序；同一主题中的多个分区可以位于不同机器上，方便后续扩容操作。</p>
</li>
<li>
<p>副本(Replica)：消息的备份称为副本，在创建主题时可以指定副本数量。</p>
</li>
<li>
<p>重平衡(Rebalance)：当消费组(多个消费者将会形成一个消费组)中的某个消费者实例挂掉或者新增一个消费者实例时，其他消费者实例可以自动地重新分配订阅主题，这个过程叫重平衡。</p>
</li>
<li>
<p>AR与ISR：AR即可分配副本 Assigned Replicas；而所有与leader副本保持一定同步状态的副本（包含leader副本）构成ISR（In-Sync Replicas）；ISR集合是AR集合中的一个子集。</p>
</li>
<li>
<p>ISR的伸缩：leader 副本负责维护和跟踪 ISR 集合中所有 follower 副本的滞后状态，当 follower 副本落后太多或失效时，leader 副本会把它从 ISR 集合中剔除。如果 OSR（Out-Sync Replicas） 集合中有 follower 副本“追上”了 leader 副本，那么 leader 副本会把它从 OSR 集合转移至 ISR 集合。默认情况下，当 leader 副本发生故障时，只有在 ISR 集合中的副本才有资格被选举为新的 leader，而在 OSR 集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变）。</p>
<p>replica.lag.time.max.ms ： 这个参数的含义是 Follower 副本能够落后 Leader 副本的最长时间间隔，默认10s。</p>
<p>unclean.leader.election.enable：是否允许 Unclean 领导者选举。开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。</p>
</li>
<li>
<p>HW与LW：HW（High Watermark）俗称高水位，它标识了一个特定的偏移量(offset)，消费者只能拉取此偏移量之前的数据；LW（Low Watermark）俗称低水位，它同样标识了一个特定的偏移量(offset)，一般为AR集合中最小的LSO值；LW值的增长与副本的拉取或删除请求息息相关。</p>
</li>
<li>
<p>LSO：LSO 是LogStartOffset的缩写，一般情况下，日志文件的起始偏移量 logStartOffset 等于第一个日志分段的 baseOffset，但这并不是绝对的，logStartOffset 的值可以通过 DeleteRecordsRequest 请求(比如使用 KafkaAdminClient 的 deleteRecords()方法、使用 kafka-delete-records.sh 脚本、日志的清理和截断等操作）进行修改。</p>
</li>
<li>
<p>LEO：LEO是 LogEndOffset 的缩写，它标识当前日志文件中下一条待写入消息的 offset，如下图中 offset 为9的位置即为当前日志文件的 LEO，LEO 的大小相当于当前日志分区中最后一条消息的 offset 值加1。分区 ISR 集合中的每个副本都会维护自身的 LEO，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息</p>
</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314010.png" alt="img" loading="lazy"></figure>
<ul>
<li>发送者，broker集群，消费者基本协作结构图</li>
</ul>
<p><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314011.png" alt="img" loading="lazy"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314011.png" alt="" loading="lazy"></p>
<ul>
<li>主题剖析结构图</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315001.png" alt="img" loading="lazy"></figure>
<ul>
<li>一个两节点的 kafka 集群支持的 2 个消费组的四个分区 (P0-P3)。消费者 A 有两个消费者实例，消费者 B 有四个消费者实例。</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315002.png" alt="img" loading="lazy"></figure>
<h3 id="基本特性">基本特性</h3>
<ul>
<li>高吞吐、低延迟：收发消息快是kafka最重要的特性之一，即使在非常廉价的机器上，kafka也可以轻松达到每秒几十万条消息的传输速率，且此过程中最低延迟仅有几毫秒；一般可以用来做实时日志聚合。</li>
<li>持久化：kafka中的所有消息都会被其持久化存储在文件系统中；在这个过程中，kafka采用顺序写磁盘及直接写页缓存的机制提高IO效率（顺序写磁盘的速度接近于随机写内存的速度：<a href="https://queue.acm.org/detail.cfm?id=1563874">点击此处跳转文章页面</a>）。</li>
<li>水平扩容(Scale out)：kafka提供分区机制以达到动态扩容的效果，即增加分区数（不支持减少分区操作）；</li>
<li>分区容错(Partition-tolerance)：即使集群中的某个节点挂掉，kafka仍然可以提供可用性（可用性与一致性间的冲突，kafka目前采用的是ISR机制进行了部分妥协）。</li>
<li>核心API：Producer API，Consumer API，Streams API，Connect API，Admin API</li>
</ul>
<h3 id="生产者producer">生产者(Producer)</h3>
<h4 id="简介-3">简介</h4>
<p>生产者无需经过路由便可将消息发送至主分区所在的服务器上；为了实现这个功能，所有的kafka服务器节点都能够响应这样的元数据请求：哪些服务器存活，主题的主分区位于哪台服务器上。</p>
<p>对于生产者而言，只需要执行发送请求，消息将会自动根据路由规则分配到不同的分区上。</p>
<p>消息路由规则：如果指定了 partition，则直接使用；如果未指定 partition 但指定了 key，则通过对 key 的 value 进行hash 选出一个 partition（形如：Math.abs(key.hashCode()) % partitions.size()）； 如果partition 和 key 都未指定，便会轮询选出一个 partition。</p>
<h4 id="整体流程架构">整体流程架构</h4>
<p>整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。</p>
<p>在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。</p>
<p>Sender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。</p>
<p>RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315003.png" alt="img" loading="lazy"></figure>
<h5 id="序列化器">序列化器</h5>
<p>生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka；在消费者侧同样需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换成相应的对象。网络传输过程中，序列化是必须的操作。自定义时需要实现org.apache.kafka.common.serialization.Serializer</p>
<pre><code>package org.apache.kafka.common.serialization;

import org.apache.kafka.common.header.Headers;

import java.io.Closeable;
import java.util.Map;

/**
 * 一个用于转换对象为字节的接口
 *
 * 实现此接口的类应具有不带参数的构造函数
 * 
 * @param &lt;T&gt; Type to be serialized from.
 */
public interface Serializer&lt;T&gt; extends Closeable {

    /**
     * 配置当前类，此方法一般是在创建 KafkaProducer 实例的时候调用的，主要用来确定编码类型。
     * @param configs configs in key/value pairs
     * @param isKey whether is for key or value
     */
    default void configure(Map&lt;String, ?&gt; configs, boolean isKey) {
        // intentionally left blank
    }

    /**
     * 转换数据为字节数组（执行序列化操作）
     * 可以进行编解码，如果 Kafka 客户端提供的几种序列化器都无法满足应用需求，
     * 则可以选择使用如 Avro、JSON、Thrift、ProtoBuf 和 Protostuff 等通用的序列化工具来实现，
     * 或者使用自定义类型的序列化器来实现。  
     *
     * @param topic topic associated with data
     * @param data typed data
     * @return serialized bytes
     */
    byte[] serialize(String topic, T data);

    /**
     * 转换数据为字节数组（执行序列化操作）
     * 可以进行编解码，如果 Kafka 客户端提供的几种序列化器都无法满足应用需求，
     * 则可以选择使用如 Avro、JSON、Thrift、ProtoBuf 和 Protostuff 等通用的序列化工具来实现，
     * 或者使用自定义类型的序列化器来实现。  
     *
     * @param topic topic associated with data
     * @param headers headers associated with the record
     * @param data typed data
     * @return serialized bytes
     */
    default byte[] serialize(String topic, Headers headers, T data) {
        return serialize(topic, data);
    }

    /**
     * 关闭序列化器
     * 注意：此方法必须是幂等的，因为它可能会被多次调用。
     */
    @Override
    default void close() {
        // intentionally left blank
    }
}

</code></pre>
<h5 id="分区器">分区器</h5>
<p>分区器的作用就是为消息分配partition。如果消息 ProducerRecord 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值；或者依赖轮询分配partition（可参考消息路由规则）。Kafka 中提供的默认分区器是 org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了 org.apache.kafka.clients.producer.Partitioner 接口。</p>
<pre><code>package org.apache.kafka.clients.producer.internals;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.utils.Utils;

import java.util.Map;

/**
 * 默认的分区策略器
 * &lt;ul&gt;
 * &lt;li&gt;如果记录中指定了分区，则直接使用
 * &lt;li&gt;如果未制定分区，但存在key，则基于key的hash值选择一个分区（拥有相同 key 的消息会被写入同一个分区，前提是分区数不会变更）
 * &lt;li&gt;如果分区和key都不存在，则选择在批处理已满时更改的粘性分区（轮询操作）。
 * 
 * See KIP-480 for details about sticky partitioning.
 */
public class DefaultPartitioner implements Partitioner {

    private final StickyPartitionCache stickyPartitionCache = new StickyPartitionCache();

    public void configure(Map&lt;String, ?&gt; configs) {}

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param key The key to partition on (or null if no key)
     * @param keyBytes serialized key to partition on (or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        return partition(topic, key, keyBytes, value, valueBytes, cluster, cluster.partitionsForTopic(topic).size());
    }

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param numPartitions The number of partitions of the given {@code topic}
     * @param key The key to partition on (or null if no key)
     * @param keyBytes serialized key to partition on (or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster,
                         int numPartitions) {
        if (keyBytes == null) {
            return stickyPartitionCache.partition(topic, cluster);
        }
        // 基于key 的hash值选择一个翻去
        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
    }

    public void close() {}
  
    /**
     * 轮询选择分区
     */
    public void onNewBatch(String topic, Cluster cluster, int prevPartition) {
        stickyPartitionCache.nextPartition(topic, cluster, prevPartition);
    }
}

</code></pre>
<pre><code>package org.apache.kafka.clients.producer;

import org.apache.kafka.common.Configurable;
import org.apache.kafka.common.Cluster;

import java.io.Closeable;

/**
 * 分区器接口，自定义分区器时，需要实现此接口。
 */
public interface Partitioner extends Configurable, Closeable {

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param key The key to partition on (or null if no key)
     * @param keyBytes The serialized key to partition on( or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes The serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);

    /**
     * 关闭分区器时执行相关操作
     */
    public void close();


    /**
     * 通知分区器有一个已创建的新批处理数据。当时用粘性分区器时，次方法会自动为新批处理数据选择一个粘性分区。
     * @param topic The topic name
     * @param cluster The current cluster metadata
     * @param prevPartition The partition previously selected for the record that triggered a new batch
     */
    default public void onNewBatch(String topic, Cluster cluster, int prevPartition) {
    }
}

</code></pre>
<h5 id="拦截器">拦截器</h5>
<p>生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。消费者拦截器主要在消费到消息或在提交消费位移时进行一些定制化的操作。自定义时需要实现org.apache.kafka.clients.producer. ProducerInterceptor</p>
<pre><code>package org.apache.kafka.clients.producer;

import org.apache.kafka.common.Configurable;

/**
 * 一个插件接口，允许拦截（并可能改变）生产者发布到kafka集群中的记记录
*/
public interface ProducerInterceptor&lt;K, V&gt; extends Configurable {
    /**
     * 将消息序列化及分区（如果分区未指定）前对消息进行相关定制化处理。
     * @param record the record from client or the record returned by the previous interceptor in the chain of interceptors.
     * @return producer record to send to topic/partition
     */
    public ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; record);

    /**
     * 当发送到服务器的记录已被确认，或者在发送到服务器之前发送记录失败时调用此方法。
     * 此方法通常在调用用户callback之前调用
     * 此方法通常运行在 Producer 的 background I/O线程中，因此这个方法中的实现代码逻辑越简单越好，否则会影响消息的发送速度。
     * 
     * This method will generally execute in the background I/O thread, so the implementation should be reasonably fast.
     * Otherwise, sending of messages from other threads could be delayed.
     *
     * @param metadata The metadata for the record that was sent (i.e. the partition and offset).
     *                 If an error occurred, metadata will contain only valid topic and maybe
     *                 partition. If partition is not given in ProducerRecord and an error occurs
     *                 before partition gets assigned, then partition will be set to RecordMetadata.NO_PARTITION.
     *                 The metadata may be null if the client passed null record to
     *                 {@link org.apache.kafka.clients.producer.KafkaProducer#send(ProducerRecord)}.
     * @param exception The exception thrown during processing of this record. Null if no error occurred.
     */
    public void onAcknowledgement(RecordMetadata metadata, Exception exception);

    /**
     * 用于关闭拦截器时执行逻辑，例如清理相关资源等
     */
    public void close();
}

</code></pre>
<h5 id="处理顺序">处理顺序</h5>
<p>拦截器-&gt;序列化器-&gt;分区器；KafkaProducer 在将消息序列化和计算分区之前会调用生产者拦截器的 onSend() 方法来对消息进行相应的定制化操作。然后生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka。最后可能会被发往分区器为消息分配分区。</p>
<h3 id="消费者consumer">消费者(Consumer)</h3>
<h4 id="简介-4">简介</h4>
<p>kafka consumer 订阅感兴趣的主题，同时向其所在的主分区发送fetch请求，获取需要进行消费的消息；需要注意的是，此过程中，consumer的每个请求都需要在partition中指定offset，从而消费从offset处开始的message。因此，consumer对offset的控制便尤为重要，可以依此来进行回退重消费操作。</p>
<h4 id="关于offset">关于offset</h4>
<p>大多数消息系统都在 broker 上保存被消费消息的元数据。也就是说，当消息被传递给 consumer，broker 要么立即在本地记录该事件，要么等待 consumer 的确认后再记录。这是一种相当直接的选择，而且事实上对于单机服务器来说，也没其它地方能够存储这些状态信息。</p>
<p>由于大多数消息系统用于存储的数据结构规模都很小，所以这也是一个很实用的选择，因为只要 broker 知道哪些消息被消费了，就可以在本地立即进行删除，一直保持较小的数据量。</p>
<p>然而，此过程中要一直保持broker与consumer的数据一致性不是一件容易的事；例如consumer已消费了数据，但是回执给broker时出现网络错误导致broker没有删除本地记录，这时消息便有可能会重复消费；并且这种记录操作对于broker而言也是一个不小的性能负担（首先对其加锁，确保该消息只被发送一次，然后将其永久的标记为 consumed，以便将其移除）。</p>
<p>为了应对上述问题，Kafka 使用了完全不同的方式来解决消息丢失问题。</p>
<p>首先Kafka 的 topic 被分割成了一组完全有序的 partition，其中每一个 partition 在任意给定的时间内只能被每个订阅了这个 topic 的 consumer group中的一个 consumer 消费。这意味着 partition 中 每一个 consumer 的位置仅仅是一个数字，即下一条要消费的消息的 offset。这使得被消费的消息的状态信息相当少，每个 partition 只需要一个数字。这个状态信息还可以作为周期性的 checkpoint。这以非常低的代价实现了和消息确认机制等同的效果。</p>
<p>这种方式还有一个附加的好处，那就是consumer 可以回退到之前的 offset 来再次消费之前的数据，这个操作违反了队列的基本原则，但事实证明对大多数 consumer 来说这是一个必不可少的特性。 例如，如果 consumer 的代码有 bug，并且在 bug 被发现前已经有一部分数据被消费了，那么 consumer 可以在 bug 修复后通过回退到之前的 offset 来再次消费这些数据。需要注意的是，对于重复消费的消息要保持幂等操作。</p>
<h4 id="push-vs-pull">Push vs Pull</h4>
<p>关于消息的推送拉取，kafka采用的是：Producer push数据到broker，Consumer从broker pull数据。针对消费者而言，无论是pull-based还是push-based都有各自的优缺点。</p>
<p>pull-based：数据传输速率由Consumer控制，可防止服务因大量数据冲击而宕机，同时也简化了broker的设计；且，此模式中，Producer可以达到最大化量产消息。当然，其缺点就是如果 broker 中没有数据，consumer 可能会在一个紧密的循环中结束轮询，实际上却是在忙于等待数据的到达。为了避免 busy-waiting，kafka在 pull 请求中加入参数，使得 consumer 在一个“long pull”中阻塞等待，直到数据到来（还可以选择等待给定字节长度的数据来确保传输长度）。</p>
<p>push-based：消息能最快的被指定消费者所消费，但是相应的broker设计将会更复杂，且数据传输速率过大时，容易冲击消费者服务，导致其宕机无法提供服务。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka安装&使用&配置]]></title>
        <id>https://philosopherzb.github.io/post/kafka-an-zhuang-andshi-yong-andpei-zhi/</id>
        <link href="https://philosopherzb.github.io/post/kafka-an-zhuang-andshi-yong-andpei-zhi/">
        </link>
        <updated>2022-06-04T08:42:31.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka安装，使用以及spring-kafka配置信息。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/lake-6278825_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="win10下部署kafka">Win10下部署kafka</h2>
<p>主要涉及Java，zookeeper，kafka，步骤如下。</p>
<h3 id="java安装">Java安装</h3>
<p>JDK下载：<a href="https://www.oracle.com/java/technologies/javase-downloads.html">点此此处跳转官网下载页面</a></p>
<p>安装过程比较简单，基本都是下一步即可，唯一需要注意的是环境变量的设置；安装完后效果如下图所示（注意：Java安装路径不要太深（目录过多），同时文件夹命名不要存在空格，否则将导致kafka部署失败）</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314001.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314002.png" alt="img" loading="lazy"></figure>
<h3 id="zookeeper安装">zookeeper安装</h3>
<p>下载地址：<a href="https://zookeeper.apache.org/releases.html">点击此处跳转官网下载页面</a></p>
<p>找一个稳定的版本下载即可：</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314003.png" alt="img" loading="lazy"></figure>
<ol>
<li>下载后解压到一个目录：eg: D:\Program\zookeeper\zookeeper-3.4.14</li>
<li>在zookeeper-3.4.14目录下，新建两个文件夹，并命名(eg: data,log)，(路径：D:\Program\zookeeper\zookeeper-3.4.14\data,D:\Program\zookeeper\zookeeper-3.4.14\log)</li>
<li>进入Zookeeper设置目录，eg: D:\Program\zookeeper\zookeeper-3.4.14\conf；复制“zoo_sample.cfg”副本并将副本重命名为“zoo.cfg”；在任意文本编辑器（eg：记事本）中打开zoo.cfg；找到并编辑dataDir=D:\Program\zookeeper\zookeeper-3.4.14\data；dataLogDir=D:\Program\zookeeper\zookeeper-3.4.14\log</li>
<li>进入D:\Program\zookeeper\zookeeper-3.4.14\bin目录，双击zkServer.cmd即可运行zk。</li>
</ol>
<h3 id="kafka安装">kafka安装</h3>
<p>下载地址：<a href="http://kafka.apache.org/downloads.html">点击此处跳转官网下载页面</a></p>
<p>2.8之后kafka支持不依赖zookeeper启动：<a href="https://kafka.apache.org/quickstart">点击此处跳转详细说明页面</a></p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314004.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>下载后解压缩。eg: D:\Program\kafka_2.13-2.7.0</p>
</li>
<li>
<p>建立一个空文件夹 logs. eg: D:\Program\kafka_2.13-2.7.0\logs</p>
</li>
<li>
<p>进入config目录，编辑 server.properties文件(eg: 用“写字板”打开)。；找到并编辑log.dirs=D:\Program\kafka_2.13-2.7.0\logs；找到并编辑zookeeper.connect=localhost:2181。表示本地运行。(Kafka会按照默认，在9092端口上运行，并连接zookeeper的默认端口：2181)</p>
</li>
<li>
<p>运行：命令行下切入D:\Program\kafka_2.13-2.7.0目录，随后执行如下命令即可（注意先开启zookeeper）：.\bin\windows\kafka-server-start.bat .\config\server.properties</p>
</li>
<li>
<p>可能存在的报错：</p>
<p>输入行太长。 命令语法不正确：目录树太深了，减少几个目录即可。</p>
<p>找不到或无法加载主类 Files\java\jdk8\lib;D:\Program：此错误由目录存在空格所导致。解决：找到D:\Program\kafka_2.13-2.7.0\bin\windows，用编辑器（eg：记事本）打开kafka-run-class.bat，查看配置是否有加上双引号</p>
</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314005.png" alt="img" loading="lazy"></figure>
<p>如果加了双引号仍然无法启动，那可以去查看java安装路径中的文件夹是否存在空格，如果存在，将空格去掉即可。</p>
<p>启动图如下：</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314006.png" alt="img" loading="lazy"></figure>
<h2 id="简单kafka命令">简单kafka命令</h2>
<p>部分kafka命令，基于win10（如果非win系统，使用bin目录下的.sh即可）</p>
<h3 id="创建topic两个副本-四个分区">创建topic（两个副本。四个分区）</h3>
<pre><code>kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic testTopic  
$ bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --partitions 3 --replication-factor 3 --topic topic_test( Kafka 版本 &gt;= 2.2 支持此方式（推荐）)  
$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
</code></pre>
<h3 id="查看topic">查看topic</h3>
<p>查看topic列表：kafka-topics.bat --list --zookeeper localhost:2181</p>
<p>查看topic详情：bin/kafka-topics.sh --zookeeper host12:2181  --describe --topic ltopicName</p>
<pre><code>$ bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092
Topic:quickstart-events  PartitionCount:1    ReplicationFactor:1 Configs:
Topic: quickstart-events Partition: 0    Leader: 0   Replicas: 0 Isr: 0
</code></pre>
<p>清除Kafka topic下所有消息：kafka-topics.sh --zookeeper zookeeper地址:端口 --delete --topic topic_name</p>
<h3 id="删除topic">删除topic</h3>
<p>linux：./bin/kafka-topics  --delete --zookeeper 【zookeeper server】  --topic 【topic name】</p>
<p>win：kafka-topics.bat --delete --zookeeper localhost:2181 --topic testTopic</p>
<p>如果kafaka启动时加载的配置文件中server.properties没有配置delete.topic.enable=true，那么此时的删除并不是真正的删除，而是把topic标记为：marked for deletion</p>
<p>彻底删除topic，可以如下操作：</p>
<ol>
<li>删除kafka存储目录（server.properties文件log.dirs配置，默认为&quot;/tmp/kafka-logs&quot;）相关topic目录</li>
<li>登录zookeeper客户端：命令：./bin/zookeeper-client</li>
<li>找到topic所在的目录：ls /brokers/topics</li>
<li>找到要删除的topic，执行命令：rmr /brokers/topics/【topic name】即可，此时topic被彻底删除。</li>
</ol>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314007.png" alt="img" loading="lazy"></figure>
<h3 id="发送消息至topic">发送消息至topic</h3>
<pre><code>$ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
This is my first event
This is my second event
</code></pre>
<h3 id="消费topic消息">消费topic消息</h3>
<p>从头开始查看kafka topic下的数据：kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic_name --from-beginning</p>
<p>按照偏移量查看topic下数据：kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic_name --offset latest --partition 0</p>
<p>--offset设置偏移量 latest代表最后 ，可以设置区间，不设置结尾的话默认为查询到latest(最后)</p>
<p>--partition 设置分区 使用偏移量查询时一定要设置分区才能查询</p>
<pre><code>$ bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
This is my first event
This is my second event
</code></pre>
<h2 id="spring-kafka属性配置">spring-kafka属性配置</h2>
<p>spring已经封装了kafka，所以在springboot项目中可以非常便捷的集成kafka，引入其依赖，如下：</p>
<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;
  &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;
  &lt;version&gt;${last.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>文档地址：<a href="https://docs.spring.io/spring-kafka/docs/current/reference/html/">点击此处跳转官网页面</a></p>
<p>属性配置：<a href="https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html#spring.kafka.admin.client-id">点击此处跳转说明页面</a></p>
<h3 id="producer的配置参数">producer的配置参数</h3>
<pre><code>#procedure要求leader在考虑完成请求之前收到的确认数，用于控制发送记录在服务端的持久化，其值可以为如下：
#acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为-1。
#acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。
#acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1的设置。
#可以设置的值为：all(-1), 0, 1
spring.kafka.producer.acks=1
 
#每当多个记录被发送到同一分区时，生产者将尝试将记录一起批量处理为更少的请求， 
#这有助于提升客户端和服务器上的性能，此配置控制默认批量大小（以字节为单位），默认值为16384
spring.kafka.producer.batch-size=16384
 
#以逗号分隔的主机：端口对列表，用于建立与Kafka集群的初始连接
spring.kafka.producer.bootstrap-servers
 
#生产者可用于缓冲等待发送到服务器的记录的内存总字节数，默认值为33554432
spring.kafka.producer.buffer-memory=33554432
 
#ID在发出请求时传递给服务器，用于服务器端日志记录
spring.kafka.producer.client-id
 
#生产者生成的所有数据的压缩类型，此配置接受标准压缩编解码器（'gzip'，'snappy'，'lz4'），
#它还接受'uncompressed'以及'producer'，分别表示没有压缩以及保留生产者设置的原始压缩编解码器，
#默认值为producer
spring.kafka.producer.compression-type=producer
 
#key的Serializer类，实现类实现了接口org.apache.kafka.common.serialization.Serializer
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
 
#值的Serializer类，实现类实现了接口org.apache.kafka.common.serialization.Serializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
 
#如果该值大于零时，表示启用重试失败的发送次数
spring.kafka.producer.retries
</code></pre>
<h3 id="consumer的配置参数">consumer的配置参数</h3>
<pre><code>#当'enable.auto.commit'为true时，该配置表示消费者offset自动提交给Kafka的频率（以毫秒为单位），默认值为5000。
spring.kafka.consumer.auto-commit-interval;
 
#当Kafka中没有初始偏移量或者服务器上不再存在当前偏移量时该怎么办，默认值为latest，表示自动将偏移重置为最新的偏移量
#可选的值为latest, earliest, none
spring.kafka.consumer.auto-offset-reset=latest;
 
#以逗号分隔的主机：端口对列表，用于建立与Kafka群集的初始连接。
spring.kafka.consumer.bootstrap-servers;
 
#ID在发出请求时传递给服务器;用于服务器端日志记录。
spring.kafka.consumer.client-id;
 
#如果为true，则消费者的偏移量将在后台定期提交，默认值为true(2.3后默认为false)
spring.kafka.consumer.enable-auto-commit=true;
 
#如果没有足够的数据立即满足“fetch.min.bytes”给出的要求，服务器在回答获取请求之前将阻塞的最长时间（以毫秒为单位）
#默认值为500
spring.kafka.consumer.fetch-max-wait;
 
#服务器应以字节为单位返回获取请求的最小数据量，默认值为1，对应的kafka的参数为fetch.min.bytes。
spring.kafka.consumer.fetch-min-size;
 
#用于标识此使用者所属的使用者组的唯一字符串。
spring.kafka.consumer.group-id;
 
#心跳与消费者协调员之间的预期时间（以毫秒为单位），默认值为3000
spring.kafka.consumer.heartbeat-interval;
 
#密钥的反序列化器类，实现类实现了接口org.apache.kafka.common.serialization.Deserializer
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
 
#值的反序列化器类，实现类实现了接口org.apache.kafka.common.serialization.Deserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
 
#一次调用poll()操作时返回的最大记录数，默认值为500
spring.kafka.consumer.max-poll-records;
</code></pre>
<h3 id="listener的配置参数">listener的配置参数</h3>
<pre><code>#侦听器的AckMode
#当enable.auto.commit的值设置为false时，该值会生效；为true时不会生效
spring.kafka.listener.ack-mode;
 
#在侦听器容器中运行的线程数
spring.kafka.listener.concurrency;
 
#轮询消费者时使用的超时（以毫秒为单位）
spring.kafka.listener.poll-timeout;
 
#当ackMode为“COUNT”或“COUNT_TIME”时，偏移提交之间的记录数
spring.kafka.listener.ack-count;
 
#当ackMode为“TIME”或“COUNT_TIME”时，偏移提交之间的时间（以毫秒为单位）
spring.kafka.listener.ack-time;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[敏感数据加密方案]]></title>
        <id>https://philosopherzb.github.io/post/min-gan-shu-ju-jia-mi-fang-an/</id>
        <link href="https://philosopherzb.github.io/post/min-gan-shu-ju-jia-mi-fang-an/">
        </link>
        <updated>2022-05-31T07:43:18.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述一种基于云服务的敏感数据加密设计方案。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/sunset-1117008_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="引言">引言</h2>
<h3 id="需求描述">需求描述</h3>
<p>为了防止敏感数据泄露，造成资产损失，故需要对相关数据进行加密处理；同时，允许部分加密数据支持模糊搜索。</p>
<h2 id="详细设计">详细设计</h2>
<h3 id="总体设计">总体设计</h3>
<p>加密服务提供针对敏感数据存储的加密能力，用于防止因外部或内部安全威胁所导致的数据泄露问题，从而提高数据安全的防护水平。</p>
<p>基本功能：</p>
<ul>
<li>基础安全保障：加解密从根本上夯实了数据安全性。对敏感字段加密后，可以有效防止数据库内容被直接盗取。且密钥以租户维度隔离，有效解决应用的水平权限隔离问题。</li>
<li>数据库内容和密钥存储管理分离：接入方只存储加密数据，不保存密钥。只需接入本产品提供的SDK即可（实现细节将由SDK处理）。这在增强安全系数的同时，也简化了开发者管理、存储密钥的成本。</li>
<li>安全与服务平滑性兼得：SDK提供智能的、丰富的api可以自动识别数据库中存量密文的版本、自动加密、解密。接入方引入SDK后，可以做到在不停服务的条件下进行密钥升级。</li>
<li>接入简单便捷：加解密方案不依赖硬件，只需要引入加解密SDK即可，对数据库无特殊要求，使用成本、改造成本低。</li>
<li>保证数据完整性传输：所有接口都将使用非对称加密RSA进行签名校验，保证了数据的完整性。</li>
</ul>
<h3 id="整体业务流程">整体业务流程</h3>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310009.png" alt="img" loading="lazy"></figure>
<h4 id="接入方使用">接入方使用</h4>
<p>引入加解密SDK，初始化client后，直接调用对应的函数即可。</p>
<h4 id="加解密云服务">加解密云服务</h4>
<p>1、允许用户输入应用级别的RSA公私钥，应用公私钥功能：应用使用私钥加签数据，云服务使用公钥验签数据。此值支持变更。</p>
<p>2、自动生成云服务RSA公私钥，云服务公私钥功能：云服务使用私钥加签数据，应用使用公钥验签数据。此值不支持变更。</p>
<p>3、自动生成AES密钥（外部获取此值时，将会被RSA加密），此密钥不展示。此值支持变更。</p>
<p>4、自动生成伪随机码（日期+雪花算法生成），用于验伪操作，客户端需持有此参数并传递。此值支持变更。</p>
<p>5、自动生成appid，用于唯一标识一个应用（同一个客户允许存在多个appid），云服务前缀+雪花算法生成。</p>
<p>6、appId加version为组合唯一索引；其中version默认为0，每次变更AES密钥时，version+1。</p>
<p>7、允许设置过期时间（默认90天），最大有效期（默认120天，必须大于过期时间）；单位：天。主要用于客户端本地缓存的有效时长。</p>
<p>8、允许设置滑动窗口（默认4）大小及压缩长度（默认3）；用于加密search类型的数据。</p>
<h3 id="整体技术方案">整体技术方案</h3>
<h4 id="关于算法">关于算法</h4>
<p>对称加密与非对称加密混合使用：</p>
<ol>
<li>云服务自动生成出一对秘钥pub/pri。将私钥保密，将公钥公开。</li>
<li>接入方请求云服务时，拿到云服务的公钥pub。</li>
<li>云服务通过AES计算出一个对称加密的秘钥X。 然后使用应用公钥将X进行加密。</li>
<li>接入方发送加签请求到云服务获取加密后的秘钥X，云服务对请求进行验签，通过了方才返回秘钥X。</li>
<li>接入方得到加密后的秘钥X，便可以使用应用私钥解密，得到AES秘钥。</li>
<li>之后便可以使用AES秘钥进行敏感数据的加解密操作。</li>
</ol>
<p>注意：接入方不需要额外的开发，只需引入云服务SDK，并调用对应的接口即可。</p>
<h5 id="rsa">RSA</h5>
<p>将用于签名（保证数据完整性）及加密（保证数据安全性）</p>
<table>
<thead>
<tr>
<th>开放平台签名算法名称</th>
<th>标准签名算法名称</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>RSA2</td>
<td>SHA256WithRSA</td>
<td>强制要求 RSA 密钥的长度至少为 2048</td>
</tr>
<tr>
<td>RSA</td>
<td>SHA1WithRSA</td>
<td>对 RSA 密钥的长度不限制，推荐使用 2048 位以上</td>
</tr>
</tbody>
</table>
<p>由于计算能力的飞速发展，从安全性角度考虑，云服务推荐使用 SHA256WithRSA 的签名算法。该算法在摘要算法上比 SHA1WithRSA 有更强的安全能力。</p>
<p>注意避免公私钥混用：不同签名算法的签名密钥是隔离的。由于同时提供了两套签名算法，若选择了特定的签名算法，请保证使用对应的私钥签名，同时使用对应的云服务公钥进行验签。</p>
<h5 id="aes">AES</h5>
<p>将用于加解密敏感数据，此数据传输时，会被RSA加密保护，算法特性如下：</p>
<ol>
<li>加密算法使用“AES/CBC/PKCS5Padding”</li>
<li>加密用的初始向量会直接编码到加密数据中, 因此相同数据内容多次加密的结果不同</li>
<li>不同应用使用不同的秘钥，不同应用间加密数据无法解密（以appId作为区分）</li>
<li>支持秘钥升级, 密文中会包含加密密文用的秘钥版本，更新后支持老版本密文解密（appId + version 获取唯一AES秘钥）</li>
<li>传输过程AES秘钥将会被应用RSA公钥加密，返回时需要使用应用RSA私钥解密。</li>
</ol>
<h4 id="加密类型">加密类型</h4>
<table>
<thead>
<tr>
<th>类型</th>
<th>phone</th>
<th>id</th>
<th>simple</th>
<th>search</th>
</tr>
</thead>
<tbody>
<tr>
<td>描述</td>
<td>手机号</td>
<td>有规律的数字，例如身份证</td>
<td>普通文本类型</td>
<td>支持模糊搜索的文本类型</td>
</tr>
</tbody>
</table>
<p>1、phone，id为有规律的数字, 其加密算法只支持使用尾号后4位搜索。如13912345678的手机号加密后可用5678搜索。</p>
<p>2、接入方如果存在phone，id类的加密类型，建议增加一列检索串（创建索引），用于匹配搜索（List<code>&lt;DO&gt;</code> objects =  SELECT * FROM table WHERE phone=‘encryptedData’）；如果不想增加额外的检索串，也可以在原有的加密手机号字段上建立前缀索引，可缩短一定的模糊匹配查询时间；当然也可以在加密手机号字段上直接建立普通索引，因为加密后的索引串会放在整个加密字符的最前方（List<code>&lt;DO&gt;</code> objects =  SELECT * FROM table WHERE phone like ‘encryptedData%’）。关于如何截取检索串可在附录中查看。</p>
<p>3、simple类型只进行加密，不支持模糊搜索。</p>
<p>4、search类型支持模糊搜索，基本实现原理是根据4位英文字符（半角），2个中文字符（全角）为一个检索条件。将一个字段拆分为多个。</p>
<p>比如：test123，使用4个字符为一组的加密方式。切割结果为：[test, est1, st12, t123]，第一组 test，第二组est1，第三组st12，第四组 t123… 依次类推，如果需要检索 所有包含 检索条件4个字符的数据 比如：test，加密字符后通过key like “%partial%” 查库。</p>
<p>因为密文检索开启后 密文长度会膨胀几倍以上，如果没有强需求建议不开启。</p>
<p>使用这种方式存在一定的代价：</p>
<ul>
<li>支持模糊查询加密方式，产出的密文比较长；</li>
<li>支持的模糊查询子句长度必须大于等于4个英文/数字，或者2个汉字。不支持过短的查询(出于安全考虑)；</li>
<li>如果数据本身长度不够4个数字或2个汉字, 则此时输入全文即可搜索.  如: 原始数据为&quot;安&quot;的, 使用&quot;安&quot;调用平台搜索接口即可获取搜索用文本；</li>
<li>返回的结果列表中有可能有多余的结果，需要增加筛选的逻辑：对记录先解密，再筛选；</li>
</ul>
<h4 id="加密格式">加密格式</h4>
<p>注意：其中 Index 是检索信息，同一份数据多次加密后的结果都不会变。</p>
<p>手机号(phone)和身份证(id)格式，其中手机号(phone)分隔符：SEP=$；身份证(id)分隔符：SEP=#</p>
<table>
<thead>
<tr>
<th>SEP</th>
<th>Index</th>
<th>SEP</th>
<th>EncryptedData</th>
<th>SEP</th>
<th>Version</th>
<th>SEP</th>
<th>SEP</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>search类文本，SEP=~</p>
<table>
<thead>
<tr>
<th>SEP</th>
<th>EncryptedData</th>
<th>SEP</th>
<th>Index</th>
<th>SEP</th>
<th>Version</th>
<th>SEP</th>
<th>SEP</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>simple类文本，SEP=~</p>
<table>
<thead>
<tr>
<th>SEP</th>
<th>EncryptedData</th>
<th>SEP</th>
<th>Version</th>
<th>SEP</th>
</tr>
</thead>
<tbody></tbody>
</table>
<h4 id="加密样例">加密样例</h4>
<p>加密算法支持模糊搜索(可选)，模糊加密的数据比一般加密更长一些，大概长度扩大 10-15 倍。</p>
<p>加密数据的示例如下：</p>
<p>phone(手机号)加密方式，加密类型：phone</p>
<table>
<thead>
<tr>
<th>手机号</th>
<th>13012345678</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>$mMHmK1rfa+OZ23eYhO1AUQ==$q6cFTUyW6SuJD2NReTUVBQ==$1$$</td>
</tr>
<tr>
<td>后四位手机号</td>
<td>5678</td>
</tr>
<tr>
<td>5678检索串</td>
<td>mMHmK1rfa+OZ23eYhO1AUQ==</td>
</tr>
</tbody>
</table>
<p>身份证加密方式，加密类型：id</p>
<table>
<thead>
<tr>
<th>身份证</th>
<th>200300190002039898</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>#uJfxXOF0EJ+6V8H51L9rdg==#ao/gDujB17PTObfSjeAOvl/YxmScgYuxQPl2wokwlec=#1##</td>
</tr>
<tr>
<td>后四位身份证</td>
<td>9898</td>
</tr>
<tr>
<td>9898检索串</td>
<td>uJfxXOF0EJ+6V8H51L9rdg==</td>
</tr>
</tbody>
</table>
<p>普通文本加密方式，加密类型：simple</p>
<table>
<thead>
<tr>
<th>地址</th>
<th>浙江省杭州市滨江区星耀城</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>~thzORLtQtSKYQZW2UFvM64bMN5NhDPb8PSRZ5KNLtl3ZJ+sIYfzMN5gxPy7+ba4o~1~</td>
</tr>
</tbody>
</table>
<p>支持搜索的文本加密方式，加密类型：search</p>
<table>
<thead>
<tr>
<th>域名</th>
<th>www.test.hostname.com</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>~J/7ro/3scxjvrPXFB5+OxxOZdv45iqXbkP4doNfcEAo=~Q9bpt3Na0G8oOwOe1i9y+Eanjg7Tfr5U3vcYuMyC2iZi7ciYfvC+bNP6pS29uUPpo4L3spYx~1~~</td>
</tr>
<tr>
<td>需要查询的字符</td>
<td>test.hostname</td>
</tr>
<tr>
<td>test.hostname检索串</td>
<td>1i9y+Eanjg7Tfr5U3vcYuMyC2iZi7ciYfvC+bNP6</td>
</tr>
</tbody>
</table>
<h4 id="加密长度">加密长度</h4>
<p>加密后由于增加了密文信息, 将比原文要长, 各种常见字段加密后的数据长度参考表如下:</p>
<p>注: 当发生加密秘钥version字段变更时, 密文长度可能会发生变化. 建议设计字段时在表中字段上加10个左右字符。</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>密文长度（支持搜索）</th>
<th>密文长度（不支持搜索）（使用simple类型加密）</th>
</tr>
</thead>
<tbody>
<tr>
<td>phone（手机号）</td>
<td>54</td>
<td>28</td>
</tr>
<tr>
<td>id（身份证为例）</td>
<td>74</td>
<td>48</td>
</tr>
<tr>
<td>文本数据（5字符，全汉字）</td>
<td>46</td>
<td>28</td>
</tr>
<tr>
<td>文本数据（10字符，全汉字）</td>
<td>86</td>
<td>48</td>
</tr>
<tr>
<td>文本数据（20字符，全汉字）</td>
<td>170</td>
<td>92</td>
</tr>
<tr>
<td>文本数据（40字符，全汉字）</td>
<td>334</td>
<td>176</td>
</tr>
</tbody>
</table>
<h2 id="数据库设计">数据库设计</h2>
<h3 id="数据库ddl">数据库DDL</h3>
<pre><code>-- DROP TABLE IF EXISTS `tb_app_config_aes`;
CREATE TABLE `tb_app_config_aes`
(
  `id`             bigint(20)  NOT NULL AUTO_INCREMENT COMMENT '自增主键',
  `app_id`         varchar(64) NOT NULL COMMENT '应用id，用于唯一标识应用；生成规则：云服务前缀+雪花算法生成',
  `aes_key`        char(24)    NOT NULL COMMENT 'aes秘钥,固定长度',
  `secret_version` bigint(16)  NOT NULL COMMENT '版本号，用于升级aesKey',
  `is_delete`      tinyint(1)  NOT NULL DEFAULT 0 COMMENT '删除标识，0-未删除，1-已删除',
  `create_time`    datetime    NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `modify_time`    datetime    NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`),
  unique key `unique_index_app_id_secret_version` (`app_id`, `secret_version`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8 COMMENT ='AES秘钥配置表';
  
-- DROP TABLE IF EXISTS `tb_app_config_rsa`;
CREATE TABLE `tb_app_config_rsa`
(
  `id`                         bigint(20)    NOT NULL AUTO_INCREMENT COMMENT '自增主键',
  `app_id`                     varchar(64)   NOT NULL COMMENT '应用id，用于唯一标识应用；生成规则：云服务前缀+雪花算法生成',
  `tenant_id`                  varchar(128)  NOT NULL COMMENT '租户id',
  `tenant_name`                varchar(255)  NOT NULL COMMENT '租户名称',
  `app_pub_key`                varchar(512)  NOT NULL COMMENT '应用公钥，用于验签接口，同时加密AES秘钥',
  `das_pub_key`                varchar(512)  NOT NULL COMMENT '云服务公钥，用于接入方验签',
  `das_pri_key`                varchar(2048) NOT NULL COMMENT '云服务私钥，云服务使用此私钥进行加签',
  `encrypt_slide_size`         int(3)        NOT NULL DEFAULT 4 COMMENT '滑动窗口大小',
  `encrypt_index_compress_len` int(3)        NOT NULL DEFAULT 3 COMMENT '密文滑窗压缩长度',
  `random_num`                 varchar(64)   NOT NULL COMMENT '伪随机码，生成规则：日期+雪花算法生成',
  `invalid_time`               int(4)        NOT NULL DEFAULT 90 COMMENT '过期时间，单位：天，默认90天',
  `max_invalid_time`           int(4)        NOT NULL DEFAULT 120 COMMENT '最大有效期，单位：天，默认120天；必须大于invalidTime',
  `is_delete`                  tinyint(1)    NOT NULL DEFAULT 0 COMMENT '删除标识，0-未删除，1-已删除',
  `create_time`                datetime      NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `modify_time`                datetime      NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`),
  unique key `unique_index_app_id` (`app_id`),
  unique key `unique_index_tenant_id` (`tenant_id`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8 COMMENT ='RSA秘钥配置表';

</code></pre>
<h2 id="附录">附录</h2>
<h3 id="rsa公私钥生成代码-java版">RSA公私钥生成代码-java版</h3>
<pre><code>import java.security.KeyPair;
import java.security.KeyPairGenerator;
import java.security.NoSuchAlgorithmException;
import java.security.PrivateKey;
import java.security.PublicKey;
import java.security.SecureRandom;
import java.util.Base64;
 
/**
 * @author philosopherZB
 * @date 2022/1/13
 */
public class GenerateKeyUtils {
    private static KeyPair KEY_PAIR_INSTANCE = null;
    private static String ALGORITHM = &quot;RSA&quot;;
    private static Integer KEY_LENGTH = 2048;
 
    /**
     * 生成默认公钥--algorithm=RSA; keyLength=2048
     *
     * @return publicKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genDefaultPublicKey() throws NoSuchAlgorithmException {
        return genPublicKey(ALGORITHM, KEY_LENGTH);
 
    }
 
    /**
     * 生成默认私钥--algorithm=RSA; keyLength=2048
     *
     * @return privateKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genDefaultPrivateKey() throws NoSuchAlgorithmException {
        return genPrivateKey(ALGORITHM, KEY_LENGTH);
    }
 
    /**
     * @param algorithm 标准的算法名: https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html
     * @param keyLength 指定长度
     * @return publicKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genPublicKey(String algorithm, int keyLength) throws NoSuchAlgorithmException {
        PublicKey publicKey = getKeyPairInstance(algorithm, keyLength).getPublic();
        return Base64.getEncoder().encodeToString(publicKey.getEncoded());
 
    }
 
    /**
     * @param algorithm 标准的算法名: https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html
     * @param keyLength 指定长度
     * @return privateKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genPrivateKey(String algorithm, int keyLength) throws NoSuchAlgorithmException {
        PrivateKey privateKey = getKeyPairInstance(algorithm, keyLength).getPrivate();
        return Base64.getEncoder().encodeToString(privateKey.getEncoded());
    }
 
    /**
     * getKeyPairInstance
     *
     * @param algorithm 标准的算法名: https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html
     * @param keyLength 指定长度
     * @return KeyPair
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    private static KeyPair getKeyPairInstance(String algorithm, int keyLength) throws NoSuchAlgorithmException {
        if (KEY_PAIR_INSTANCE == null) {
            KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(algorithm);
            SecureRandom secureRandom = new SecureRandom();
            keyPairGenerator.initialize(keyLength, secureRandom);
            KEY_PAIR_INSTANCE = keyPairGenerator.generateKeyPair();
        }
 
        return KEY_PAIR_INSTANCE;
    }
 
    public static void main(String[] args) throws NoSuchAlgorithmException {
        System.out.println(&quot;publicKey: &quot; + genPublicKey(&quot;RSA&quot;, 2048));
        System.out.println(&quot;privateKey: &quot; + genPrivateKey(&quot;RSA&quot;, 2048));
 
        System.out.println(&quot;default publicKey: &quot; + genDefaultPublicKey());
        System.out.println(&quot;default privateKey: &quot; + genDefaultPrivateKey());
    }
}

</code></pre>
<h3 id="检索串提取代码示例-java版">检索串提取代码示例-java版</h3>
<pre><code>public static String extractIndex(String encryptedData) {
    if (encryptedData == null || encryptedData.length() &lt; 4) {
        return null;
    }
    char sepInData = encryptedData.charAt(0);
    if (encryptedData.charAt(encryptedData.length() - 2) != sepInData) {
        return null;
    }
    String[] parts = StringUtils.split(encryptedData, sepInData);
    if (sepInData == '$' || sepInData == '#') {
        return parts[0];
    } else {
        return parts[1];
    }
}

</code></pre>
<h3 id="sdk使用样例-java版">SDK使用样例-java版</h3>
<pre><code>public static void main(String[] args) {
    String privateKey = &quot;MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDGbO1R5OQ0Ff27V+k1FuPBnKMqwC3c0HFnAFu0ZUWH5FTyfkMahhsRntg7/uLk2QM1v0rx7kEdd/ExhOs5z68nO8xbyX/YbC+DX/NH3IsvNRrDU+xfpZdnuMT3bjtlGsasTIOK8S3DlHfiGpO4HPgeY7mBfduBaPStmJFLMM4Xpg3piD4r8mmq+2dAIhq6vX8GpwVqap0XLk8TcYY5h8WQ0FTbTSbRUNN/+YHmlwbEJVa+NR8qaUoo75/WHeVgjNlZ8SAfdjMt1oVWmibSiKYDr1JBVZrPD4CjBy6UR9jDrTxTrwmGsvHUzE3ZThPJZvJEWVXHbMCTs4KReUds/UcPAgMBAAECggEAXSsAM5e53wsEXFbm1VquDlax9nzODASDes3cQZPblfcMO+A1OdsGErv25BTGDJYo/6+WTQqF4IRU5991Y2u03kMhrWdrc/84QANpg7B2WfAhZN2e+zoRYU5MjbFgihSMfJJgoXik+FRaBfxcp/JSPlKs47RowNa7LFeawSdlXYxy+eXCouYomGe0aCPACKfHRyBWDRCf+eK/UwXRcmiyHi8hXOa/IfsaBsZNLtHR3rFod4x+hZUoIJAuXpupc2KW4qsK8ImW9BDcx2p2EO6iggw6MZMZkIo7NRHf3aaaLfpVsebjHm5lBF95ptVUP4CFpfoNevn3y09D4nhGjBP7AQKBgQDnZIcX+XJDykYES10jClxKndibhLYN9zNp7FOiin1Sfd5tUux+VXNEocWSMIqz8wuuI6kyrigwa0E2Kis3QANLizsaOu39jCW5kAHFm19txG3IgntPix95qADl9fQAO1Y3Nbh+zL43FH8f0s066L3V9bTvmZ6rN9vf8GF/xHrfLwKBgQDbhuUop370RhFbgN1CIFM6m6gio7pft7NW6YLyn4yXa/vbpw7c7B+c5TA1ru9maaXlF+Sff5WtLVK10czghDOdI0B5qrm62+TEhOEk7C6dgNsCdaHuDzqA2MYEmq4HJhoXrwM+xvzrlCwNZQ1AIRQWKdceKK9YgEnZQXQU9TEeIQKBgQCfPELrcLH9jLlaQzK45mxUvQNPIqjWO4OaJRP5CyzrE8t5mFM/LTbByEHaNKV+6IblM41AXzExAN5DlAlhYB/kYNAvYNZeYY+kf0F4509ojoCuN3z8ZFUot0DG/9cGQc829zUbrXJJHUXOdJbfL0NUdl4pdKIIWcxp81ZlQqT76QKBgHMlCzfKux1XTy1mpydTGzSnhoY8yLoB+dBBhQzLwQt/eUhaFMKuG1rJIANYcXuPOJO0d5dtbU27cyGpHMQ6s3PdlKj8cpTfV9v4MruSIlU8zCM7Hidm13HTwfGSTGu1gYQgqRwZdXn/ayfPdCbJ8uY5JftMrcRG7fVFjqSbgxrhAoGAVPNPW/QOVarHZDCVJi0XtOhl1g6oR51FGkOZ13SEuu8271v0PxcP55Hib6WvCPDOFB75oPaa1HmQs/WL41RCFMlELH/DhwEEkd2hLFmou4AiiB+M2DQODxagyINOx0RGd7FZuhiIWQTocqXj2PV71gkn5q4pIGKLsk1aklLTKXY=&quot;;
    String dasPubKey = &quot;MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAiPNXFw6tQyfpNi63OoR5E+VR+KS4Sy1+1EFNiHTIc/f5AXXZtmo8CVTgAM8X7a2GHLMwRqjH1JZ/2Da9HHn8zHdEtSAbObxMtTKPrnQOG5NrTuA3hfRb/4N00iZZ2KZMr5fTXJ4824VMr2/fQZySwDd0bOPTmrNnlHLu6ErFvfJwjQqbhWVC1VhRkGzvT81O2SM+ALuTnbgoFGqFyaUE9YUP57COA/Hw4Yz+GmQkHxs9ELPvikFSGdBdptDvHQ2dTprskRW8UU/v0XjVED8jeayiqKxJFn2Yejq43eqkH+SV6c9R1jE39qhMyEX7hVvzSMcyvONSo5Za4R5zLqap4wIDAQAZ&quot;;
    // 初始化一次即可（单例模式）
    SecurityClient securityClient =
            new DefaultSecurityClient(&quot;http://127.0.0.1:8999&quot;, &quot;cloud20220209651316091208540160&quot;, privateKey, &quot;20220209651322386368110592&quot;, dasPubKey);
    try {
        // 加密类型: TYPE_PHONE-手机号; TYPE_ID-id; TYPE_SIMPLE-普通加密；TYPE_SEARCH-支持搜索的加密
        // version 为AES版本号，可从云服务中查看
        String encryptData = securityClient.encrypt(&quot;www.test.hostname.com&quot;, SecurityConstants.TYPE_SEARCH, 1L);
        System.out.println(&quot;encryptData: &quot; + encryptData);

        boolean isEncryptData = securityClient.isEncryptData(encryptData, SecurityConstants.TYPE_SEARCH);
        System.out.println(&quot;isEncryptData: &quot; + isEncryptData);

        String decryptData = securityClient.decrypt(encryptData, SecurityConstants.TYPE_SEARCH, 1L);
        System.out.println(&quot;decryptData: &quot; + decryptData);

        String searchData = securityClient.search(&quot;test.hostname&quot;, SecurityConstants.TYPE_SEARCH, 1L);
        System.out.println(&quot;searchData: &quot; + searchData);
    } catch (DasSecurityException e) {
        e.printStackTrace();
    }
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[分布式事务详解]]></title>
        <id>https://philosopherzb.github.io/post/fen-bu-shi-shi-wu-xiang-jie/</id>
        <link href="https://philosopherzb.github.io/post/fen-bu-shi-shi-wu-xiang-jie/">
        </link>
        <updated>2022-05-21T06:09:37.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述单事务到分布式事务的转变以及对应分布式事务解决方案。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/iceland-1768744_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="事务演变">事务演变</h2>
<h3 id="概要">概要</h3>
<p>随着业务的不断发展以及业务复杂度的提升，传统单体服务逐渐暴露出了一些问题，如开发效率低、可维护性差、架构扩展难、部署不灵活等。因此，走上分布式微服务是一条必然的道路。</p>
<p>在分布式中，每个服务便是一个独立的进程，各个服务间可单独迭代，互不影响，甚至可以用不同的语言开发，只需事先定义好对接规范即可。</p>
<p>分布式很好地解决了单体服务的一些问题，但随之而来的便是一系列分布式难题。本篇将要讲述的便是分布式事务这个问题。</p>
<h3 id="本地事务">本地事务</h3>
<p>本地事务，一般也被称之为数据库事务。它主要功能是将多条SQL语句当做一个整体来处理，保证这一个整体要么执行成功，要么执行失败。</p>
<p>比较典型的例子就是转账服务了：A转账100给B，那么对应有A-100和B+100两条语句，事务必须保证这两条语句的整体性，否则就会出现A-100，但B没有增加100的场景。</p>
<p>上述例子体现了事务的一个重要特性：原子性。事实上，事务具备四个基本特性：原子性，一致性，隔离性，永久性；通常也简称为ACID特性。</p>
<ul>
<li>Atomicity(原子性)：一个事务便是一个整体，是不可再被分隔的最小独立单元；在一个事务中的操作，要么同时成功，要么同时失败，不会存在部分成功，部分失败的场景。</li>
<li>Consistency(一致性)：在事务开始前以及结束后，数据库的完整性没有被破坏。完整性包括约束、级联、触发器及其任意组合。</li>
<li>Isolation(隔离性)：数据库允许事务并发执行读写，为了防止出现多个事务并发写导致数据不一致，便有了隔离性。</li>
<li>Durability(持久性)：事务提交后，数据将会被永久地保存，即使系统故障也不会丢失。</li>
</ul>
<p>隔离级别：</p>
<table>
<thead>
<tr>
<th>Isolation Level</th>
<th>Dirty Reads</th>
<th>Non-Repeatable Reads</th>
<th>Phantom Reads</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read uncommitted</td>
<td>允许</td>
<td>允许</td>
<td>允许</td>
</tr>
<tr>
<td>Read committed(Sql server, Oracle)</td>
<td>不允许</td>
<td>允许</td>
<td>允许</td>
</tr>
<tr>
<td>Repeatable reads(Mysql)</td>
<td>不允许</td>
<td>不允许</td>
<td>允许</td>
</tr>
<tr>
<td>Serializable</td>
<td>不允许</td>
<td>不允许</td>
<td>不允许</td>
</tr>
</tbody>
</table>
<ul>
<li>Dirty reads：A事务可以读到B事务还未提交的数据。</li>
<li>Non-repeatable read：A事务读取一行数据，B事务后续修改了这行数据，A事务再次读取这行数据，结果得到的数据不同。</li>
<li>Phantom reads：A事务通过SELECT ... WHERE得到一些行，B事务插入新行或者删除已有的行使得这些行满足A事务的WHERE条件，A事务再次SELECT ... WHERE结果比上一次多/少了一些行。</li>
</ul>
<p>注意1：mysql默认使用RR隔离级别，这是由于binlog的格式问题（statement-记录修改的SQL语句,row-记录每行实际数据的变更,mixed-前面两种混合）所导致的，在5.0之前binlog只有statement一种格式，而主从复制时，这会导致数据的不一致。</p>
<p>注意2：其他数据库选用RC隔离级别，是由于RR隔离级别增加了间隙锁，会增加发生死锁的概率；同时，条件列未命中索引时，会锁全表，RC只会锁行。</p>
<p>注意3：RC隔离级别下，主从复制需要采用binlog的row格式，基于行的复制，这样不会出现主从不一致问题。</p>
<h3 id="分布式事务">分布式事务</h3>
<p>随着业务量的增加，单体服务所能承载的数据量越来越多，系统的运行速度逐渐下降，这时候便需要进行服务独立化（微服务）。</p>
<p>比如跨行转账，或者本行跨服务转账等都是比较典型的分布式场景；这里的每个服务都有自己独立的数据库，为了避免服务不可用，网络连接异常等情造成的数据不一致，分布式事务便应运而生了。</p>
<p>分布式事务的本质是为了多服务之间事务的正确执行，是对本地事务的一个扩展，但它只遵循部分ACID规范。所以，有必要介绍下分布式事务中的CAP定理与BASE理论。</p>
<h4 id="cap定理">CAP定理</h4>
<p>在理论计算机科学中，CAP定理(CAP theorem)，又被称为<a href="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/">布鲁尔定理(Brewer's theorem)</a>。</p>
<ul>
<li>Consistency(一致性)：此处指的是强一致性；它要求分布式系统中一个服务的写操作成功后，其他服务的读操作都是刚刚写入的数据（最新数据）。</li>
<li>Availability(可用性)：非故障服务节点收到请求后必须给予响应（非最新数据）。</li>
<li>Partition-tolerance(分区容错性)：分布式系统在遇到任何网络分区故障时，仍然可以对外提供服务，除非整个分布式系统宕机。（ Gilbert and Lync describe partitions: the network will be allowed to lose arbitrarily many messages sent from one node to another）</li>
</ul>
<p>网络分区：在分布式系统中，不同的节点分布在不同的子网络中，由于一些特殊的原因，这些子节点之间出现了网络不通的状态，但他们的内部子网络是正常的。从而导致了整个系统的环境被切分成了若干个孤立的区域，这就是分区。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310001.png" alt="img" loading="lazy"></figure>
<h4 id="cap定理验证">CAP定理验证</h4>
<p>事实上，在分布式系统中，CAP三者是无法同时存在的，且由于分布式的缘故，分区容灾是必须存在的。</p>
<p>为了验证CAP定理，假设网络中存在两个节点N1和N2，它们之间网络互通，其中N1上存在服务A和独立数据库D1，N2上存在服务B和独立数据库D2。此时，A和B是分布式系统中的一部分，同时对外提供相关服务。</p>
<ul>
<li>一致性要求A，B服务中的数据是一致的，即D1=D2。</li>
<li>可用性要求不管是请求A还是B都可以得到确认的响应。</li>
<li>分区容灾则是指出现网络故障或其他异常场景时，都不会影响A，B之间的正常运行，除非服务本身宕机。</li>
</ul>
<p>在分布式系统中，网络通信时最常见的，就比如上述N1与N2之间；现在假设N1与N2因网络故障而无法通信，为了支持这种网络异常，即要满足分区容灾，那么如果还要同时满足一致性与可用性是否可行了？</p>
<p>可以简单模拟一下：当N1与N2断开网络时，一个请求抵达服务A，更新了D1上的一条数据；此时为了一致性要求，需要将该条更新数据同步至服务B上的数据库D2，但由于网络故障，两个服务无法通信，此时便面临两种选择。</p>
<ul>
<li>牺牲一致性，保证可用性；当下次请求服务B时响应旧的数据。</li>
<li>牺牲可用性，保证一致性；阻塞等待网络恢复，将数据更新至服务B。</li>
</ul>
<h4 id="cap抉择">CAP抉择</h4>
<p>CAP三者究竟该如何选择，才能满足业务需求，这是每个分布式系统架构中都需要考虑。</p>
<ul>
<li>CA(一致性与可用性)：如果选择CA，那么就需要一种非常严格的强一致性协议来进行保障；且，不允许出现网络故障或节点错误，否则整个系统将会不可用。显然，CA不是很适合分布式系统。</li>
<li>CP(一致性与分区容灾)：CP保证了数据的一致性，但由于P的存在，某些时候一个请求将会被无限延长。</li>
<li>AP(可用性与分区容灾)：AP保证了可用性，但却丢失了数据的一致性，在某些场景（银行转账服务）中是难以忍受的。</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310002.png" alt="img" loading="lazy"></figure>
<h4 id="base理论">BASE理论</h4>
<p>由上述CAP定理中可以知道，分布式系统是无法同时满足三者的，只能从中取二。由于分布式系统中网络环境的不可信，分区容灾是必选的；其次可用性保证了用户体验，除开一些需要强一致性的场景(支付，转账等)，应该先选；但如果没有一致性，分布式系统也就失去了存在的意义。</p>
<p>针对此种场景，可以对CAP中的强一致性做一定的让步，有些时候，我们只关注数据的最终一致性即可。于是，BASE理论便诞生了。</p>
<ul>
<li>Basically Available(基本可用)：相对高可用而言，基本可用要求系统即使出现了重大故障，仍然能够提供一些基本型的服务。例如响应时间上的损失以及功能上的损失(熔断，降级)。</li>
<li>Soft state(软状态)：软状态是相对原子性(硬状态，即多个节点间的数据完全一致)而言的。该状态允许数据存在中间状态，并认为该状态并不影响系统的整体可用性，即允许系统在多个不同节点的数据同步存在一定的延时。（注意：对于软状态,我们允许中间状态存在，但不可能一直是中间状态，必须要有个期限，系统保证在没有后续更新的前提下,在这个期限后,系统最终返回上一次更新操作的值,从而达到数据的最终一致性,这个容忍期限（不一致窗口的时间）取决于通信延迟，系统负载，数据复制方案设计，复制副本个数等，DNS是一个典型的最终一致性系统。）</li>
<li>Eventually consisten(最终一致性)：相对强一致性而言，最终一致性仅仅要求数据在经过一段合理的延时后（软状态），最终抵达一致即可。</li>
</ul>
<p>关于最终一致性的变种模型，一般在实践中，五种模式会组合使用：</p>
<ul>
<li>因果一致性(Causal consistency)：如果节点A在更新完某个数据后通知了节点B，那么节点B之后对该数据的访问和修改都是基于A更新后的值。与此同时，和节点A没有因果关系的节点C则没有这样的限制。</li>
<li>读己所写(Read your writes)：一个节点总能访问自己更新过的最新值，是因果一致性的特定形式。</li>
<li>会话一致性(session consistency)：系统保证在一个有效的会话中实现读己所写(Read your writes)。</li>
<li>单调读一致性(Monotonic read consistency)：节点A从系统中读到一个数据项D的某个值V后，节点A后续对数据项D的访问都不会返回比值V更旧的值。</li>
<li>单调写一致性(Monotonic write consistency)：系统需保证来自同一个节点的写操作被顺序执行。</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310003.png" alt="img" loading="lazy"></figure>
<h2 id="解决方案">解决方案</h2>
<h3 id="概要-2">概要</h3>
<p>因为业务的多样性与复杂性，再加上分布式事务解决方案也并未银弹，所以，在实际开发中，应该以本身业务作为作出发点，选择最适合自己的方案。</p>
<h3 id="二阶段提交协议">二阶段提交协议</h3>
<p>在分布式系统中，为了保证多节点间事务的正确执行，便需要一个协调者来管理参与事务的所有节点，确保操作结果符合执行预期。其中，XA便是一个典型的分布式事务处理协议。</p>
<p>XA协议是由X/Open组织提出的分布式事务处理规范，它主要定义了(全局)事务管理器TM和(局部)资源管理器RM之间的接口；是通过二阶段提交协议来保证强一致性的。</p>
<ul>
<li>第一阶段(prepare)：事务管理者TM(协调者)向资源管理者RM(参与者)发送prepare请求，RM(参与者)进行预提交并将结果响应给TM。</li>
<li>第二阶段(commit/rollback)：如果所有RM(参与者)预提交结果都为成功，则TM(协调者)向所有RM(参与者)发送commit请求，否则发送rollback请求。</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310004.png" alt="img" loading="lazy"></figure>
<p>二阶段提交协议虽然为强一致性提出了一套解决方案，但需要注意的是，其中也有几个不可忽略的缺点。</p>
<ul>
<li>同步阻塞：TM(协调者)控制着所有RM(参与者)的操作(准备与实际提交)，这个过程是同步的，TM(协调者)必须等待所有的RM(参与者)返回操作结果才能进行下一步操作；如果在这个过程中有其他请求进来，将会被阻塞。</li>
<li>单点故障：无论是TM(协调者)还是RM(参与者)发生故障，整个流程都将会陷入无限期等待中。</li>
<li>数据不一致：极端情况下，prepare成功，但commit出现部分提交，部分宕机，便会导致数据不一致。</li>
</ul>
<h3 id="三阶段提交协议">三阶段提交协议</h3>
<p>三阶段提交协议是二阶段提交协议的改良版本，它增加了超时机制用来解决同步阻塞问题；实际上是将第一阶段prepare拆分为canCommit和preCommit两个阶段。</p>
<p>preCommit阶段，TM(协调者)在发送真正的commit请求之前，会再次检查各个RM(参与者)的状态，以确保它们的状态一致。</p>
<p>当然，某些极端场景下，同样会出现数据不一致；如：第三阶段的commit出现机器宕机。</p>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310005.png" alt="img" loading="lazy"></figure>
<h3 id="tcc">TCC</h3>
<p>在二阶段提交协议，资源管理者RM负责准备、提交与回滚，而事务管理者TM则负责协调所有RM具体进行哪些操作；资源管理者RM有很多种实现方式，其中TCC(Try-Confirm-Cancel)便是资源管理者的一种服务化的实现。</p>
<ul>
<li>Try阶段：尝试执行，完成业务检查（一致性），预留业务资源（隔离性）。</li>
<li>Confirm阶段：真正的执行具体业务操作，保证幂等。</li>
<li>Cancel阶段：回滚操作，释放try阶段预留的业务资源，保证幂等。</li>
</ul>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310006.png" alt="img" loading="lazy"></figure>
<p>关于TCC理论及设计可参考：<a href="https://www.sofastack.tech/blog/seata-tcc-theory-design-realization/">点击此处跳转文章页面</a></p>
<p>关于TCC使用场景可参考：<a href="https://www.sofastack.tech/blog/seata-tcc-applicable-models-scenarios/">点击此处跳转文章页面</a></p>
<h3 id="saga">Saga</h3>
<p>saga会将一个长事务(long lived transaction)拆分为多个短事务，然后saga自己进行调度管理。saga有点类似于tcc，但是没有try阶段。</p>
<p>当流程出现异常导致部分事务执行失败时，saga会进行补偿操作。此时saga有两种选择：backward recovery 和 forward recovery。</p>
<ul>
<li>逆向恢复(backward recovery)：即回滚操作，将之前所有成功的节点进行回滚，以达到数据一致的目的。</li>
<li>正向恢复(forward recovery)：根据save-point尽最大努力不断重试，以达到数据一致的目的。</li>
</ul>
<p>sagas论文地址：<a href="https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf">点击此处跳转论文页面</a></p>
<h3 id="重试补偿模式">重试补偿模式</h3>
<p>在实际业务开发中，可以针对某些特定的业务添加额外的消息表记录失败信息，后续定时地进行补偿操作。例如京东订单成功后会发送邮件信息，此时可简单的拆分为订单服务与邮件服务，时序图如下：</p>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310007.png" alt="img" loading="lazy"></figure>
<h2 id="开源分布式事务框架">开源分布式事务框架</h2>
<h3 id="rocketmq">RocketMQ</h3>
<p>从4.3版本开始提供基于2PC(二阶段提交协议)加补偿机制的分布式事务消息，其中补偿机制主要针对2PC过程超时或失败的场景。可参考：<a href="http://rocketmq.apache.org/rocketmq/the-design-of-transactional-message/">点击此处跳转页面</a></p>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310008.png" alt="img" loading="lazy"></figure>
<ol>
<li>生产者发送半消息(half message)至MQ服务端。</li>
<li>如果发送半消息成功，则执行本地事务。</li>
<li>基于本地事务的执行结果发送commit或rollback消息到MQ服务端。</li>
<li>如果commit/rollback消息丢失或者生产者在执行本地事务时处于pended状态，MQ服务端将会发送检查消息到同组(same group)的每个生产者获取事务消息的状态(CommitTransaction,RollbackTransaction,Unknown)。</li>
<li>生产者基于本地事务状态恢复commit/rollback消息。</li>
<li>已提交的消息会被MQ服务端分发给消费者，而回滚类消息则会被丢弃。</li>
</ol>
<p>注意1：半消息(half message)是无法被消费的，因为它位于一个独立的内部half-topic中，对消费者是不可见的；commit过程相当于从half-topic中取出消息进行重新投递。</p>
<p>注意2：使用事务消息功能将会降低rocketMQ的执行效率，因为写过程被放大了。</p>
<h3 id="tcc-transaction">TCC-Transaction</h3>
<p>基于TCC模式开发的分布式事务框架，具体可参考：<a href="https://github.com/changmingxie/tcc-transaction">点击此处跳转官网地址</a></p>
<h3 id="servicecomb-saga">ServiceComb-Saga</h3>
<p>servicecomb-saga是servicecomb中的一部分，比之tcc而言少了一个try操作，更加轻量化，具体可参考：<a href="https://github.com/apache/servicecomb-saga-actuator">点击此处跳转官网地址</a></p>
<h3 id="seata">Seata</h3>
<p>阿里开源的分布式事务解决方案，致力于提供高性能和简单易用，包括AT，TCC，SAGA以及XA事务模式。具体可参考：<a href="http://seata.io/zh-cn/">点击此处跳转官网地址</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bash条件判断详解]]></title>
        <id>https://philosopherzb.github.io/post/bash-tiao-jian-pan-duan-xiang-jie/</id>
        <link href="https://philosopherzb.github.io/post/bash-tiao-jian-pan-duan-xiang-jie/">
        </link>
        <updated>2022-05-13T09:20:29.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述bash条件判断规则语法。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/rocks-1757593_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="条件判断">条件判断</h2>
<h3 id="if结构">if结构</h3>
<p>if是最常用的条件判断结构，只有符合给定条件时，才会执行指定的命令。它的语法如下。</p>
<p>注意：如果写在同一行，关键字之间需要使用分号隔离。分号是 Bash 的命令分隔符</p>
<pre><code>if commands; then
  commands
[elif commands; then
  commands...]
[else
  commands]
fi

# 例子 判断环境变量$USER是否等于root
if test $USER = &quot;root&quot;; then
  echo &quot;Hello root.&quot;
else
  echo &quot;You are not root.&quot;
fi

# 单行书写例子
if true; then echo 'hello world'; fi

</code></pre>
<p>if后面可以跟任意数量的命令。这时，所有命令都会执行，但是判断真伪只看最后一个命令，即使前面所有命令都失败，只要最后一个命令返回0，就会执行then的部分。</p>
<pre><code>if false; i=3; true; then echo 'hello world'; fi
echo ${i}

</code></pre>
<h3 id="test命令">test命令</h3>
<p>if结构的判断条件，一般使用test命令，有三种形式。</p>
<pre><code># 写法一
test expression
# 写法二
[ expression ]
# 写法三
[[ expression ]]

</code></pre>
<p>三种形式是等价的，但是第三种形式还支持正则判断，前两种不支持。</p>
<p>expression是一个表达式。这个表达式为真，test命令执行成功（返回值为0）；表达式为伪，test命令执行失败（返回值为1）。</p>
<p>注意：第二种和第三种写法，[ ]与内部的表达式之间必须有空格。</p>
<pre><code>read i
# 写法一
if test ${i} == 1 ; then
  echo &quot;input param: ${i}&quot;
fi
# 写法二
if [ ${i} == 1 ] ; then
   echo &quot;input param: ${i}&quot;
fi
# 写法三
if [[ ${i} == 1 ]] ; then
   echo &quot;input param: ${i}&quot;
fi

</code></pre>
<h2 id="判断表达式">判断表达式</h2>
<p>[[ ]]使用在条件判断中，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt; 和 &gt; 操作符能够正常存在于 [[ ]] 条件判断结构中，但是如果出现在 [ ] 结构中的话，会报错。</p>
<p>执行的时候，需要用bash test.sh；因为[[]]是bash脚本中的命令（bash是sh的增强版本）。</p>
<p>注意：[[  ]]中操作符与变量之间需要有空格，否则会被当做一个变量处理。</p>
<h3 id="文件判断">文件判断</h3>
<ul>
<li>[ -a file ]：如果 file 存在，则为true。</li>
<li>[ -b file ]：如果 file 存在并且是一个块（设备）文件，则为true。</li>
<li>[ -c file ]：如果 file 存在并且是一个字符（设备）文件，则为true。</li>
<li>[ -d file ]：如果 file 存在并且是一个目录，则为true。</li>
<li>[ -e file ]：如果 file 存在，则为true。</li>
<li>[ -f file ]：如果 file 存在并且是一个普通文件，则为true。</li>
<li>[ -g file ]：如果 file 存在并且设置了组 ID，则为true。</li>
<li>[ -G file ]：如果 file 存在并且属于有效的组 ID，则为true。</li>
<li>[ -h file ]：如果 file 存在并且是符号链接，则为true。</li>
<li>[ -k file ]：如果 file 存在并且设置了它的“sticky bit”，则为true。</li>
<li>[ -L file ]：如果 file 存在并且是一个符号链接，则为true。</li>
<li>[ -N file ]：如果 file 存在并且自上次读取后已被修改，则为true。</li>
<li>[ -O file ]：如果 file 存在并且属于有效的用户 ID，则为true。</li>
<li>[ -p file ]：如果 file 存在并且是一个命名管道，则为true。</li>
<li>[ -r file ]：如果 file 存在并且可读（当前用户有可读权限），则为true。</li>
<li>[ -s file ]：如果 file 存在且其长度大于零，则为true。</li>
<li>[ -S file ]：如果 file 存在且是一个网络 socket，则为true。</li>
<li>[ -t fd ]：如果 fd 是一个文件描述符，并且重定向到终端，则为true。 这可以用来判断是否重定向了标准输入／输出错误。</li>
<li>[ -u file ]：如果 file 存在并且设置了 setuid 位，则为true。</li>
<li>[ -w file ]：如果 file 存在并且可写（当前用户拥有可写权限），则为true。</li>
<li>[ -x file ]：如果 file 存在并且可执行（有效用户有执行／搜索权限），则为true。</li>
<li>[ file1 -nt file2 ]：如果 FILE1 比 FILE2 的更新时间最近，或者 FILE1 存在而 FILE2 不存在，则为true。</li>
<li>[ file1 -ot file2 ]：如果 FILE1 比 FILE2 的更新时间更旧，或者 FILE2 存在而 FILE1 不存在，则为true。</li>
<li>[ FILE1 -ef FILE2 ]：如果 FILE1 和 FILE2 引用相同的设备和 inode 编号，则为true。</li>
</ul>
<pre><code>file=/opt/shellScriptDir/test1.sh
if [[ -e ${file} &amp;&amp; -a ${file} ]]; then
    echo &quot;${file} exist&quot;
    if [[ -f ${file} ]]; then
        echo &quot;${file} is normal file&quot;
    fi
    if [[ -d ${file} ]]; then
        echo &quot;${file} is directory&quot;
    fi
    if [[ -r ${file} ]]; then
        echo &quot;${file} is readable&quot;
    fi
    if [[ -w ${file} ]]; then
        echo &quot;${file} is writable&quot;
    fi
    if [[ -x ${file} ]]; then
        echo &quot;${file} is executable/searchable&quot;
    fi
else
    echo &quot;${file} not exist&quot;
fi

</code></pre>
<p>注意：上述判断中，如果使用的是单个中括号[]时，$file需要用双引号括起来，否则判断将会失误。因为当$file为空时，-e会判断为真，如果放在双引号中，返回的是空字符串，[ -e &quot;&quot; ]会判断为伪。</p>
<pre><code># 下面的例子会输出 not exist
file=
if [ -e &quot;${file}&quot; ]; then
    echo &quot;${file} exist&quot;
else
    echo &quot;${file} not exist&quot;
fi

# 下面的例子会输出 exist
file=
if [ -e ${file} ]; then
    echo &quot;${file} exist&quot;
else
    echo &quot;${file} not exist&quot;
fi

</code></pre>
<h3 id="字符串判断">字符串判断</h3>
<ul>
<li>[ string ]：如果string不为空（长度大于0），则判断为真。</li>
<li>[ -n string ]：如果字符串string的长度大于零，则判断为真。</li>
<li>[ -z string ]：如果字符串string的长度为零，则判断为真。</li>
<li>[ string1 = string2 ]：如果string1和string2相同，则判断为真。</li>
<li>[ string1 == string2 ] 等同于[ string1 = string2 ]。</li>
<li>[ string1 != string2 ]：如果string1和string2不相同，则判断为真。</li>
<li>[ string1 '&gt;' string2 ]：如果按照字典顺序string1排列在string2之后，则判断为真。</li>
<li>[ string1 '&lt;' string2 ]：如果按照字典顺序string1排列在string2之前，则判断为真。</li>
</ul>
<p>注意，test命令内部的&gt;和&lt;，必须用引号引起来（或者是用反斜杠转义，或者使用双中括号）。否则，它们会被 shell 解释为重定向操作符。</p>
<p>字符串判断时，变量要放在双引号之中，比如[ -n &quot;$COUNT&quot; ]，否则变量替换成字符串以后，test命令可能会报错，提示参数过多。另外，如果不放在双引号之中，变量为空时，命令会变成[ -n ]，这时会判断为真。如果放在双引号之中，[ -n &quot;&quot; ]就判断为伪。</p>
<p>如果不想使用双引号，也可以使用双括号。</p>
<pre><code>str=fwfw
if [[ -z ${str} ]]; then
    echo &quot;${str} length =0&quot;
elif [[ -n ${str} ]]; then
    echo &quot;${str} length &gt;0&quot;
    if [[ ${str} = &quot;fwfw&quot; ]]; then
        echo &quot;${str} exist&quot;
    fi
fi

</code></pre>
<h3 id="整数判断">整数判断</h3>
<ul>
<li>[ integer1 -eq integer2 ]：如果integer1等于integer2，则为true。</li>
<li>[ integer1 -ne integer2 ]：如果integer1不等于integer2，则为true。</li>
<li>[ integer1 -le integer2 ]：如果integer1小于或等于integer2，则为true。</li>
<li>[ integer1 -lt integer2 ]：如果integer1小于integer2，则为true。</li>
<li>[ integer1 -ge integer2 ]：如果integer1大于或等于integer2，则为true。</li>
<li>[ integer1 -gt integer2 ]：如果integer1大于integer2，则为true。</li>
</ul>
<pre><code>a=10
b=20
if [ ${a} -lt ${b} ]
then
    echo &quot;${a} &lt; ${b}&quot;
else
    echo &quot;${a} &gt;= ${b}&quot;
fi

# 使用双中括号，效果一致
if [[ ${a} &lt; ${b} ]]
then
    echo &quot;${a} &lt; ${b}&quot;
else
    echo &quot;${a} &gt;= ${b}&quot;
fi

</code></pre>
<h3 id="正则判断">正则判断</h3>
<p>[[ expression ]]这种判断形式，支持正则表达式。</p>
<pre><code># regex是一个正则表示式，=~是正则比较运算符。
[[ string =~ regex ]]
</code></pre>
<pre><code>read input
if [[ ${input} =~ [0-9] ]]; then
    echo &quot;match num value = ${input}&quot;
fi

</code></pre>
<h3 id="逻辑运算">逻辑运算</h3>
<ul>
<li>AND运算：符号&amp;&amp;，也可使用参数-a。</li>
<li>OR运算：符号||，也可使用参数-o。</li>
<li>NOT运算：符号!。</li>
</ul>
<pre><code>## 使用双中括号，可以直接在命令内容拼接逻辑运算符
read input
if [[ ${input} =~ [0-9] &amp;&amp; ${input} == 3 ]]; then
    echo &quot;match num value = ${input}&quot;
fi

</code></pre>
<p>&amp;&amp; 和 || 也被称作命令控制操作符，可以用来聚合多个逻辑运算命令。</p>
<pre><code>file=/opt/shellScriptDir/temp
[[ -d  ${file} ]] || echo &quot;${file} not exist&quot;
</code></pre>
<h3 id="算术判断">算术判断</h3>
<p>bash提供了(( ... ))作为算术条件，用于进行算术运算的判断。</p>
<p>注意，算术判断不需要使用test命令，而是直接使用((...))结构。这个结构的返回值，决定了判断的真伪。</p>
<p>如果算术计算的结果是非零值，则表示判断成立。这一点跟命令的返回值正好相反，需要小心。</p>
<pre><code># 输出match num value true
if [[ 0 ]]; then
    echo &quot;match num value true&quot;
else
    echo &quot;value false&quot;
fi

# 输出value false
if (( 0 )); then
    echo &quot;match num value true&quot;
else
    echo &quot;value false&quot;
fi

</code></pre>
<p>(( ... ))可以用作变量赋值，赋值完成后将会返回变量的值。</p>
<pre><code># 输出match num value: 1
if (( var=1 )); then
    echo &quot;match num value: ${var}&quot;
else
    echo &quot;value false&quot;
fi

</code></pre>
<h3 id="case结构判断">case结构判断</h3>
<p>case结构用于多值判断，可以为每个值指定对应的命令，跟包含多个elif的if结构等价，但是语义更好。</p>
<pre><code># 语法格式
# expression是一个表达式，pattern是表达式的值或者一个模式，可以有多条，
# 用来匹配多个值，每条以两个分号（;）结尾。
case expression in
  pattern )
    commands ;;
  pattern )
    commands ;;
  ...
esac

</code></pre>
<pre><code># 简单实例
echo &quot;input value[1-3]: &quot;
read input
case ${input} in
    1) echo &quot;read value is 1&quot;;;
    2) echo &quot;read value is 2&quot;;;
    3) echo &quot;read value is 3&quot;;;
    *) echo &quot;read value is not match: ${input}&quot;;;
esac

</code></pre>
<p>case的匹配模式可以使用各种通配符，类似于下方所示：</p>
<ul>
<li>a)：匹配a。</li>
<li>a|b)：匹配a或b。</li>
<li>[[:alpha:]])：匹配单个字母。</li>
<li>???)：匹配3个字符的单词。</li>
<li>*.txt)：匹配.txt结尾。</li>
<li>*)：匹配任意输入，通过作为case结构的最后一个模式。</li>
</ul>
<pre><code># 匹配数值，单个字符，两个字符
echo &quot;input value[number or character]: &quot;
read input
case ${input} in
    [0-9]) echo &quot;read number value is: ${input}&quot;;;
    [[:lower:]] | [[:upper:]]) echo &quot;read character value is: ${input}&quot;;;
    ??) echo &quot;read double input value is: ${input}&quot;;;
    *) echo &quot;read value is not match: ${input}&quot;;;
esac

</code></pre>
<p>Bash 4.0之前，case结构只能匹配一个条件，然后就会退出case结构。Bash 4.0之后，允许匹配多个条件，这时可以用;;&amp;终止每个条件块。</p>
<pre><code>echo &quot;input character value: &quot;
read input
case ${input} in
  [[:upper:]])    echo &quot;'${input}' is upper case.&quot; ;;&amp;
  [[:lower:]])    echo &quot;'${input}' is lower case.&quot; ;;&amp;
  [[:alpha:]])    echo &quot;'${input}' is alphabetic.&quot; ;;&amp;
  [[:digit:]])    echo &quot;'${input}' is a digit.&quot; ;;&amp;
  [[:graph:]])    echo &quot;'${input}' is a visible character.&quot; ;;&amp;
  [[:punct:]])    echo &quot;'${input}' is a punctuation symbol.&quot; ;;&amp;
  [[:space:]])    echo &quot;'${input}' is a whitespace character.&quot; ;;&amp;
  [[:xdigit:]])   echo &quot;'${input}' is a hexadecimal digit.&quot; ;;&amp;
esac

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shell基本知识]]></title>
        <id>https://philosopherzb.github.io/post/shell-ji-ben-zhi-shi/</id>
        <link href="https://philosopherzb.github.io/post/shell-ji-ben-zhi-shi/">
        </link>
        <updated>2022-04-30T09:03:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述shell基本知识。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/nature-2147400_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="简介">简介</h2>
<h3 id="简介-2">简介</h3>
<p>Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。</p>
<p>Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。</p>
<p>Shell 脚本（shell script），是一种为 shell 编写的脚本程序。业界所说的 shell 通常都是指 shell 脚本（故此处也沿用该说明），需要注意的是：shell 和 shell script 是两个不同的概念。</p>
<p>查看安装的shell信息，两种查看方式任选一种键入回车即可得到相关信息。</p>
<pre><code>ls -l /bin/*sh
cat /etc/shells

</code></pre>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230307001.png" alt="img" loading="lazy"></figure>
<h2 id="基本语法bash">基本语法（Bash）</h2>
<h3 id="变量">变量</h3>
<h4 id="基础变量">基础变量</h4>
<p>创建变量时，需遵循如下规则：</p>
<ul>
<li>命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。</li>
<li>不允许出现空格及标点符号。</li>
<li>不能使用bash里的关键字（可用help命令查看保留关键字）。</li>
</ul>
<p>变量声明的语法如下（注意：等号两边不能存在空格，读取时使用$符）：</p>
<pre><code>variable=vlaue
</code></pre>
<pre><code># 简单例子
# 变量 a 赋值为字符串 hello
a=hello
# $a 等效于 ${a}，加花括号是为了帮助解释器识别变量的边界
# 如：echo $a_world 将不会输出内容，因为变量a_world不存在
# 但是可以使用echo ${a}_world，将会输出hello_world
echo ${a}

# 变量值包含空格，就必须放在引号里面        
b=&quot;world shell&quot;
echo ${b}

# 变量值可以引用其他变量的值
c=&quot;${a} ${b} !&quot;
echo ${c}

# 变量值可以使用转义字符
d=&quot;\t content \n&quot;
echo ${d}

# 变量值可以是命令的执行结果
e=$(ls -l /bin/*sh)
echo ${e}

# 变量值可以是数学运算的结果
f=$((4 * 5))
echo ${f}

# 变量重复赋值，后面的赋值将会覆盖前面的赋值
foo=1
foo=2
echo ${foo}

# 变量的值是变量，如果需要输出，可以使用${!varname}愈发
myvar=USER
echo ${!myvar}

</code></pre>
<h4 id="特殊变量">特殊变量</h4>
<p>Bash（是shell的一个增强版本）提供了一些特殊变量，特殊变量的值由shell预定义，用户不用进行赋值。</p>
<ul>
<li>$?: 表示上一个命令的退出码，用来判断上一个命令是否执行成功，0表示成功，非0表示失败。</li>
<li>$$: 表示当前shell的进程id，也可以用来命名临时文件</li>
<li>$_: 表示上个命令的最后一个参数</li>
<li>$!: 表示最近一个后台执行的异步命令的进程id</li>
<li>$0: 表示当前shell的名称（在命令直接执行时）或者脚本名（在脚本中执行时）</li>
<li>$-: 表示当前shell的启动参数</li>
<li>$@ $#: 表示脚本中的参数数量（两者不同之处：在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 &quot; * &quot; 等价于 &quot;1 2 3&quot;（传递了一个参数），而 &quot;@&quot; 等价于 &quot;1&quot; &quot;2&quot; &quot;3&quot;（传递了三个参数）。）</li>
</ul>
<pre><code># $? 例子
ls -l /bin/bash
echo $?
ls -l notexistfile
echo $?
# 输出结果
-rwxr-xr-x 1 root root 1183448 Jun 18  2020 /bin/bash
0
ls: cannot access 'notexistfile': No such file or directory
2

</code></pre>
<h4 id="变量默认值">变量默认值</h4>
<p>Bash 提供四个特殊语法，跟变量的默认值有关，目的是保证变量不为空。</p>
<ul>
<li>${varname:-word} 语法含义：如果变量varname存在且不为空，则返回它的值，否则返回word。它的目的是返回一个默认值，比如${count:-0}表示变量count不存在时返回0。</li>
<li>${varname:=word} 语法含义：如果变量varname存在且不为空，则返回它的值，否则将它设为word，并且返回word。它的目的是设置变量的默认值，比如${count:=0}表示变量count不存在时返回0，且将count设为0。</li>
<li>${varname:+word} 语法含义：如果变量名存在且不为空，则返回word，否则返回空值。它的目的是测试变量是否存在，比如${count:+1}表示变量count存在时返回1（表示true），否则返回空值。</li>
<li>${varname:?message} 语法含义：如果变量varname存在且不为空，则返回它的值，否则打印出varname: message，并中断脚本的执行。如果省略了message，则输出默认的信息“parameter null or not set.”。它的目的是防止变量未定义，比如${count:?&quot;undefined!&quot;}表示变量count未定义时就中断执行，抛出错误，返回给定的报错信息undefined!。</li>
</ul>
<pre><code>echo ${a:-0}
echo ${a:=word}
echo ${a:+1}
echo ${b:?&quot;undefined!&quot;}
# 输出结果
0
word
1
./test1.sh: line 5: b: undefined!

</code></pre>
<h4 id="变量命令">变量命令</h4>
<p>declare命令可以声明一些特殊类型的变量，为变量设置一些限制，比如声明只读类型的变量和整数类型的变量。</p>
<p>declare语法格式：declare OPTION VARIABLE=value，命令的主要参数（OPTION）如下：</p>
<ul>
<li>-a：声明数组变量。</li>
<li>-f：输出所有函数定义。</li>
<li>-F：输出所有函数名。</li>
<li>-i：声明整数变量。</li>
<li>-l：声明变量为小写字母。</li>
<li>-p：查看变量信息。</li>
<li>-r：声明只读变量。</li>
<li>-u：声明变量为大写字母。</li>
<li>-x：该变量输出为环境变量。</li>
</ul>
<p>readonly命令等同于declare -r，用来声明只读变量，不能改变变量值，也不能unset变量。</p>
<p>let命令声明变量时，可以直接执行算术表达式。</p>
<h4 id="数组变量">数组变量</h4>
<p>数组中可以存放多个值。Bash Shell 只支持一维数组（不支持多维数组），初始化时不需要定义数组大小。</p>
<p>与大部分编程语言类似，数组元素的下标由 0 开始。</p>
<p>Shell 数组用括号来表示，元素用&quot;空格&quot;符号分割开，语法格式如下：</p>
<pre><code>array_name=(value1 value2 ... valuen)
</code></pre>
<pre><code># 数组例子
array_test=(&quot;hello&quot; &quot;world&quot; &quot;shell&quot;)
echo ${array_test[2]}
# 获取所有元素
echo ${array_test[@]}
echo ${array_test[*]}

</code></pre>
<h4 id="字符串变量进阶例子">字符串变量&lt;进阶例子&gt;</h4>
<pre><code>var=&quot;opt/temp/test.sh&quot;
# 字符串长度
echo ${#var}

# 截取字符串 ${varname:offset:length}: 从位置offset开始（从0开始计算），长度为length
echo ${var:0:3}
# 如果省略length，则从位置offset开始，一直返回到字符串的结尾。
echo ${var:0}
# 如果offset为负值，表示从字符串的末尾开始算起。
# 注意，负数前面必须有一个空格（如果不想写空格，可以填0），以防止与${variable:-word}的变量的设置默认值语法混淆。
# 这时，如果还指定length，则length不能小于零。
echo ${var: -16:3}
echo ${var: -13}
echo ${var:0-16:3}
echo ${var:0-13}

# 输出大写
echo ${var^^}
# 输出小写
echo ${var,,}

# 匹配删除字符串
# 匹配模式pattern可以使用*、?、[]等通配符。

# ${variable#pattern}
# 如果 pattern 匹配变量 variable 的开头，删除最短匹配（非贪婪匹配）的部分，返回剩余部分
# #*/ 表示从左边开始删除第一个 / 号及左边的所有字符，即删除opt/，结果为：temp/test.sh
echo ${var#*/}

# ${variable##pattern}
# 如果 pattern 匹配变量 variable 的开头，删除最长匹配（贪婪匹配）的部分，返回剩余部分
# ##*/ 表示从左边开始删除最后（最右边）一个 / 号及左边的所有字符，即删除opt/temp/，结果为：test.sh
echo ${var##*/}

# ${variable%pattern}
# 如果 pattern 匹配变量 variable 的结尾，删除最短匹配（非贪婪匹配）的部分，返回剩余部分
# %/* 表示从右边开始，删除第一个 / 号及右边的字符，即删除/test.sh，结果：opt/temp
echo ${var%/*}

# ${variable%%pattern}
# 如果 pattern 匹配变量 variable 的结尾，删除最长匹配（贪婪匹配）的部分，返回剩余部分
# %%/* 表示从右边开始，删除最后（最左边）一个 / 号及右边的字符，即删除/temp/test.sh，结果：opt
echo ${var%%/*}

</code></pre>
<h3 id="算术运算符">算术运算符</h3>
<h4 id="算术表达式">算术表达式</h4>
<p>((...))语法可以进行整数的算术运算。该表达式可以忽略内部的空格，在其内部可以使用()改变运算顺序。输出结果时，需要在前面加上$符。</p>
<pre><code>a=2
echo $(( ${a} + 2 ))
# 可以不使用$或者${}（花括号是为了确定边界）引用变量
echo $(( + 2))

# 赋值
i=$((a + 2))
echo &quot;i= ${i}&quot;

</code></pre>
<p>expr命令同样支持算是运算，其可以使用变量替换。</p>
<p>注意：表达式与运算符之间需要有空格，否则会被当做字符串输出。</p>
<pre><code>a=2
expr ${a} + 2

# 赋值，两种方式都可进行赋值，``符号位于esc键下方，并非单引号
b=$(expr ${a} + 2)
c=`expr ${a} + 2`
echo &quot;b= ${b}&quot;
echo &quot;c= ${c}&quot;

</code></pre>
<p>((...))语法支持的算术运算符如下。</p>
<ul>
<li>+：加法</li>
<li>-：减法</li>
<li>*：乘法</li>
<li>/：除法（整除）</li>
<li>%：余数</li>
<li>**：指数</li>
<li>++：自增运算（作为前缀是先运算后返回值，作为后缀是先返回值后运算）</li>
<li>--：自减运算（作为前缀是先运算后返回值，作为后缀是先返回值后运算）</li>
</ul>
<h4 id="进制数值">进制数值</h4>
<p>Bash 的数值默认都是十进制，但是在算术表达式中，也可以使用其他进制。</p>
<ul>
<li>number：没有任何特殊表示法的数字是十进制数（以10为底）。</li>
<li>0number：八进制数。</li>
<li>0xnumber：十六进制数。</li>
<li>base#number：base进制的数。</li>
</ul>
<pre><code>echo $((016))
echo $((0xee))
echo $((2#00000011))

</code></pre>
<h4 id="位运算">位运算</h4>
<p>Bash 支持二进制位运算符</p>
<ul>
<li>&lt;&lt;：位左移运算，把一个数字的所有位向左移动指定的位。</li>
<li>&gt;&gt;：位右移运算，把一个数字的所有位向右移动指定的位。</li>
<li>&amp;：位的“与”运算，对两个数字的所有位执行一个AND操作。</li>
<li>|：位的“或”运算，对两个数字的所有位执行一个OR操作。</li>
<li>~：位的“否”运算，对一个数字的所有位取反。</li>
<li>!：逻辑“否”运算</li>
<li>^：位的异或运算（exclusive or），对两个数字的所有位执行一个异或操作。</li>
</ul>
<h4 id="逻辑运算">逻辑运算</h4>
<p>Bash 支持逻辑运算符</p>
<ul>
<li>&lt;/-lt：小于</li>
<li>&gt;/-gt：大于</li>
<li>&lt;=/-le：小于或相等</li>
<li>&gt;=/ge：大于或相等</li>
<li>==/-eq：相等</li>
<li>!=/-ne：不相等</li>
<li>&amp;&amp;：逻辑与</li>
<li>||：逻辑或</li>
<li>expr1?expr2:expr3：三元条件运算符。若表达式expr1的计算结果为非零值（算术真），则执行表达式expr2，否则执行表达式expr3。</li>
</ul>
<h4 id="赋值与求值运算">赋值与求值运算</h4>
<p>逗号,在$((...))内部是求值运算符，执行前后两个表达式，并返回后一个表达式的值。</p>
<pre><code>echo $((foo = 1 + 2, 3 * 4))
echo ${foo}
# 输出
12
3

</code></pre>
<p>赋值运算如下：</p>
<ul>
<li>parameter = value：简单赋值。</li>
<li>parameter += value：等价于parameter = parameter + value。</li>
<li>parameter -= value：等价于parameter = parameter – value。</li>
<li>parameter *= value：等价于parameter = parameter * value。</li>
<li>parameter /= value：等价于parameter = parameter / value。</li>
<li>parameter %= value：等价于parameter = parameter % value。</li>
<li>parameter &lt;&lt;= value：等价于parameter = parameter &lt;&lt; value。</li>
<li>parameter &gt;&gt;= value：等价于parameter = parameter &gt;&gt; value。</li>
<li>parameter &amp;= value：等价于parameter = parameter &amp; value。</li>
<li>parameter |= value：等价于parameter = parameter | value。</li>
<li>parameter ^= value：等价于parameter = parameter ^ value。</li>
</ul>
<pre><code>echo $((foo = 3))
echo $((foo *= 2))
# 输出
3
6

</code></pre>
<h2 id="流程控制">流程控制</h2>
<h3 id="条件判断">条件判断</h3>
<p>1、简单if语句</p>
<pre><code># 简单if语句
if condition
then
    command
fi

</code></pre>
<p>2、if else语句</p>
<pre><code># if else语句
if condition
then
    command
else
    command
fi

</code></pre>
<p>3、if else-if else语句</p>
<pre><code># if else-if else语句
if condition1
then
    command1
elif condition2 
then 
    command2
else
    commandN
fi

</code></pre>
<pre><code># 例子
a=10
b=20
# [[ ]]使用在条件判断中，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt; 和 &gt; 操作符能够正常存在于 [[ ]] 条件判断结构中，但是如果出现在 [ ] 结构中的话，会报错。
# 执行的时候，需要用bash test.sh；因为[[]]是bash脚本中的命令（bash是sh的增强版本）。
if [[ ${a} &lt; ${b} ]]
then
    echo &quot;a &lt; b&quot;
else
    echo &quot;a &gt;= b&quot;
fi

</code></pre>
<h3 id="循环语句">循环语句</h3>
<p>Bash 提供三种循环语法for、while和until。</p>
<p>while循环有一个判断条件，只要符合条件，就不断循环执行指定的语句。关键字do可以跟while不在同一行，这时两者之间不需要使用分号分隔。</p>
<pre><code>while condition; do
  commands
done

</code></pre>
<p>until循环与while循环恰好相反，只要不符合判断条件（判断条件失败），就不断循环执行指定的语句。一旦符合判断条件，就退出循环。关键字do可以与until不写在同一行，这时两者之间不需要分号分隔。</p>
<pre><code>until condition; do
  commands
done

</code></pre>
<p>for...in循环用于遍历列表中的每一项。关键词do可以跟for写在同一行，两者使用分号分隔。</p>
<pre><code>for variable in list
do
  commands
done

</code></pre>
<p>for循环支持C语言的循环语法。</p>
<pre><code># expression1用来初始化循环条件，expression2用来决定循环结束的条件，
# expression3在每次循环迭代的末尾执行，用于更新值。
# 注意，循环条件放在双重圆括号之中。另外，圆括号之中使用变量，不必加上美元符号$。
for (( expression1; expression2; expression3 )); do
  commands
done

# 等同于下述表达式
(( expression1 ))
while (( expression2 )); do
  commands
  (( expression3 ))
done

# 例子
for (( i=0; i&lt;5; i=i+1 )); do
  echo $i
done

</code></pre>
<p>Bash 提供了两个内部命令break和continue，用来在循环内部跳出循环。</p>
<p>break命令立即终止循环，程序继续执行循环块之后的语句，即不再执行剩下的循环。</p>
<p>continue命令立即终止本轮循环，开始执行下一轮循环。</p>
<pre><code># break
a=(first second third)
for i in ${a[*]}; do
    echo ${i}
    if [[ ${i} == &quot;first&quot; ]]; then break; fi
done

# continue
a=(first second third)
for i in ${a[*]}; do
    if [[ ${i} == &quot;second&quot; ]]; then continue; fi
    echo ${i}
done

</code></pre>
<h2 id="函数">函数</h2>
<p>函数（function）是可以重复使用的代码片段，有利于代码的复用。它与别名（alias）的区别是，别名只适合封装简单的单个命令，函数则可以封装复杂的多行命令。</p>
<p>函数总是在当前 Shell 执行，这是跟脚本的一个重大区别，Bash 会新建一个子 Shell 执行脚本。如果函数与脚本同名，函数会优先执行。但是，函数的优先级不如别名，即如果函数与别名同名，那么别名优先执行。</p>
<pre><code># 第一种
fn() {
  # codes
}
# 第二种
function fn() {
  # codes
}

</code></pre>
<p>$1~$9：函数的第一个到第9个的参数。如果函数的参数多于9个，那么第10个参数可以用${10}的形式引用，以此类推</p>
<pre><code>printParam(){
    echo &quot;first param is $1&quot;
}
printParam wqrwq

function log_msg(){
    echo &quot;[$(date '+%F %T')]: $@&quot;
}
log_msg this is sample log message

</code></pre>
<p>函数体内支持局部变量的声明。同时，也支持修改全局变量。</p>
<pre><code>function fn(){
    local str=paramValue
    echo &quot;local: str=${str}&quot;
}
fn
echo &quot;global: str=${str}&quot;

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[python3基本知识]]></title>
        <id>https://philosopherzb.github.io/post/python3-ji-ben-zhi-shi/</id>
        <link href="https://philosopherzb.github.io/post/python3-ji-ben-zhi-shi/">
        </link>
        <updated>2022-04-16T07:52:59.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述python3脚本知识。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/mountains-209956_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="简介">简介</h2>
<h3 id="简介-2">简介</h3>
<p>Python 是一个高层次的结合了解释性、编译性、互动性和面向对象的脚本语言（计算机编程语言）。</p>
<p>Python 的设计具有很强的可读性，相比其他语言经常使用英文关键字，其他语言的一些标点符号，它具有比其他语言更有特色的语法结构。</p>
<p>注意：python3与python2具备较大的差别，他们之间并不完全兼容。</p>
<p>Python的优点：</p>
<ol>
<li>简单和明确，做一件事只有一种方法。</li>
<li>学习曲线低，跟其他很多语言相比，Python更容易上手。</li>
<li>开放源代码，拥有强大的社区和生态圈。</li>
<li>解释型语言，天生具有平台可移植性。</li>
<li>支持两种主流的编程范式（面向对象编程和函数式编程）都提供了支持。</li>
<li>可扩展性和可嵌入性，可以调用C/C++代码，也可以在C/C++中调用Python。</li>
<li>代码规范程度高，可读性强，适合有代码洁癖和强迫症的人群。</li>
</ol>
<p>Python的缺点主要集中在以下几点。</p>
<ol>
<li>执行效率稍低，因此计算密集型任务可以由C/C++编写。</li>
<li>代码无法加密，但是现在很多公司都不销售卖软件而是销售服务，这个问题会被淡化。</li>
<li>在开发时可以选择的框架太多（如Web框架就有100多个），有选择的地方就有错误。</li>
</ol>
<h2 id="基本语法">基本语法</h2>
<h3 id="变量">变量</h3>
<h4 id="基础变量">基础变量</h4>
<p>创建变量时，需遵循以下规则：</p>
<ul>
<li>第一个字符必须是字母表中字母或下划线 _ 。</li>
<li>标识符的其他的部分由字母、数字和下划线组成。</li>
<li>标识符对大小写敏感。</li>
<li>不能使用python保留字。</li>
<li>python3支持中文作为变量名，非 ASCII 标识符也是允许的。（不建议使用中文作为变量名）</li>
</ul>
<pre><code># 查看关键字
import keyword
var = keyword.kwlist
print(var)

</code></pre>
<h4 id="注释与编码">注释与编码</h4>
<p>单行注释使用#号，多行注释可以使用'''或者&quot;&quot;&quot;，python3编码默认为utf-8（可以修改）。</p>
<pre><code># 单行注释信息
&quot;&quot;&quot;
多行注释
&quot;&quot;&quot;

# 修改编码
# -*- coding: GBK -*-

</code></pre>
<h4 id="语法结构">语法结构</h4>
<p>python对于代码块需要使用缩进来表示，缩进的空格数是可变的，但是同一个代码块中的缩进需要保持一致，一般使用4个空格。</p>
<p>为了方便后期维护，在代码之间可以适当的使用空行进行代码功能隔离。</p>
<pre><code>a = &quot;12&quot;
if n := len(a) &gt; 1:
    print(&quot;True&quot;)
else:
    print(&quot;False&quot;)

</code></pre>
<p>python中关于一条语句过长需要换行的写法如下（使用反斜杠\）：</p>
<pre><code>var = &quot;var1&quot; \
    &quot;var2&quot; \
    &quot;var3&quot;
print(var)

</code></pre>
<p>python支持多行语句并做一行处理</p>
<pre><code>import sys; x = 'runoob'; sys.stdout.write(x + '\n')
</code></pre>
<h2 id="基本数据类型">基本数据类型</h2>
<p>Python3有六个标准的数据类型：Number，String，List，Tuple，Set，Dictionary。</p>
<p>其中不可变数据：Number，String，Tuple</p>
<p>可变数据：List，Set，Dictionary</p>
<h3 id="number数字">Number（数字）</h3>
<p>python3支持int(长整型，相当于python2的Long)，float，bool(python2没有布尔值；1等于True，0等于False，可用于算术运算)，complex(复数)。</p>
<pre><code># 可以使用type或者isinstance函数判断变量类型。
# type()不会认为子类是一种父类类型。即type事先不知道变量类型。
# isinstance()会认为子类是一种父类类型。即isinstance事先知道变量类型。
a, b, c, d = 1, 2.2, False, 4+1j
print(type(a), type(b), type(c), type(d))
print(isinstance(a, int))

# bool 算术运算
a = True
b, c, d = a + 1, a - 1, a * 10
print(a, b, c, d)

# 强制转换
a = &quot;1&quot;
print(a, int(a), bool(a), float(a))

</code></pre>
<h3 id="string字符串">String（字符串）</h3>
<p>python3使用''或者&quot;&quot;定义字符串，使用反斜杠\转义特殊字符，也可以使用r让反斜杠不转义；连接字符串用+号，重复字符串用*号。</p>
<pre><code>a = &quot;test\ncontent&quot;
b = r&quot;test\ncontent&quot;
print(a)
print(b)
print(b + b)
print(b*3)

</code></pre>
<p>字符串的截取语法：variable[头下标:尾下标]，规则为左闭右开区间；</p>
<p>从左往右，头下标从0开始（0可省略）；从右往左，尾下标从-1开始（注意：由于左闭右开原则，-1并不会取到最后一位字符，如需取最后一位字符，需将尾下标置空）。</p>
<pre><code>a = &quot;test content&quot;
print(a[:5])
print(a[0:5])
print(a[-12:-1])
print(a[-12:])

# 判断字符是否在目标字符串中
print(&quot;te&quot; in a)
print(&quot;t&quot; not in a)

# 字符串格式化
a = &quot;%s, %d&quot; % (&quot;weq&quot;, 2)
print(a)

# 使用f-string进行格式化，可以不用%s之类的转换符（python版本需要3.6及以上）
a = &quot;content&quot;
b = f&quot;test {a}&quot;
print(b)
print(f&quot;{1+2=}&quot;)

</code></pre>
<h3 id="tuple元组">Tuple（元组）</h3>
<p>元组使用()进行赋值，使用逗号分隔不同元素；()中可以存在不同类型的元素，如数字，字符串，嵌套元组等。</p>
<p>元组的索引与截取规则与字符串一致，可参考3.2节内容。</p>
<p>注意：元组中的元素是不能被修改的；只包含一个元素时，需要在元素后面添加逗号，否则括号会被当作运算符使用</p>
<pre><code>tup = (&quot;www&quot;, 12306, 99.99, 1, 2, 3)
tup2 = (&quot;test&quot;,)
print(tup)
print(tup[:2])
print(tup[-1:])
print(tup + tup2)
print(tup * 2)
print(&quot;www&quot; in tup)

# 元组截取时，支持第三个参数：步长
print(tup[0:6:2])

</code></pre>
<h3 id="list列表">List（列表）</h3>
<p>列表与元组规则基本一致，不过列表使用[]进行赋值，且其中的元素是可以被修改的。</p>
<pre><code>list1 = [&quot;www&quot;, 12306, 99.99, 1, 2, 3]
list2 = [&quot;test&quot;]
print(list1)
print(list1[:2])
print(list1[-1:])
print(list1 + list2)
print(list1 * 2)
print(&quot;www&quot; in list1)

# 截取时，支持第三个参数：步长
print(list1[0:6:2])

# 修改列表元素
list2[0] = &quot;update&quot;
print(list2)

</code></pre>
<p>借助内置函数，列表也可以被当做堆栈（后进先出）或者队列（先进先出）使用。</p>
<p>注意：列表用作队列时相对比较低效。因为在列表的末尾添加和弹出元素非常快，但是在列表的开头插入或弹出元素却很慢 (因为所有的其他元素都必须移动一位)。</p>
<pre><code># 列表作为堆栈，利用append函数添加一个元素到堆栈的顶端，利用pop函数从堆栈顶部取出一个元素
stack = [1, 2, 3]
stack.append(4)
stack.append(5)
print(stack)
stack.pop()
print(stack)

# 列表作为队列，需要借助collections.deque操作列表两端
from collections import deque

queue = deque([1, 2, 3])
queue.append(4)
queue.append(5)
print(queue)
queue.popleft()
print(queue)

</code></pre>
<p>python中的列表推导式：列表推导式提供了一个更简单的创建列表的方法。常见的用法是把某种操作应用于序列或可迭代对象的每个元素上，然后使用其结果来创建列表，或者通过满足某些特定条件元素来创建子序列。</p>
<p>列表推导式的结构是由一对方括号所包含的以下内容组成：一个表达式，后面跟一个 for 子句，然后是零个或多个 for 或 if 子句。 其结果将是一个新列表，由对表达式依据后面的 for 和 if 子句的内容进行求值计算而得出。</p>
<p>注意：如果表达式是一个元组，那么其必须加上括号。</p>
<pre><code># 简单列表推导式
t = [x * 2 for x in range(3)]
print(t)

list_x = [1, 3, 5]
list_y = [2, 4, 6]
print([x * y for x in list_x for y in list_y])

fresh_str = ['  f   ', '  r', 'w   ']
print([x.strip() for x in fresh_str])

# 组合两个列表中的不同元素，将返回一个列表，列表中的元素类型是元组
t = [(x, y) for x in [1, 2, 3] for y in [2, 1, 3] if x != y]
print(t)

# 嵌套列表推导式
list_1 = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
t = [[row[i] for row in list_1] for i in range(4)]
print(t)

</code></pre>
<h3 id="set集合">Set（集合）</h3>
<p>集合是由不重复元素组成的无序的集。它的基本用法包括成员检测和消除重复元素。集合对象也支持像 联合，交集，差集，对称差分等数学运算。</p>
<p>使用{}花括号或<a href="https://docs.python.org/zh-cn/3.9/library/stdtypes.html#set">set()</a> 函数可以用来创建集合。注意：要创建一个空集合只能用 set() 而不能用 {}，因为后者是创建一个空字典。</p>
<pre><code># 去重
sites = {&quot;www&quot;, &quot;12306&quot;, &quot;com&quot;, &quot;www&quot;}
print(sites)
print(&quot;www&quot; in sites)

# 运算
a = set(&quot;abcd&quot;)
b = set(&quot;defg&quot;)
print(a &amp; b)
print(a | b)
print(a ^ b)
# a集合包含的元素而b集合不包含
print(a - b)

# 集合支持推导式
a = {x for x in 'abcdefg' if x not in 'abc'}
print(a)

</code></pre>
<h3 id="dictionary字典">Dictionary（字典）</h3>
<p>字典（dictionary）是Python中另一个非常有用的内置数据类型。</p>
<p>列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。</p>
<p>字典是一种映射类型，字典用 { } 标识，它是一个无序的 键(key) : 值(value) 的集合。</p>
<p>键(key)必须使用不可变类型。</p>
<p>在同一个字典中，键(key)必须是唯一的。</p>
<pre><code># 字典，类比如json，或者map，是一个kv键值对的集合
dict_test = {'name': 'hello', 'age': 100}
# 获取字典的值
print(dict_test['name'])
print(dict_test.get('age'))
# 获取键值对
for k, v in dict_test.items():
    print(k, v)
  
# 使用dict函数构造字典
d = dict([('name', 'jack'), ('age', 100)])
print(d)
d2 = dict(name='tom', age=90)
print(d2)

# 字典支持推导式
a = {x: x * 2 for x in (1, 2, 3)}
print(a)

</code></pre>
<h2 id="流程控制">流程控制</h2>
<h3 id="条件判断">条件判断</h3>
<p>if 语法如下所示：</p>
<ul>
<li>每个条件后面要使用冒号 :，表示接下来是满足条件后要执行的语句块。</li>
<li>使用缩进来划分语句块，相同缩进数的语句在一起组成一个语句块。</li>
<li>在Python中没有switch – case语句。</li>
</ul>
<pre><code># 语法格式
if condition_1:
    statement_block_1
elif condition_2:
    statement_block_2
else:
    statement_block_3
  
# 简单实例
var = 10
if var &gt; 10:
    print(&quot;value(&quot; + str(var) + &quot;) grate than 10&quot;)
elif var == 10:
    print(&quot;value(&quot; + str(var) + &quot;) equals 10&quot;)
else:
    print(&quot;value(&quot; + str(var) + &quot;) less than 10&quot;)  
  
# := 赋值表达式运算符，又称海象运算符（python3.8）
a = &quot;12&quot;
if n := len(a) &gt; 1:
    print(&quot;True&quot;)
else:
    print(&quot;False&quot;)  

</code></pre>
<h3 id="循环语句">循环语句</h3>
<p>break（终止循环）及continue（跳过本次循环）用于流程控制。pass是空语句，是为了保持程序结构的完整性。</p>
<p>while语句</p>
<pre><code># 简单例子
var = 1
while var &lt; 3:
    print(&quot;value: &quot; + str(var))
    var += 1
  
# pass占位语句
while True:
    pass  

</code></pre>
<p>for语句（break（终止循环）及continue（跳过本次循环）用于流程控制）</p>
<pre><code># 简单例子
# 列表 for循环
list1 = [&quot;first&quot;, 'second', 123, 45.6]
print(len(list1))
for var in list1:
    print(var, end=&quot;; &quot;)
  
# 获取索引及其值
for i, v in enumerate(list1):
    print(i, v)   
  
# 多个list，使用zip函数聚合
list2 = ['map', 'json', 'session', 'cookie']
for q1, q2, in zip(list1, list2):
    print('param1={0}, param2={1}'.format(q1, q2))
  
# 内置的range函数可以遍历数字序列
# 简单range，
for i in range(3):
    print(i)
# 指定区间
for i in range(1, 3):
    print(i)
# 指定区间及步长
for i in range(1, 10, 2):
    print(i)
  
# 迭代器
list3 = [&quot;good&quot;, &quot;fire&quot;, &quot;hello&quot;]
it = iter(list2)
print(next(it))
for var in it:
    print(var)  

</code></pre>
<h2 id="函数">函数</h2>
<h3 id="函数-2">函数</h3>
<p>函数是组织好的，可重复使用的，用来实现单一，或相关联功能的代码段。函数能提高应用的模块性，和代码的重复利用率。</p>
<ul>
<li>函数代码块以 def 关键词开头，后接函数标识符名称和圆括号 ()。</li>
<li>任何传入参数和自变量必须放在圆括号中间，圆括号之间可以用于定义参数。</li>
<li>函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。</li>
<li>函数内容以冒号 : 起始，并且缩进。</li>
<li>return [表达式] 结束函数，选择性地返回一个值给调用方，不带表达式的 return 相当于返回 None。</li>
</ul>
<pre><code># 可变对象传递给函数，将会改变其中的值；不可变对象传递不会改变值（如果修改，则返回一个新的变量）
def test_n(var):
    var[0] = 3

test1 = [2, 3]
test_n(test1)
print(test1)

# 必需参数须以正确的顺序传入函数。调用时的数量必须和声明时的一样。
def test_1(var):
    print(var)

test_1(1)

# 关键字参数和函数调用关系紧密，函数调用使用关键字参数来确定传入的参数值。
# 使用关键字参数允许函数调用时参数的顺序与声明时不一致，因为 Python 解释器能够用参数名匹配参数值。
def test_2(var1, var2):
    print(var1, var2)

test_2(var2=2, var1=1)

# 默认参数：调用函数时，如果没有传递参数，则会使用默认参数
# 默认参数最好使用不可变数据类型，否则可能会出现超出预期的场景
def test_3(var1, var2=2):
    print(var1, var2)

test_3(1)

# 不定长参数: 当需要一个函数能处理比当初声明时更多的参数时，这些参数就叫做不定长参数
# *表示一个元组
def test_4(var1, *var2):
    print(var1)
    print(var2)

test_4(1, 10, 20, 30)

# **表示一个字典
def test_4(var1, **var2):
    print(var1)
    print(var2)

test_4(1, a=10, b=20, c=30)

# 文档字符串
# 第一行应该是对象目的的简要概述。为简洁起见，它不应显式声明对象的名称或类型，因为这些可通过其他方式获得（除非名称恰好是描述函数操作的动词）。这一行应以大写字母开头，以句点结尾。
# 如果文档字符串中有更多行，则第二行应为空白，从而在视觉上将摘要与其余描述分开。后面几行应该是一个或多个段落，描述对象的调用约定，它的副作用等。
def test_5():
    &quot;&quot;&quot;Do nothing, but document it.

    No, really, it doesn't do anything
    &quot;&quot;&quot;
    print(&quot;test&quot;)

print(test_5.__doc__)

</code></pre>
<h3 id="匿名函数">匿名函数</h3>
<p>python 使用 lambda 来创建匿名函数。</p>
<p>所谓匿名，意即不再使用 def 语句这样标准的形式定义一个函数。</p>
<ul>
<li>lambda 只是一个表达式，函数体比 def 简单很多。</li>
<li>lambda的主体是一个表达式，而不是一个代码块。仅仅能在lambda表达式中封装有限的逻辑进去。</li>
<li>lambda 函数拥有自己的命名空间，且不能访问自己参数列表之外或全局命名空间里的参数。</li>
<li>虽然lambda函数看起来只能写一行，却不等同于C或C++的内联函数，后者的目的是调用小函数时不占用栈内存从而增加运行效率。</li>
</ul>
<pre><code># lambda函数
# 语法
lambda [arg1 [,arg2,.....argn]]:expression

# 简单例子
sum_result = lambda a1, a2: a1 + a2
print(sum_result(2, 3))

</code></pre>
<h2 id="错误与异常">错误与异常</h2>
<h3 id="异常">异常</h3>
<p>即使语句或表达式在语法上是正确的，但在尝试执行时，它仍可能会引发错误。在执行时检测到的错误被称为异常，异常不一定会导致严重后果。</p>
<pre><code># 简单异常
print(3 / 0)
# 输出
Traceback (most recent call last):
  File &quot;D:/knowledge/python/pycharm/pythonProject/learn/test.py&quot;, line 123, in &lt;module&gt;
    print(3 / 0)
ZeroDivisionError: division by zero

# 处理异常
def test_6(x, y):
    try:
        x / y
    except Exception as msg:
        print(msg)

test_6(3, 0)

# try finally， finally语句中的代码始终都会执行
def test_7(x, y):
    try:
        x / y
    except ZeroDivisionError as msg:
        print(msg)
    finally:
        print(&quot;finally exec&quot;)

test_7(3, 0)

# raise抛出异常
raise Exception(&quot;raise exception&quot;)

</code></pre>
<p>一个 <a href="https://docs.python.org/zh-cn/3.9/reference/compound_stmts.html#try">try</a> 语句可能有多个 except 子句，以指定不同异常的处理程序。 最多会执行一个处理程序。 处理程序只处理相应的 try 子句中发生的异常，而不处理同一 try 语句内其他处理程序中的异常。 一个 except 子句可以将多个异常命名为带括号的元组。</p>
<pre><code># 一个except多个异常
try:
    expression
except (RuntimeError, OSError, TypeError):
    pass
  
# 多个except
# 最后的 except 子句可以省略异常名，以用作通配符。但请谨慎使用，因为以这种方式很容易掩盖真正的编程错误！
# 它还可用于打印错误消息，然后重新引发异常（同样允许调用者处理异常）   
import sys
try:
    f = open('myfile.txt')
    s = f.readline()
    i = int(s.strip())
except OSError as err:
    print(&quot;OS error: {0}&quot;.format(err))
except ValueError:
    print(&quot;Could not convert data to an integer.&quot;)
except:
    print(&quot;Unexpected error:&quot;, sys.exc_info()[0])
    raise 

</code></pre>
<p>使用with语句可以自动关闭流</p>
<pre><code># with关键字会自动关闭文件,try捕获异常输出
try:
    with open('/opt/pythonDir/temp.txt', 'w+') as f:
        f.write(&quot;test content&quot;)
    print(f.close())
except IOError as err:
    print(&quot;exception: {0}&quot;.format(err))

</code></pre>
<h2 id="面向对象">面向对象</h2>
<h3 id="python中的类">python中的类</h3>
<p>类实例化后，可以使用其属性，实际上，创建一个类之后，可以通过类名访问其属性。</p>
<pre><code># 创建类
class FirstClass:
    name = &quot;python&quot;

    def fn(self):
        print(self.name)
        return &quot;hello world&quot;


# 实例化类
first_class = FirstClass()
# 调用变量
print(&quot;name: &quot;, first_class.name)
# 调用函数
print(&quot;function: &quot;, first_class.fn())

# 定义初始化函数
class People:
    # 基本属性
    name = &quot;&quot;
    age = 0
    # 使用双下划线定义私有属性，其无法被类外部所访问
    __private_attribute = &quot;private_attribute&quot;

    def __init__(self, name, age):
        self.name = name
        self.age = age

    def speak(self):
        print(&quot;{0} speak: {1}, {2}&quot;.format(self.name, self.age, self.__private_attribute))

    def action(self):
        print(&quot;base class action&quot;)


# people = People(&quot;Jack&quot;, 100)
# people.speak()

# 继承类
class Employee(People):
    profession = &quot;&quot;

    def __init__(self, name, age, profession):
        People.__init__(self, name, age)
        self.profession = profession

    def speak(self):
        print(&quot;{0} speak: {1}, {2}&quot;.format(self.name, self.age, self.profession))

    # 重写toString方法
#   def __str__(self):
#       return &quot;Employee:{name: %s, age: %d, profession: %s}&quot; % (self.name, self.age, self.profession)

    # 重写toString方法（使用f-string格式化）
    def __str__(self):
        return f&quot;Employee:(name: {self.name}, age: {self.age}, profession: {self.profession})&quot;

teacher = Employee(&quot;Tom&quot;, 40, &quot;teacher&quot;)
teacher.speak()
teacher.action()
print(teacher.name)

json_str = json.dumps(teacher.__dict__)
print(json_str)
teacher2 = json.loads(json_str)
print(teacher2)

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lua基本知识]]></title>
        <id>https://philosopherzb.github.io/post/lua-ji-ben-zhi-shi/</id>
        <link href="https://philosopherzb.github.io/post/lua-ji-ben-zhi-shi/">
        </link>
        <updated>2022-04-02T09:10:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Lua脚本知识。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/tianjin-2185510_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="简介与安装">简介与安装</h2>
<h3 id="简介">简介</h3>
<p>Lua是一个强大，高效且轻量化的嵌入式脚本语言，由 clean C（标准 C 和 C++ 间共通的子集）实现成一个库，支持过程编程，面向对象编程，函数编程以及数据驱动编程，以此来供任何需要的程序使用。</p>
<p>同时，作为一门扩展式语言，Lua 没有 &quot;main&quot; 程序的概念：它只能 嵌入 一个宿主程序中工作，该宿主程序被称为 被嵌入程序 或者简称 宿主 。 宿主程序可以调用函数执行一小段 Lua 代码，可以读写 Lua 变量，可以注册 C 函数让 Lua 代码调用。依靠 C 函数，Lua 可以共享相同的语法框架来定制编程语言，从而适用不同的领域。</p>
<p>这也是其设计目的：为了给应用程序提供灵活的可扩展及定制化功能。</p>
<h3 id="安装">安装</h3>
<p>linux/mac安装lua，命令如下，第四行根据系统自行选择，可在<a href="http://www.lua.org/ftp">官网地址</a>中获取最新的lua下载信息。</p>
<pre><code>curl -R -O http://www.lua.org/ftp/lua-5.*.*.tar.gz
tar zxf lua-5.*.*.tar.gz
cd lua-5.*.*
make linux/macosx test
make install

</code></pre>
<p>windows下载LuaForWindows（双击安装即可）：</p>
<ul>
<li>Github下载地址：<a href="https://github.com/rjpcomputing/luaforwindows/releases">点击此处跳转下载页面</a></li>
<li>Google下载地址 : <a href="https://code.google.com/p/luaforwindows/downloads/list">点击此处跳转下载页面</a></li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230306010.png" alt="img" loading="lazy"></figure>
<h2 id="基本概念">基本概念</h2>
<h3 id="值与类型">值与类型</h3>
<p>Lua 是一门动态类型语言，这意味着变量没有类型，只有值才有类型；值可以存储在变量中，作为参数传递或者返回。</p>
<p>Lua 中有八种基本类型：nil、boolean、number、string、function、userdata、thread 和 table</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>nil</td>
<td>NIL是值nil的类型，用于与其他值区分；通常用来表示一个有意义的值不存在时的状态。如果用于条件判断，其等同于boolean中的false。</td>
</tr>
<tr>
<td>boolean</td>
<td>包含两个值：true和false</td>
</tr>
<tr>
<td>number</td>
<td>整数及实数(浮点数)：标准Lua使用64位整数及双精度(64位)浮点数；小型机器和嵌入式系统可以使用32位整数及单精度(32位)浮点数。</td>
</tr>
<tr>
<td>string</td>
<td>不可变的字节序列(字符串)，可以使用双引号(&quot;&quot;)，单引号('')，双中括号([[]])表示</td>
</tr>
<tr>
<td>function</td>
<td>由C或者Lua编写的函数</td>
</tr>
<tr>
<td>userdata</td>
<td>存储在变量中的C语言数据，用户数据类型的值是一个内存块，分为：完全用户数据：指一块由Lua管理的内存对应的对象；轻量用户数据：一个简单的C指针。Lua可使用元表(metatable)对userdata进行操作;如若需要修改用户数据中的值，只能通过C API进行处理，这保证了数据仅被宿主机所控制。</td>
</tr>
<tr>
<td>thread</td>
<td>一个独立的执行序列，主要用于实现协程(coroutine)。注意：Lua中的线程与操作系统没有任何关系，因此，它可以为那些不支持原生线程的系统提供协程支持。线程跟协程的区别：线程可以同时多个运行，而协程任意时刻只能运行一个，并且处于运行状态的协程只有被挂起（suspend）时才会暂停。</td>
</tr>
<tr>
<td>table</td>
<td>一个关联数组，是Lua中唯一的数据结构。它可被用于表示普通数组、序列、符号表、集合、记录、图、树等等。</td>
</tr>
</tbody>
</table>
<h3 id="错误处理">错误处理</h3>
<p>由于 Lua 是一门嵌入式扩展语言，其所有行为均源于宿主程序中代码对某个 Lua 库函数的调用（如果是单独使用 Lua 时，那么Lua 程序就是宿主程序）。因此，在编译或运行 Lua 代码块的过程中，无论何时发生错误，控制权都返回给宿主，由宿主负责采取恰当的措施（比如打印错误消息）。</p>
<p>在Lua中的可以通过asset函数或者error函数处理错误。</p>
<p>asset函数首先会检查第一个参数，如果没有问题则不做任何事情，否则输出第二个参数作为错误信息。使用样例：</p>
<pre><code>-- 错误处理
local function add(i, j)
    assert(type(i) == &quot;number&quot;, &quot;i 不是一个数字&quot;)
	assert(type(j) == &quot;number&quot;, &quot;j 不是一个数字&quot;)
	return i + j
end
print(add(rd, 1))
-- 输出
lua: demo.lua:3: i 不是一个数字
stack traceback:
	[C]: in function 'assert'
	demo.lua:3: in function 'add'
	demo.lua:7: in main chunk
	[C]: ?

</code></pre>
<p>error函数：error (message [, level])，显式地抛出一个错误，内容为参数message，</p>
<p>Level参数指示获得错误的位置: Level=1[默认]：为调用error位置(文件+行号)；Level=2：指出哪个调用error的函数的函数；Level=0:不添加错误位置信息。</p>
<pre><code>error('error msg..')
--输出
lua: demo.lua:1: error msg..
stack traceback:
	[C]: in function 'error'
	demo.lua:1: in main chunk
	[C]: ?

</code></pre>
<p>如果需要在 Lua 中捕获这些错误，可以使用 <a href="https://www.bookstack.cn/read/lua-5.3/2.md#pdf-pcall">pcall</a> 或<a href="https://www.bookstack.cn/read/lua-5.3/2.md#pdf-xpcall">xpcall</a>在 保护模式 下调用一个函数。</p>
<p>使用xpcall或pcall时，需要提供一个 消息处理函数 用于错误抛出时调用。该函数需接收原始的错误消息，并返回一个新的错误消息。</p>
<p>它在错误发生后栈尚未展开时调用，因此可以利用栈来收集更多的信息，比如通过探知栈来创建一组栈回溯信息。同时，该处理函数也处于保护模式下，所以该函数内发生的错误会再次触发它（递归）。如果递归太深，Lua 会终止调用并返回一个合适的消息。</p>
<h3 id="垃圾收集">垃圾收集</h3>
<p>Lua采用了自动内存管理（类似于Java）。这意味着使用者不用操心新创建的对象需要的内存如何分配出来，也不用考虑在对象不再被使用后怎样释放它们所占用的内存。</p>
<p>Lua 运行了一个 垃圾收集器 来收集所有 死对象（即在 Lua 中不可能再访问到的对象）来完成自动内存管理的工作。</p>
<p>Lua 中所有用到的内存，如：字符串、表、用户数据、函数、线程、内部结构等，都服从自动管理。</p>
<p>Lua 实现了一个增量标记-扫描收集器。它使用这两个数字来控制垃圾收集循环：垃圾收集器间歇率 和 垃圾收集器步进倍率。这两个数字都使用百分数为单位（例如：值 100 在内部表示 1 ）。</p>
<p>垃圾收集器间歇率控制着收集器需要在开启新的循环前要等待多久。增大这个值会减少收集器的积极性。当这个值比 100 小的时候，收集器在开启新的循环前不会有等待。设置这个值为 200 就会让收集器等到总内存使用量达到之前的两倍时才开始新的循环。</p>
<p>垃圾收集器步进倍率控制着收集器运作速度相对于内存分配速度的倍率。增大这个值不仅会让收集器更加积极，还会增加每个增量步骤的长度。不要把这个值设得小于 100 ，那样的话收集器就工作的太慢了以至于永远都干不完一个循环。默认值是 200 ，这表示收集器以内存分配的“两倍”速工作。</p>
<p>如果将步进倍率设为一个非常大的数字（比程序可能用到的字节数还大 10% ），收集器的行为就像一个 stop-the-world 收集器。接着若把间歇率设为 200 ，收集器的行为就和过去的 Lua 版本一样了：每次 Lua 使用的内存翻倍时，就做一次完整的收集。</p>
<p>通过在 C 中调用 lua_gc或在 Lua 中调用collectgarbage ([opt [, arg]])来控制自动内存管理。通过参数 opt 它提供了一组不同的功能：</p>
<ul>
<li>collectgarbage(&quot;collect&quot;): 做一次完整的垃圾收集循环。</li>
<li>collectgarbage(&quot;count&quot;): 以 K 字节数为单位返回 Lua 使用的总内存数。 这个值有小数部分，所以只需要乘上 1024 就能得到 Lua 使用的准确字节数（除非溢出）。</li>
<li>collectgarbage(&quot;restart&quot;): 重启垃圾收集器的自动运行。</li>
<li>collectgarbage(&quot;setpause&quot;): 将 arg 设为收集器的 间歇率。 返回 间歇率 的前一个值。</li>
<li>collectgarbage(&quot;setstepmul&quot;): 返回 步进倍率 的前一个值。</li>
<li>collectgarbage(&quot;step&quot;): 单步运行垃圾收集器。 步长&quot;大小&quot;由 arg 控制。 传入 0 时，收集器步进（不可分割的）一步。 传入非 0 值， 收集器收集相当于 Lua 分配这些多（K 字节）内存的工作。 如果收集器结束一个循环将返回 true 。</li>
<li>collectgarbage(&quot;stop&quot;): 停止垃圾收集器的运行。 在调用重启前，收集器只会因显式的调用运行。</li>
<li>collectgarbage(&quot;isrunning&quot;): 返回表示收集器是否工作的布尔值。</li>
</ul>
<pre><code>demoTable = {&quot;LUa&quot;, &quot;shell&quot;, &quot;python&quot;, &quot;Java&quot;, &quot;ruby&quot;}
print(collectgarbage(&quot;count&quot;))
demoTable = nil

print(collectgarbage(&quot;count&quot;))
print(collectgarbage(&quot;collect&quot;))
print(collectgarbage(&quot;count&quot;))
-- 输出
21.0859375
21.1123046875
0
19.498046875

</code></pre>
<h3 id="协程coroutine">协程（coroutine）</h3>
<p>Lua中的协程也被称作协同式多线程，代表了一段独立执行的线程。它与线程非常的类似：拥有独立的堆栈、局部变量、指令指针，同时又与其他协程共享全局变量。</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>coroutine.create()</td>
<td>创建一个协程。其唯一的参数是该协程的主函数。create 函数只负责新建一个协程并返回其句柄（一个 thread 类型的对象）；而不会启动该协程。</td>
</tr>
<tr>
<td>coroutine.resume()</td>
<td>执行一个协程。第一次调用coroutine.resume时，第一个参数应传入coroutine.create返回的线程对象，然后协程从其主函数的第一行开始执行。传递给coroutine.resume的其他参数将作为协程主函数的参数传入。协程启动之后，将一直运行到它终止或调用coroutine.yield。</td>
</tr>
<tr>
<td>coroutine.yield()</td>
<td>挂起协程，让出执行权。一般与resume配合使用。协程挂起时，对应的最近coroutine.resume函数会立刻返回，即使该挂起操作发生在内嵌函数调用中（即不在主函数，但在主函数直接或间接调用的函数内部）。在协程挂起的情况下，coroutine.resume也会返回 true，并加上传给coroutine.yield的参数。当下次重启同一个协程时，协程会接着从挂起点继续执行。此时，此前挂起点处对coroutine.yield的调用会返回，返回值为传给coroutine.resume的第一个参数之外的其他参数。</td>
</tr>
<tr>
<td>coroutine.status()</td>
<td>查看线程状态：dead，suspended，running</td>
</tr>
<tr>
<td>coroutine.warp()</td>
<td>与create类似，也是创建一个协程。不同之处在于，它不返回协程本身，而是返回一个函数。调用这个函数将启动该协程。传递给该函数的任何参数均当作coroutine.resume的额外参数。</td>
</tr>
<tr>
<td>coroutine.running()</td>
<td>返回一个running协程的协程号。</td>
</tr>
</tbody>
</table>
<p>协程样例及步骤解释：</p>
<p>1、函数foo加载，协程co创建。</p>
<p>2、第15行print执行，由coroutine.resume触发执行co协程。</p>
<p>2.1、接着打印第7行数据，对应输出结果第28行。</p>
<p>2.2、随后第8行调用函数foo，开始执行第2行，打印结果为第29行</p>
<p>2.3、最后第3行执行，协程被挂起，打印结果对应第30行。</p>
<p>注意：yield调用后的返回值为下次resume同一协程时的输入参数。对应第8行的r。</p>
<p>3、第16行执行，打印协程状态，为suspended（挂起）。</p>
<p>4、第18行执行，resume同一协程，输入参数“r”对应第8行的local r，也即第3行yield挂起后的返回值。</p>
<p>4.1、第9行打印，对应结果为第32行。</p>
<p>4.2、第10行执行，协程挂起，返回r，s两个值，作为下次resume时可输入的值。</p>
<p>5、第21行执行，resume同一协程，输入参数“x”，“y”，对应第10行的local r，s。</p>
<p>5.1、第11行执行，打印协程内容，对接结果为第35行。</p>
<p>5.2、协程结束，打印结果为第36行。</p>
<p>6、第22执行，显示协程已dead（死亡），对应结果为第37行。</p>
<p>7、第24行执行，resume协程，开始提示，协程已经dead，对应结果为第38行。</p>
<pre><code>function foo (a)
  print(&quot;foo&quot;, a)
  return coroutine.yield(2*a)
end

co = coroutine.create(function (a,b)
      print(&quot;co-body&quot;, a, b)
      local r = foo(a+1)
      print(&quot;co-body&quot;, r)
      local r, s = coroutine.yield(a+b, a-b)
      print(&quot;co-body&quot;, r, s)
      return b, &quot;end&quot;
end)

print(&quot;main&quot;, coroutine.resume(co, 1, 10))
print(coroutine.status(co))

print(&quot;main&quot;, coroutine.resume(co, &quot;r&quot;))
print(coroutine.status(co))

print(&quot;main&quot;, coroutine.resume(co, &quot;x&quot;, &quot;y&quot;))
print(coroutine.status(co))

print(&quot;main&quot;, coroutine.resume(co, &quot;x&quot;, &quot;y&quot;))
print(coroutine.status(co))

-- 输出
co-body	1	10
foo	2
main	true	4
suspended
co-body	r
main	true	11	-9
suspended
co-body	x	y
main	true	10	end
dead
main	false	cannot resume dead coroutine
dead

</code></pre>
<h2 id="基本语法">基本语法</h2>
<h3 id="变量">变量</h3>
<p>在Lua中的进行赋值操作，是不需要指定类型的。如下：</p>
<pre><code>-- 简单赋值
z = 1
print(z)
-- 多重赋值，同时也支持多重返回值
a,b = 3,4
print(a)
print(b)
return a,b

-- 本地（局部）变量赋值
local x,y=5,6
print(x)
print(y)
-- 交换数据
x,y = y,x
print(x)
print(y)

-- 代码块
do
    local str = &quot;world&quot;
	print(str)
end

</code></pre>
<h3 id="表达式">表达式</h3>
<p>在Lua中的表达式基本与高级语言相差无几，如下：</p>
<p>操作符</p>
<ol>
<li>算术运算符：+ - * / ^ (加减乘除幂)</li>
<li>关系运算符：&lt; &gt; &lt;= &gt;= == ~=</li>
<li>逻辑运算符：and or not</li>
<li>连接运算符：..</li>
</ol>
<p>有几个操作符需要注意：</p>
<ul>
<li>a ~= b 即 a 不等于 b</li>
<li>a ^ b 即 a 的 b 次方</li>
<li>a .. b 将 a 和 b 作为字符串连接</li>
</ul>
<p>优先级：</p>
<ol>
<li>^</li>
<li>not -(负号)</li>
<li>*/</li>
<li>+-</li>
<li>..</li>
<li>&lt; &gt; &lt;= &gt;= ~= ==</li>
<li>and</li>
<li>or</li>
</ol>
<h3 id="控制流">控制流</h3>
<p>Lua以if for while等来进行流程控制，具体如下所示：</p>
<pre><code>-- if语句
local num = &quot;21&quot;
if (tonumber(num)~=nil) then
    print(&quot;tonumber result: &quot;..tonumber(num))
else
    print(&quot;tonumber result is not number&quot;)
end

-- for语句
-- 三个数字分别表示初始值，终止值，步长
for i = 2, 10, 2 do
    print(i)
end
-- i，v分别表示数组对应的索引及值，注意：Lua中的数组是从1开始排序
demoTable = {&quot;LUa&quot;, &quot;shell&quot;, &quot;python&quot;, &quot;Java&quot;, &quot;ruby&quot;}
for i,v in ipairs(demoTable) do
    print(i)
    print(v)
end

-- while语句
local i = 0
while i &lt; 2 do
    print(i)
	i = i + 1
	if (i == 1) then break end
end

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MySQL日志系统]]></title>
        <id>https://philosopherzb.github.io/post/mysql-ri-zhi-xi-tong/</id>
        <link href="https://philosopherzb.github.io/post/mysql-ri-zhi-xi-tong/">
        </link>
        <updated>2022-03-19T08:07:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述MySQL日志系统，包括InnoDB引擎日志。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/hd-wallpaper-2836301_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="日志系统">日志系统</h2>
<h3 id="概要">概要</h3>
<p>日志是mysql数据库的重要组成部分，记录着数据库运行期间的各种信息；其中包括错误日志，查询日志，慢查询日志，二进制日志等。</p>
<table>
<thead>
<tr>
<th>日志类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Error log</td>
<td>mysql启动、运行、停止期间发生的问题记录</td>
</tr>
<tr>
<td>General query log</td>
<td>客户端建立连接以及期间发生的所有sql操作</td>
</tr>
<tr>
<td>Slow query log</td>
<td>记录执行时间过长或没有使用索引的查询语句</td>
</tr>
<tr>
<td>Binary log</td>
<td>记录数据库的变动操作，如insert，create，alert等语句，所以该日志也可用于主从复制。</td>
</tr>
<tr>
<td>Relay log</td>
<td>中继日志一般用于接受复制源的数据变动（主从复制时会用到）</td>
</tr>
<tr>
<td>DDL log (metadata log)</td>
<td>记录了执行DDL语句的元数据操作</td>
</tr>
<tr>
<td>Engine log</td>
<td>引擎会有自己的额外日志</td>
</tr>
</tbody>
</table>
<h3 id="错误日志">错误日志</h3>
<p>错误日志包含了数据库启动和宕机时的记录；除此之外，它还包含服务启动、运行、宕机时的一些诊断性日志如errors,warnings和notes等。</p>
<p>一般情况下，错误日志有助于检查数据库运行状态，查看日志文件语句如下：</p>
<pre><code>show variables like '%log_error%';

</code></pre>
<h3 id="查询日志">查询日志</h3>
<p>查询日志记录了mysql服务实例的所有操作，如select,delete,insert等；一般情况，该日志不会被打开，避免造成mysql性能下降。查看该日志文件的语句如下：</p>
<pre><code>show variables like '%general_log%';
</code></pre>
<h3 id="慢查询日志">慢查询日志</h3>
<p>慢查询日志由执行时间超过long_query_time的sql语句组成，且必须有至少min_examined_row_limit行数据被检查过。</p>
<p>慢查询日志可以有效的跟踪执行时间过长或者没有使用索引的查询语句。查看语句如下：</p>
<pre><code>-- 查看日志文件
show variables like '%slow_query_log%';
-- 查看参数值
show variables like '%long_query_time%';
show variables like '%min_examined_row_limit%';

-- log_quries_not_using_indexes 是否将不使用索引的查询语句记录到慢查询日志中，无论查询速度有多快。
SET GLOBAL log_queries_not_using_indexes=ON;
show variables like 'log_queries_not_using_indexes';

-- 如果log_quries_not_using_indexes处于开启状态，那么mysql还提供了log_throttle_queries_not_using_indexes 用来控制每分钟写入多少条数据
show variables like 'log_throttle_queries_not_using_indexes';

-- 日志的输出格式，FILE或者table
show variables like 'log_output';

</code></pre>
<h3 id="中继日志">中继日志</h3>
<p>中继日志一般用于主从复制，基本结构与binlog一致。</p>
<h3 id="元数据日志">元数据日志</h3>
<p>元数据日志记录了通过数据库定义的能够影响表分区的语句操作，例如： ALTER TABLE t3 DROP PARTITION p2；mysql必须确保分区被完全移除，且该分区位于table t3的分区列表中的定义也被移除。为了防止在移除分区操作时机器宕机导致的移除失败，元数据日志便诞生了，因为其记录了具体的语句操作，mysql完全可以根据这些语句重新进行移除操作。</p>
<h3 id="二进制日志">二进制日志</h3>
<p>二进制日志记录了对mysql数据库执行更新的所有操作，需要注意的是不包括查询类操作，如select，show等；同时，某些操作本身可能并未对数据库进行修改，但这些操作仍然会被写入二进制日志，如update一条不存在的数据（update t set a=1 where a=2）。</p>
<pre><code>-- 查看二进制日志是否开启，默认情况下是关闭的
show variables like '%log_bin%';

</code></pre>
<h4 id="作用">作用</h4>
<p>二进制日志的具体用途如下：</p>
<ul>
<li>恢复(recovery)：二进制日志包含数据所有的更新操作，一旦数据库宕机，完全可以根据二进制日志进行备份恢复。如，在一个数据库全备文件恢复后，用户可以通过二进制日志进行point-in-time的恢复。</li>
<li>复制(replication)：一般与relay log配合使用，进行主从复制操作。</li>
<li>审计(audit)：可以检查该日志来判断是否有注入攻击。</li>
</ul>
<h4 id="配置">配置</h4>
<p>二进制日志默认是关闭状态，需要手动配置才能开启。mysql配置文件为/etc/my.cnf，使用vim编辑该文件，键入如下内容：</p>
<pre><code># binlog
# 指明存储文件位置
log-bin=/temp/mysql-bin.log
# 设置binlog清理时间
expire-logs-days=14
# 每个文件的大小
max-binlog-size=512M
# mysql集群的服务id
server-id=1

</code></pre>
<pre><code># 参数说明
log_bin：设置此参数表示启用binlog功能，并指定路径名称

log_bin_index：设置此参数是指定二进制索引文件的路径与名称

expire-logs-days：设置binlog清理时间（手动清理：purge master logs before '2010-02-16 00:00:00';）

binlog_do_db：此参数表示只记录指定数据库的二进制日志

binlog_ignore_db：此参数表示不记录指定的数据库的二进制日志

max_binlog_cache_size：此参数表示binlog使用的内存最大的尺寸

binlog_cache_size：此参数表示binlog使用的内存大小，可以通过状态变量binlog_cache_use和binlog_cache_disk_use来帮助测试。
binlog_cache_use：使用二进制日志缓存的事务数量
binlog_cache_disk_use:使用二进制日志缓存但超过binlog_cache_size值并使用临时文件来保存事务中的语句的事务数量

max_binlog_size：Binlog最大值，最大和默认值是1GB，该设置并不能严格控制Binlog的大小，尤其是Binlog比较靠近最大值而又遇到一个比较大事务时，
为了保证事务的完整性，不可能做切换日志的动作，只能将该事务的所有SQL都记录进当前日志，直到事务结束

</code></pre>
<pre><code># binlog刷盘策略参数配置
sync_binlog：这个参数直接影响mysql的性能和完整性
sync_binlog=0：当事务提交后，Mysql仅仅是将binlog_cache中的数据写入Binlog文件，但不执行fsync操作，而是让Filesystem自行决定什么时候来做同步。此模式相对来说性能最高，但不安全，宕机时容易丢失数据。
sync_binlog=1：每次事务提交时，都将binlog刷入磁盘。此模式最安全，但性能相对偏低。
sync_binlog=n：在进行n次事务提交以后，Mysql将执行一次fsync之类的磁盘同步指令，通知文件系统将Binlog文件缓存刷新到磁盘。如果容许出现数据丢失，可以适当的提高此设置值来获取更好的性能。

</code></pre>
<h4 id="格式">格式</h4>
<p>mysql5.1版本开始引入了额外的binlog格式参数，分别为：STATEMENT，ROW，MIXED。</p>
<pre><code>-- 查看binlog格式
show variables like '%binlog_format%';
</code></pre>
<ul>
<li>STATEMENT：记录的是逻辑SQL语句，即所有的数据库更新操作。该模式下日志量相对ROW会更少，节省了IO及存储资源，性能相对有所提高。注意：在RC(Read committed，读已提交)隔离级别下，此模式会导致主从复制数据不一致问题（例如：一条删除语句与一条新增语句在两个session中提交，最终master可能是先删除后新增，但slave库复制时可是先新增后删除，导致数据不一致）。解决方案便是RC选择ROW模式，RR（可重复读有间隙锁防止session执行顺序错乱）选择STATEMENT模式。</li>
<li>ROW：记录表行数据的变更情况，而非单纯的逻辑sql语句。如果binlog格式设置为ROW，那么隔离级别可以设置为RC，以提高并发度；当然随之而来的便是IO及存储资源的增加。</li>
<li>MIXED：默认依旧使用STATEMENT格式，只在某些情况下使用ROW格式；如：NDB引擎，insert delay语句，临时表，自定义函数等。</li>
</ul>
<h3 id="引擎日志">引擎日志</h3>
<p>事务一般都有ACID四个特性，其中隔离性可以通过锁来实现；原子性，一致性，持久性则需要通过日志系统进行保障。以InnoDB为例，redo log保证了持久性，undo log则保证了原子性。</p>
<p>InnoDB架构官网链接：<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-architecture.html">点击此处跳转官网页面</a></p>
<h4 id="redo-log">redo log</h4>
<p>在mysql中，持久性作为事务四大特性之一，需要确保对数据的修改能够永久地保存下来；如果在每次更新数据时都简单地采用直接刷入磁盘的操作，将会导致整个服务性能变得低下。以InnoDB引擎为例，原因如下：</p>
<ul>
<li>InnoDB引擎与磁盘进行交互的基本单位是数据页，一次事务操作可能只修改了几个字节，这个时候将整个数据页刷入磁盘，将会浪费大量的资源。</li>
<li>一个事务操作可能涉及多个不同的数据页，且当这些数据页在磁盘上不连续时，写入性能也将变得很差（随机IO多了寻址(seek)步骤）</li>
</ul>
<p>于是，InnoDB引擎设计了redo log专门记录着事务操作引起的数据变化，确切地说是记录了事务操作对数据页做了哪些修改。</p>
<p>这个日志与磁盘配合的整个过程，在MySQL中被称之为WAL(Write-Ahead Logging，预写式技术)；WAL会先将记录写入日志，然后在系统空闲的时候或者按照设定的更新策略进行刷磁盘操作(刷脏页，fsync)。</p>
<h5 id="redo-log记录形式">redo log记录形式</h5>
<p>redo log本身只记录数据页的变更，且采用了大小固定，循环写入的实现方案进行log记录。为了实现循环写入，redo log中设置了两个标志位：checkpoint 和 write pos。</p>
<ul>
<li>write pos 表示redo log当前记录的LSN(log sequence number,日志序列号：日志空间中每条日志的结束点，用字节偏移量表示)，即记录写入位置。</li>
<li>checkpoint 表示脏页（缓存中的数据页被称为脏页）刷盘后对应的redo log所处的LSN，即记录擦除位置。</li>
</ul>
<p>在进行log记录时，如果write pos追上了checkpoint，那么就表示redo log已经写满了；此时需要停止写入，并运行checkpoint规则进行刷磁盘操作（先更新内存，再将buffer中的脏数据（缓存中的数据被称为脏数据）fsync到磁盘）。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230306004.png" alt="img" loading="lazy"></figure>
<h5 id="redo-log刷磁盘">redo log刷磁盘</h5>
<p>redo log包含两个核心部分，分别是内存中的日志缓冲(redo log buffer)以及磁盘上的日志文件(redo log file)。当执行一条更新sql时，数据会先写入redo log buffer，之后再写入redo log file。</p>
<p>在计算机操作系统中，用户空间(user space)下的缓存区数据是无法直接写入磁盘的，一般都需要经过内核空间(kernel space)的缓存区(OS Buffer)，之后才能真正写入磁盘。具体流程图如下所示：</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230306005.png" alt="" loading="lazy"></figure>
<p>在InnoDB中有一个配置参数可以用来控制日志的刷新频率：innodb_flush_log_at_trx_commit。</p>
<table>
<thead>
<tr>
<th>参数值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>0（延时写）</td>
<td>事务提交之后，日志只记录到 log buffer 中，之后每秒写一次日志到缓存（OS Buffer）并刷新（fsync）到磁盘，尚未刷新的日志可能会丢失。</td>
</tr>
<tr>
<td>1（实时写，实时刷）</td>
<td>事务每次提交都会将log buffer中的日志写入os buffer并调用fsync()刷到log file中。这种方式即使系统崩溃也不会丢失任何数据，但是因为每次提交都写入磁盘，IO的性能较差。</td>
</tr>
<tr>
<td>2（实时写，延时刷）</td>
<td>每次事务提交之后，日志写到 OS Buffer，每秒刷一次到磁盘，尚未刷新的日志可能会丢失。</td>
</tr>
</tbody>
</table>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230306007.png" alt="img" loading="lazy"></figure>
<h5 id="redo-log与binlog">redo log与binlog</h5>
<p>binlog是mysql服务共有的日志文件，而redo log则是InnoDB独有的日志文件。同时，redo log是基于crash recovery，保证MySQL宕机后的数据恢复（crash-safe）；而binlog是基于point-in-time recovery，保证服务器可以基于时间点对数据进行恢复，或者对数据进行备份。</p>
<p>注意：单纯的binlog日志系统是不具备crash-safe功能的，因为binlog一般只能提供归档功能（记录了对mysql数据库执行更新的所有操作）。</p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230306008.png" alt="img" loading="lazy"></figure>
<p>以InnoDB引擎对update table set b = 2 where a = 1语句操作为例，其中都会涉及redo log日志和binlog日志，具体流程如下：</p>
<ol>
<li>查询 a=1 这一行。如果 a=1 这一行所在的数据页本来就在内存中，便直接返回；否则，需要先从磁盘读入内存，然后再返回。</li>
<li>修改 a=1 这行的b为2，并写入新行。</li>
<li>引擎将这行新数据更新到内存（InnoDB Buffer Pool）中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。</li>
<li>sql语句写入 binlog，并把 binlog 刷入磁盘（依据配置的刷盘机制）。</li>
<li>执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230306009.png" alt="img" loading="lazy"></figure>
<h4 id="undo-log">undo log</h4>
<p>回滚日志主要是为了保证数据库事务的原子性，当一个事务对数据库进行修改时，InnoDB引擎不仅会记录redo log，还会生成对应的undo log日志；当事务执行失败或者调用rollback时，便可以利用undo log将数据回滚到修改之前的样子。</p>
<p>关于 undo Log 的存储：InnoDB 中有回滚段(rollback segment)，每个回滚段记录 1024 个 undo log segment，在每个 undo log segment 段中进行申请 undo 页。系统表空间偏移量为 5 的页记录了所有的 rollback segment header 所在的页。</p>
<p>undo log有两个作用：一是提供回滚，二是实现MVCC；两种格式：insert undo log（记录插入对应的回滚日志） 和 update undo log（记录更新和删除对应的回滚日志）。</p>
<p>undo log详解可参考：<a href="http://mysql.taobao.org/monthly/2021/10/01/">点击此处跳转页面</a></p>
<h4 id="innodb故障恢复">InnoDB故障恢复</h4>
<p>由上小节可知，InnoDB事务处理采用的是二阶段提交，因此故障恢复也将依据二阶段进行。</p>
<p>注意1：binlog的完整性依据不同的日志格式而不同，STATEMENT格式为COMMIT，而ROW格式最后一行会有一个XID event，且5.6.2版本后，mysql还新增了binlog-checknum用于验证binlog的完整性。</p>
<p>注意2：redo log与binlog有一个共同的数据字段：XID。</p>
<ol>
<li>数据库启动后，InnoDB引擎会根据redo log寻找最近的一次checkpoint位置，随后根据checkpoint对应的LSN(log sequence number,日志序列号)获取需要重做的日志。</li>
<li>根据redo log回滚未prepared和commit的事务，但对于已经prepared，但未commit的事务，暂时挂起，将其保存到一个链表中。</li>
<li>mysql读取最后一个binlog文件。binlog文件通常是以固定的文件名加一组连续的编号来命名的，并且将其记录到一个binlog索引文件中，因此索引文件中的最后一个binlog文件即是MySQL将要读取的最后一个binlog文件。</li>
<li>如果binlog中记录了上次mysql为异常关闭（文件头是否存在标记LOG_EVENT_BINLOG_IN_USE_F），则依次读取binlog中所有的log event，并将所有已提交事务的xid取出归总到一个列表；同时，定位出最后一个完整事务的位置。</li>
<li>遍历第二步中的列表，判断其是否在第四步中的提交事务列表中，如果是，则提交此事务；否则回滚。</li>
<li>将最后一个完整事务位置之后的binlog清除，到此故障恢复便已全部完成。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MySQL架构与EXPLAIN与锁]]></title>
        <id>https://philosopherzb.github.io/post/mysql-jia-gou-yu-explain-yu-suo/</id>
        <link href="https://philosopherzb.github.io/post/mysql-jia-gou-yu-explain-yu-suo/">
        </link>
        <updated>2022-03-05T02:51:12.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述MySQL架构组件，EXPLAIN语法分析以及各类锁细节。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/sea-164989_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="mysql架构">MySQL架构</h2>
<h3 id="架构图">架构图</h3>
<h4 id="国外架构图">国外架构图</h4>
<p>图片文章链接：<a href="https://www.rathishkumar.com/2016/04/understanding-mysql-architecture.html">点击此处跳转页面</a></p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230306001.png" alt="" loading="lazy"></figure>
<h4 id="国内架构图">国内架构图</h4>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230306002.png" alt="img" loading="lazy"></figure>
<h3 id="架构层说明">架构层说明</h3>
<h4 id="客户端层">客户端层</h4>
<p>mysql是一个标准的cs(client-server)服务器，请求一般由客户主动发起，包括但不限于java，python，php等。</p>
<h4 id="服务端层">服务端层</h4>
<p>服务层包含了mysql所有的核心功能，具体可分为如下几种：</p>
<ul>
<li>连接器(Connection Pool)：维护客户端与服务端的连接，认证连接账号密码，确认连接账号的权限。</li>
<li>查询缓存(Query Cache)：缓存了select语法的结果集，如果命中了缓存则直接返回数据给客户端。注意：mysql8此功能已弃用。</li>
<li>分析器(Parser)：对即将执行的sql进行分析，包括词法分析(Lexical analysis，判断从字节流生成的单词或标识是否符合规范)和语法分析(Syntactic analys，确保语句符合sql规范)；同时，会创建一个内部的parse-tree结构。</li>
<li>优化器(Optimizer)：针对内部的parse-tree，mysql可以应用多种优化技术，如重写查询，扫描表的顺序以及选择合适的索引使用等。可以用explain查看分析优化结果。</li>
</ul>
<h4 id="引擎层">引擎层</h4>
<p>mysql提供给可插拔式的引擎技术，可根据不同的业务选择不同的引擎。</p>
<h4 id="存储层">存储层</h4>
<p>实际的物理存储介质。</p>
<h3 id="explain语句">EXPLAIN语句</h3>
<p>explain可以与select，delete，insert，replace以及update语句一同使用，结果会显示来自优化器（Optimizer）的有关语句执行计划的信息。</p>
<p>官网链接：<a href="https://dev.mysql.com/doc/refman/8.0/en/explain-output.html">点击此处跳转官网页面</a></p>
<p>explain输出格式参数如下所示：</p>
<table>
<thead>
<tr>
<th>Column</th>
<th>JSON Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>select_id</td>
<td>select 标识符</td>
</tr>
<tr>
<td>select_type</td>
<td>None</td>
<td>select 类型</td>
</tr>
<tr>
<td>table</td>
<td>table_name</td>
<td>输出行对应的表名</td>
</tr>
<tr>
<td>partitions</td>
<td>partitions</td>
<td>匹配的分区</td>
</tr>
<tr>
<td>type</td>
<td>access_type</td>
<td>连接/访问类型</td>
</tr>
<tr>
<td>possible_keys</td>
<td>possible_keys</td>
<td>可供选择的匹配索引</td>
</tr>
<tr>
<td>key</td>
<td>key</td>
<td>实际匹配的索引</td>
</tr>
<tr>
<td>key_len</td>
<td>key_length</td>
<td>所选key的长度</td>
</tr>
<tr>
<td>ref</td>
<td>ref</td>
<td>与索引比较的列</td>
</tr>
<tr>
<td>rows</td>
<td>rows</td>
<td>预计要检查多少行</td>
</tr>
<tr>
<td>filtered</td>
<td>filtered</td>
<td>通过表条件过滤的行百分比</td>
</tr>
<tr>
<td>Extra</td>
<td>None</td>
<td>额外信息</td>
</tr>
</tbody>
</table>
<h4 id="idjson-name-select_id">id(JSON name: select_id)</h4>
<p>select 标识符，是一次查询中select的序列号。当行引用了另一行的联合结果时，此值可以为NULL。当然，它也可以显示联合行的结果，格式类似于：union M,N。其中M，N表示不同的行。</p>
<h4 id="select_typejson-name-none">select_type(JSON name: None)</h4>
<p>select类型，具体如下表所示：</p>
<table>
<thead>
<tr>
<th>select_type Value</th>
<th>JSON Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>SIMPLE</td>
<td>None</td>
<td>简单查询（未使用联合查询及子查询）</td>
</tr>
<tr>
<td>PRIMARY</td>
<td>None</td>
<td>多层查询时，外层查询将被标记为primary（如两表做UNION或者存在子查询的外层的表操作为PRIMARY，内层的操作为UNION）</td>
</tr>
<tr>
<td>UNION</td>
<td>None</td>
<td>UNION操作中，查询中处于内层的SELECT（内层的SELECT语句与外层的SELECT语句没有依赖关系）</td>
</tr>
<tr>
<td>DEPENDENT UNION</td>
<td>dependent (true)</td>
<td>UNION操作中，查询中处于内层的SELECT（内层的SELECT语句与外层的SELECT语句有依赖关系）</td>
</tr>
<tr>
<td>UNION RESULT</td>
<td>union_result</td>
<td>union查询结果</td>
</tr>
<tr>
<td>SUBQUERY</td>
<td>None</td>
<td>子查询</td>
</tr>
<tr>
<td>DEPENDENT SUBQUERY</td>
<td>dependent (true)</td>
<td>依赖于外部查询的子查询</td>
</tr>
<tr>
<td>DERIVED</td>
<td>None</td>
<td>衍生表</td>
</tr>
<tr>
<td>DEPENDENT DERIVED</td>
<td>dependent (true)</td>
<td>依赖于另一张表的衍生表</td>
</tr>
<tr>
<td>MATERIALIZED</td>
<td>materialized_from_subquery</td>
<td>物化子查询</td>
</tr>
<tr>
<td>UNCACHEABLE SUBQUERY</td>
<td>cacheable (false)</td>
<td>对于外层的主表，子查询不可被物化，每次都需要计算（耗时操作）</td>
</tr>
<tr>
<td>UNCACHEABLE UNION</td>
<td>cacheable (false)</td>
<td>UNION操作中，内层的不可被物化的子查询（类似于UNCACHEABLE SUBQUERY）</td>
</tr>
</tbody>
</table>
<h4 id="tablejson-name-table_name">table(JSON name: table_name)</h4>
<p>输出行所引用的表。除此之外，它还包括联合查询：union M,N（表示连表查询）；衍生表：derived M（M表的衍生结果，例如子查询的from结果）；子查询：subquery M（M表的子查询结果）。</p>
<h4 id="typejson-name-access_type">type(JSON name: access_type)</h4>
<p>表的连接查询方式，性能从高到低如下表所示：</p>
<table>
<thead>
<tr>
<th>type value</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>system</td>
<td>表中只有一行，算是一种特殊的const连接类型</td>
</tr>
<tr>
<td>const</td>
<td>单表中最多有一个匹配行，一般是primary key 或者 unique index的检索</td>
</tr>
<tr>
<td>eq_ref</td>
<td>多表连接中被驱动表的连接列上有primary key或者unique index的检索</td>
</tr>
<tr>
<td>ref</td>
<td>与eq_ref类似，但不是使用primary key或者unique index，而是普通索引。也可以是单表上non-unique索引检索</td>
</tr>
<tr>
<td>fulltext</td>
<td>使用FULLTEXT索引执行连接</td>
</tr>
<tr>
<td>ref_or_null</td>
<td>与ref类似，区别在于条件中包含对NULL的查询</td>
</tr>
<tr>
<td>index_merge</td>
<td>索引合并优化，利用一个表里的N个索引查询,key包含索引列表，key_len则表示这些索引键的最长长度。</td>
</tr>
<tr>
<td>unique_subquery</td>
<td>in的后面是一个查询primary key\unique字段的子查询，即子查询中使用eq_ref类型查询。</td>
</tr>
<tr>
<td>index_subquery</td>
<td>in的后面是一个查询普通index字段的子查询，即子查询中使用了ref类型查询。</td>
</tr>
<tr>
<td>range</td>
<td>单表索引中的范围查询,使用索引查询出单个表中的一些行数据。ref列会变为null</td>
</tr>
<tr>
<td>index</td>
<td>等于ALL。它有两种情况：(1)覆盖索引（Extra列会显示 Using index） (2)用索引的顺序做一个全表扫描。</td>
</tr>
<tr>
<td>all</td>
<td>全表扫描</td>
</tr>
</tbody>
</table>
<pre><code>-- 样例
-- const
SELECT * FROM tbl_name WHERE primary_key=1;
SELECT * FROM tbl_name
  WHERE primary_key_part1=1 AND primary_key_part2=2;

-- eq_ref
SELECT * FROM ref_table,other_table
  WHERE ref_table.key_column=other_table.column;
SELECT * FROM ref_table,other_table
  WHERE ref_table.key_column_part1=other_table.column
  AND ref_table.key_column_part2=1;

-- ref
SELECT * FROM ref_table WHERE key_column=expr;
SELECT * FROM ref_table,other_table
  WHERE ref_table.key_column=other_table.column;
SELECT * FROM ref_table,other_table
  WHERE ref_table.key_column_part1=other_table.column
  AND ref_table.key_column_part2=1;

-- ref_or_null
SELECT * FROM ref_table
  WHERE key_column=expr OR key_column IS NULL;
  
-- unique_subquery
value IN (SELECT primary_key FROM single_table WHERE some_expr)  

-- index_subquery
value IN (SELECT key_column FROM single_table WHERE some_expr)

-- range
SELECT * FROM tbl_name
  WHERE key_column = 10;
SELECT * FROM tbl_name
  WHERE key_column BETWEEN 10 and 20;
SELECT * FROM tbl_name
  WHERE key_column IN (10,20,30);
SELECT * FROM tbl_name
  WHERE key_part1 = 10 AND key_part2 IN (10,20,30);

</code></pre>
<h4 id="extrajson-name-none">Extra(JSON name: None)</h4>
<p>包含了mysql解析查询的一些额外信息。</p>
<table>
<thead>
<tr>
<th>type value</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Backword index scan</td>
<td>优化器可以在InnoDB引擎中使用降序索引，一般与Using index一同出现</td>
</tr>
<tr>
<td>const row not found</td>
<td>所要查询的表为空</td>
</tr>
<tr>
<td>Distinct</td>
<td>mysql正在查询distinct值，因此当它查到匹配行后便会停止继续搜索更多行。</td>
</tr>
<tr>
<td>impossible HAVING</td>
<td>having条件总为false，且无法搜索到任何行</td>
</tr>
<tr>
<td>Impossible WHERE</td>
<td>where条件总为false，且无法搜索到任何行</td>
</tr>
<tr>
<td>Impossible WHERE noticed after reading const tables</td>
<td>mysql读取所有的const（及system）表之后，发现where条件均不满足（即where条件为false）</td>
</tr>
<tr>
<td>no matching row in const table</td>
<td>对于一个连接查询，结果是一个空表或者没有一条满足唯一索引条件的行。</td>
</tr>
<tr>
<td>Not exists</td>
<td>优化器发现内表记录不可能满足where条件（left join，如：SELECT * FROM t1 LEFT JOIN t2 ON t1.id=t2.id WHERE t2.id IS NULL;）</td>
</tr>
<tr>
<td>Using filesort</td>
<td>MySQL 必须执行额外的检查以找出如何按排序顺序检索行。排序是通过根据连接类型遍历所有行并存储排序键和指向与WHERE子句匹配的所有行的指针来完成的。然后对键进行排序，并按排序顺序检索行</td>
</tr>
<tr>
<td>Using index</td>
<td>使用索引树中的信息从表中检索列信息，无须额外操作。（覆盖索引）</td>
</tr>
<tr>
<td>Using index for group-by</td>
<td>与Using index类似，对于group by列或者distinct列，可以利用索引检索出数据，而不需要去表里查数据、分组、排序、去重等等</td>
</tr>
<tr>
<td>Using join buffer</td>
<td>之前的表连接在nested loop之后放进join buffer，再来和本表进行join。</td>
</tr>
<tr>
<td>Using sort_union,using union,using intersect</td>
<td>index_merge的三种情况</td>
</tr>
<tr>
<td>Using temporary</td>
<td>使用了临时表来存储中间结果集，适用于group by，distinct，或order by列为不同表的列。</td>
</tr>
<tr>
<td>Using where</td>
<td>在存储引擎层检索出记录后，在server利用where条件进行过滤，并返回给客户端</td>
</tr>
</tbody>
</table>
<h2 id="mysql锁">MySQL锁</h2>
<h3 id="隔离级别">隔离级别</h3>
<table>
<thead>
<tr>
<th>Isolation Level（隔离级别）</th>
<th>Dirty Reads（脏读）</th>
<th>Non-Repeatable Reads（不可重复读）</th>
<th>Phantom Reads（幻读）</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read uncommitted（读未提交）</td>
<td>允许</td>
<td>允许</td>
<td>允许</td>
</tr>
<tr>
<td>Read committed(Sql server, Oracle)（读已提交）</td>
<td>不允许</td>
<td>允许</td>
<td>允许</td>
</tr>
<tr>
<td>Repeatable reads(Mysql)（可重复读）</td>
<td>不允许</td>
<td>不允许</td>
<td>允许</td>
</tr>
<tr>
<td>Serializable（串行化）</td>
<td>不允许</td>
<td>不允许</td>
<td>不允许</td>
</tr>
</tbody>
</table>
<ul>
<li>Dirty reads：脏读，A事务可以读到B事务还未提交的数据。</li>
<li>Non-repeatable read：不可重复读，A事务读取一行数据，B事务后续修改了这行数据，A事务再次读取这行数据，结果得到的数据不同。</li>
<li>Phantom reads：幻读，A事务通过SELECT ... WHERE得到一些行，B事务插入新行或者删除已有的行使得这些行满足A事务的WHERE条件，A事务再次SELECT ... WHERE结果比上一次多/少了一些行。</li>
</ul>
<p>注意1：mysql默认使用RR隔离级别，这是由于binlog的格式问题（statement-记录修改的SQL语句,row-记录每行实际数据的变更,mixed-前面两种混合）所导致的，在5.0之前binlog只有statement一种格式，而主从复制时，这会导致数据的不一致。</p>
<p>注意2：其他数据库选用RC隔离级别，是由于RR隔离级别增加了间隙锁，会增加发生死锁的概率；同时，条件列未命中索引时，会锁全表，RC只会锁行。</p>
<p>注意3：RC隔离级别下，主从复制需要采用binlog的row格式，基于行的复制，这样不会出现主从不一致问题。</p>
<h3 id="锁分类">锁分类</h3>
<h4 id="按操作数据的类型">按操作数据的类型</h4>
<ul>
<li>共享锁(读锁，Shared Locks，S锁)：在事务要读取一条记录时，需要先获取该记录的S锁。S锁可以在同一时刻被多个事务同时持有；即多个读事务可以并发的进行，但不允许出现写事务。可以用select ...... lock in share mode;的方式手工加上一把S锁。</li>
<li>独占锁(写锁，排他锁，Exclusive Locks，X锁)：在事务要改动一条记录时，需要先获取该记录的X锁。X锁在同一时刻最多只能被一个事务持有；即当前写事务进行时，不允许其他读事务与写事务出现。X锁的加锁方式有两种，第一种是自动加锁，在对数据进行增删改的时候，都会默认加上一个X锁；还有一种是手工加锁，可以用一个FOR UPDATE给一行数据加上一个X锁。</li>
</ul>
<h4 id="按操作数据的粒度">按操作数据的粒度</h4>
<p>粒度：指数据仓库的数据单位中保存数据的细化或综合程度的级别。细化程度越高，粒度级就越小；相反，细化程度越低，粒度级就越大。</p>
<p>在数据库中为了获取更高的并发度，每次锁定的数据范围越小越好，即锁粒度越低越好。理论上来说，如果只锁定当前操作数据将会获得最大的并发度。</p>
<p>值得注意的是锁的管理也是需要耗费额外资源。</p>
<ul>
<li>表级锁(table-level locking)：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低（MyISAM 和 MEMORY 存储引擎采用的是表级锁）。一般适合查询为主并带有少量更新（读多写少）的应用场景。</li>
<li>行级锁(row-level locking)：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高（InnoDB 存储引擎既支持行级锁也支持表级锁，但默认情况下是采用行级锁）。一般适合大量并发更新并带有少量查询（写多读少）的应用场景。</li>
<li>页级锁(page-level locking)：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般（BDB存储引擎支持页级锁，也支持表级锁）。</li>
</ul>
<h4 id="按思想维度">按思想维度</h4>
<ul>
<li>乐观锁(Optimistic Lock)：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。乐观锁不能解决脏读的问题。乐观锁, 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会根据版本号（一般情况是版本号，也可以使用时间戳等其他控制条件）判断一下在此期间别人有没有去更新这个数据。乐观锁适用于多读的应用类型，这样可以提高吞吐量。</li>
<li>悲观锁(Pessimistic Lock)：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。悲观锁，顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁，排他锁等，都是在做操作之前先上锁。</li>
</ul>
<h3 id="innodb引擎锁">InnoDB引擎锁</h3>
<p>作为mysql服务的默认引擎，其中的锁机制自然也是值得深究的。</p>
<p>查看表锁争用情况语句：show status like 'Table%';</p>
<p>查看引擎状态语句：SHOW ENGINE engine_name {STATUS | MUTEX}；例如：SHOW ENGINE INNODB STATUS</p>
<p>官网链接：<a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html">点击此处跳转官网页面</a></p>
<h4 id="innodb表级锁">InnoDB表级锁</h4>
<ul>
<li>自增锁(AUTO-INC Locks)：是一种特殊的表级锁，它仅针对拥有AUTO_INCREMENT行的表。为了保证自增键数据的连续性，每次插入数据的时候，都会给该表加上一个自增锁，直到前一个事务执行成功，后一个事务才能执行。可以通过innodb_autoinc_lock_mode变量指定自增锁模式。</li>
<li>意向共享锁(intention shared lock，IS锁)：表明一个事务试图给表中的各个行设置共享锁；如果另一个事务要给数据行设置共享锁，则需先获取该行所在的表的IS锁；select ... for share可以设置IS锁。</li>
<li>意向独占锁(intention exclusive lock，IX锁)：表明一个事务试图给表中的各个行设置独占锁；如果另一个事务要给数据行设置独占锁，则需先获取该行所在的表的IX锁；select ... for update可以设置IX锁。</li>
</ul>
<p>关于各级别锁的兼容情形如下表所示（如果一个事务请求的锁模式与当前的锁兼容，InnoDB 就将请求的锁授予该事务；反之，如果两者不兼容，该事务就要等待锁释放）：</p>
<table>
<thead>
<tr>
<th></th>
<th>X</th>
<th>IX</th>
<th>S</th>
<th>IS</th>
</tr>
</thead>
<tbody>
<tr>
<td>X</td>
<td>冲突</td>
<td>冲突</td>
<td>冲突</td>
<td>冲突</td>
</tr>
<tr>
<td>IX</td>
<td>冲突</td>
<td>兼容</td>
<td>冲突</td>
<td>兼容</td>
</tr>
<tr>
<td>S</td>
<td>冲突</td>
<td>冲突</td>
<td>兼容</td>
<td>兼容</td>
</tr>
<tr>
<td>IS</td>
<td>冲突</td>
<td>兼容</td>
<td>兼容</td>
<td>兼容</td>
</tr>
</tbody>
</table>
<h4 id="innodb行级锁">InnoDB行级锁</h4>
<p>InnoDB中的行级锁是通过锁定索引来实现的，这意味着只有通过索引条件进行检索的语句才会被施加行级锁，否则将使用表级锁。</p>
<p>注意：只有当执行计划真正使用了索引（用explain分析），才会施加行级锁；多个不同的session如果使用了同一个索引键，将会出现锁冲突场景。</p>
<p>聚集索引：一般是主键；如果主键不存在则使用不为空的唯一索引；如果主键和非空唯一索引都不存在，将会使用InnoDB引擎内置的6字节rowId（名称：GEN_CLUST_INDEX）。</p>
<h5 id="记录锁">记录锁</h5>
<p>记录锁(Record Locks)：使用精确匹配(=)锁定一个索引记录，例如：select c1 from t where c1 = 10 for update;锁定成功后，将不允许其他事务对该条记录进行修改删除操作。</p>
<p>值得注意的是，记录锁实际锁定的是索引（主键索引，唯一索引，普通索引，联合索引等），即使表并没有定义索引（对于这种情况，InnoDB会创建一个隐式的聚集索引并使用该索引来进行记录锁定）。</p>
<p>通过主键索引或唯一索引对数据进行update操作时，会自动对该行数据添加记录锁。例如：update t_test set test_name = 'demo_name' where id = 1;</p>
<h5 id="间隙锁">间隙锁</h5>
<p>间隙锁(Gap Locks)：使用范围匹配(&gt;,&lt;,between等)并请求共享或独占锁时将会锁定一个索引间隙，例如：select c1 from t where c1 between 10 and 30 for update;锁定成功后，在(10,30)之间的c1值将不允许插入。</p>
<p>间隙锁锁定的是一个不包括边界的区间，即左开右开区间。</p>
<p>语句：SELECT * FROM table_name WHERE id = 100 FOR UPDATE;如果id不是索引，则会触发间隙锁，将100之前的区间锁定(验证时更新id=100的数据同样会被阻塞)。</p>
<p>很显然，在使用范围条件检索并锁定记录时，InnoDB这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。因此，在实际应用开发中，尤其是并发插入比较多的应用，应该尽量优化业务逻辑，使用相等条件来访问更新数据，避免使用范围条件。</p>
<p>注意：间隙锁在RC(Read committed)隔离级别下将会被禁用；在RR(Repeatable reads)隔离级别下，间隙锁可以预防幻读。</p>
<h5 id="临键锁">临键锁</h5>
<p>临键锁(Next-Key Locks)：临键锁是记录锁和间隙锁的组合。也可以称之为特殊的间隙锁，它的锁定范围是索引记录及之前的间隙，是一个左开右闭区间。</p>
<p>使用范围查询并命中了非唯一索引或非主键索引的record记录，此时锁住的就是临键区间。值得注意的是临键锁只与非唯一索引列有关，在唯一索引列（包括主键列）上不存在临键锁。</p>
<p>假设一个索引值包含10,11,13和20，那么该索引可能的临键锁区间有：(negative infinity, 10]，(10, 11]，(11, 13]，(13, 20]，(20, positive infinity)。</p>
<p>对于最后一个间隙区间，临键锁会锁定索引中最大值以上的间隙。</p>
<p>mysql默认行锁类型就是临键锁(Next-Key Locks)。当使用唯一性索引，等值查询匹配到一条记录的时候，临键锁(Next-Key Locks)会退化成记录锁；没有匹配到任何记录的时候，退化成间隙锁。</p>
<p>注意：临键锁在RC(Read committed)隔离级别下将会被禁用；在RR(Repeatable reads)隔离级别下，临键锁可以预防幻读。</p>
<h3 id="innodb-mvcc">InnoDB MVCC</h3>
<p>官网链接：<a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-multi-versioning.html">点击此处跳转官网页面</a></p>
<p>InnoDB multiversion concurrency control(MVCC，多版本并发控制)，主要是为了在RC(Read committed)，RR(Repeatable reads)隔离级别下提高数据库并发性能。</p>
<p>InnoDB是一个多版本存储引擎，它会将行变动的老版本信息存储下来用于回滚及并发控制。这种特性是基于undo log日志系统实现的；多版本的信息被存储在undo 表空间一个被称作回滚段(rollback segment)的数据结构中。</p>
<p>当进行并发读写时，InnoDB基于MVCC机制可以达到不加锁的一致性读效果，从而提高并发度；需要注意的是：基于MVCC的读可能会读到老数据，因为快照读会看到在该时间点之前提交的事务所做的更改，而不会看到稍后或未提交的事务所做的更改。</p>
<p>MVCC简单步骤：当一个读事务产生时，它会进行快照读并生成一个读视图(Read View)，随后根据read view中的记录(trx_ids)访问某个表索引上的记录，通过比较trx_id来确定事务可见性，如果不可见就沿着DB_ROLL_PTR往更老的版本寻找匹配数据。</p>
<p>如下图所示，事务R开始需要查询表t上的id为1的记录，R开始时事务I已经提交，事务J还在运行，事务K还没开始，这些信息都被记录在了事务R的ReadView中。事务R从索引中找到对应的这条Record[1, C]，对应的trx_id是K，不可见。沿着Rollptr找到Undo中的前一版本[1, B]，对应的trx_id是J，不可见。继续沿着Rollptr找到[1, A]，trx_id是I可见，返回结果。（此段及图片摘自：<a href="http://mysql.taobao.org/monthly/2021/10/01/">点击此处跳转文章</a>）</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230306003.png" alt="img" loading="lazy"></figure>
<h4 id="当前读与快照读">当前读与快照读</h4>
<ul>
<li>当前读：返回的永远是最新的数据，一般通过加锁来实现，例如select ... for update；</li>
<li>快照读：基于MVCC，并由InnoDB多版本存储引擎实现，可以让一个读事务读取多版本中可见的版本数据。</li>
<li>RR级别下快照读：RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的（除了当前事务本身的更新操作），而早于Read View提交的事务所做的修改均是可见。</li>
<li>RC级别下快照读：每次快照读时都会生成一个快照和Read View，这也就是为何可以看到其他事务提交的更新的原因。</li>
</ul>
<h4 id="innodb行内部存储字段">InnoDB行内部存储字段</h4>
<ul>
<li>DB_TRX_ID：6字节，记录了最后一次行插入或更新事务的事务标识。此外，删除在内部也会被视作更新，即在行中的一个特定位上打上已删除标记。</li>
<li>DB_ROLL_PTR：7字节，一般被称作回滚指针，它指向回滚段(rollback segment中的回滚日志记录。</li>
<li>DB_ROW_ID：6字节，行id，随着行新增而自增；如果是由InnoDB自动生成的聚集索引，则索引即为该值；否则，该值不会出现在任何索引中。</li>
</ul>
<h4 id="read-view相关属性">Read View相关属性</h4>
<ul>
<li>m_ids：表示在生成READVIEW时当前系统中活跃的读写事务的事务id列表，活跃的是指当前系统中那些尚未提交的事务。</li>
<li>m_up_limit_id：表示在生成READVIEW时当前系统中活跃的读写事务中最小的事务id，也就是trx_ids中的最小值。</li>
<li>m_low_limit_id：表示生成READVIEW时系统中应该分配给下一个事务的事务id值，由于事务id一般是递增分配的，所以max_trx_id就是trx_ids中最大的那个id再加上1。</li>
<li>m_creator_trx_id：表示生成该READVIEW的事务id，由于只有在对表中记录做改动（增删改）时才会为事务分配事务id，所以在一个读取数据的事务中的事务id默认为0。</li>
</ul>
<h4 id="read-view-可见性判断具体可参见mysql源码">Read View 可见性判断（具体可参见mysql源码）</h4>
<ul>
<li>如果trx_id = m_creator_trx_id，表示当前读事务正在读取被自己修改过的记录，该版本可以被当前事务访问。</li>
<li>如果trx_id &lt; m_up_limit_id，表明生成该版本的事务在当前事务生成READVIEW前已经提交了，所以该版本可以被当前事务访问。</li>
<li>如果trx_id &gt;= m_low_limit_id，表明生成该版本的事务在当前事务生成READVIEW后提交，所以该版本不可被当前事务访问。</li>
<li>如果 trx_id在m_low_limit_id, m_up_limit_id之间，则需要判断trx_id是否在m_ids列表中；如果存在，则表明事务处于活跃状态，此时是不可见的；否则可见。</li>
</ul>
<pre><code>源码地址：https://github.com/mysql/mysql-server/blob/8.0/storage/innobase/read/read0read.cc
/**
ReadView constructor */
ReadView::ReadView()
    : m_low_limit_id(),
      m_up_limit_id(),
      m_creator_trx_id(),
      m_ids(),
      m_low_limit_no() {
  ut_d(::memset(&amp;m_view_list, 0x0, sizeof(m_view_list)));
  ut_d(m_view_low_limit_no = 0);
}

/**
Opens a read view where exactly the transactions serialized before this
point in time are seen in the view.
@param id		Creator transaction id */

void ReadView::prepare(trx_id_t id) {
  ut_ad(trx_sys_mutex_own());

  m_creator_trx_id = id;

  m_low_limit_no = trx_get_serialisation_min_trx_no();

  m_low_limit_id = trx_sys_get_next_trx_id_or_no();

  ut_a(m_low_limit_no &lt;= m_low_limit_id);

  if (!trx_sys-&gt;rw_trx_ids.empty()) {
    copy_trx_ids(trx_sys-&gt;rw_trx_ids);
  } else {
    m_ids.clear();
  }

  /* The first active transaction has the smallest id. */
  m_up_limit_id = !m_ids.empty() ? m_ids.front() : m_low_limit_id;

  ut_a(m_up_limit_id &lt;= m_low_limit_id);

  ut_d(m_view_low_limit_no = m_low_limit_no);
  m_closed = false;
}

源码地址：https://github.com/mysql/mysql-server/blob/8.0/storage/innobase/include/read0types.h
/** Check whether the changes by id are visible.
@param[in]	id	transaction id to check against the view
@param[in]	name	table name
@return whether the view sees the modifications of id. */
[[nodiscard]] bool changes_visible(trx_id_t id,
                                 const table_name_t &amp;name) const {
ut_ad(id &gt; 0);

if (id &lt; m_up_limit_id || id == m_creator_trx_id) {
  return (true);
}

check_trx_id_sanity(id, name);

if (id &gt;= m_low_limit_id) {
  return (false);

} else if (m_ids.empty()) {
  return (true);
}

const ids_t::value_type *p = m_ids.data();

return (!std::binary_search(p, p + m_ids.size(), id));
}

</code></pre>
]]></content>
    </entry>
</feed>