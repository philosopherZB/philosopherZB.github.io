<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://philosopherzb.github.io</id>
    <title>Philosopher</title>
    <updated>2023-09-26T05:27:40.256Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://philosopherzb.github.io"/>
    <link rel="self" href="https://philosopherzb.github.io/atom.xml"/>
    <subtitle>WORLD AS CODE</subtitle>
    <logo>https://philosopherzb.github.io/images/avatar.png</logo>
    <icon>https://philosopherzb.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, Philosopher</rights>
    <entry>
        <title type="html"><![CDATA[深入Redis之持久化]]></title>
        <id>https://philosopherzb.github.io/post/shen-ru-redis-zhi-chi-jiu-hua/</id>
        <link href="https://philosopherzb.github.io/post/shen-ru-redis-zhi-chi-jiu-hua/">
        </link>
        <updated>2023-01-31T08:26:07.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Redis持久化。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/himalayas-2301040_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="持久化">持久化</h2>
<h3 id="持久化简介">持久化简介</h3>
<p>Redis 是一个键值对数据库服务器，服务器中通常包括任意个非空数据库，其中每个非空数据库都允许存在任意个键值对。</p>
<p>众所周知，Redis是内存数据库，它将自己的数据存储在内存中。这样一旦服务器进程退出(断电、重启等原因)，那么数据将会丢失。为了解决这个问题，Redis提供了两种持久化的方式来将数据持久化到硬盘上，即内存快照(RDB)与AOF日志。</p>
<h3 id="rdb文件的创建">RDB文件的创建</h3>
<p>有两个Redis命令可用于手动触发创建RDB文件，分别为SAVE以及BGSAVE。</p>
<p>其中SAVE命令会阻塞Redis服务器进程（redis为单进程服务），直到RDB文件创建完毕为止，在这个过程中，服务器无法处理其他任何命令请求。</p>
<p>和SAVE命令直接阻塞服务器进程的做法不同，BGSAVE命令会派生一个子进程，然后由子进程来创建对应的RDB文件，原服务器进程（父进程）任可继续处理其他命令请求。</p>
<p>注意1：Redis单线程模型就决定了，我们要尽量避免所有会阻塞主线程的操作，由于Save命令执行期间会阻塞服务器进程，对于内存比较大的实例会造成长时间阻塞，因此线上环境不建议使用。</p>
<p>注意2：在bgsave命令执行的时候，为了避免父进程与子进程同时执行两个rdbSave的调用而产生竞争条件，客户端发送的save命令会被服务器拒绝。如果bgsave命令正在执行，bgrewriteaof（aof重写）命令会被延迟到bgsave命令之后执行，如果bgrewriteaof命令正在执行，那么客户端发送的bgsave命令会被服务器拒绝。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320020.png" alt="" loading="lazy"></figure>
<h4 id="save命令源码">save命令源码</h4>
<pre><code>// rdb.c
// save命令---在redis-cli下调用对应的save时，会触发该函数执行
void saveCommand(client *c) {
    // 检测后台是否已存在运行中的save操作，如果是则拒绝当前save操作
    if (server.child_type == CHILD_TYPE_RDB) {
        addReplyError(c,&quot;Background save already in progress&quot;);
        return;
    }
    rdbSaveInfo rsi, *rsiptr;
    rsiptr = rdbPopulateSaveInfo(&amp;rsi);
    // 执行具体的rdb文件生成操作
    if (rdbSave(SLAVE_REQ_NONE,server.rdb_filename,rsiptr) == C_OK) {
        addReply(c,shared.ok);
    } else {
        addReplyErrorObject(c,shared.err);
    }
}

/* Save the DB on disk. Return C_ERR on error, C_OK on success. */
int rdbSave(int req, char *filename, rdbSaveInfo *rsi) {
    char tmpfile[256];
    char cwd[MAXPATHLEN]; /* Current working dir path for error messages. */
    FILE *fp = NULL;
    rio rdb;
    int error = 0;

    // 1、创建临时文件并打开
    snprintf(tmpfile,256,&quot;temp-%d.rdb&quot;, (int) getpid());
    fp = fopen(tmpfile,&quot;w&quot;);
    if (!fp) {
        char *str_err = strerror(errno);
        char *cwdp = getcwd(cwd,MAXPATHLEN);
        serverLog(LL_WARNING,
            &quot;Failed opening the temp RDB file %s (in server root dir %s) &quot;
            &quot;for saving: %s&quot;,
            tmpfile,
            cwdp ? cwdp : &quot;unknown&quot;,
            str_err);
        return C_ERR;
    }

    // 2、创建并初始化rio，rio是redis对io的抽象，提供了常用的read，write，flush，checksum等函数
    rioInitWithFile(&amp;rdb,fp);
    startSaving(RDBFLAGS_NONE);

    if (server.rdb_save_incremental_fsync)
        rioSetAutoSync(&amp;rdb,REDIS_AUTOSYNC_BYTES);
    // 3、将内存信息同步至临时文件中
    if (rdbSaveRio(req,&amp;rdb,&amp;error,RDBFLAGS_NONE,rsi) == C_ERR) {
        errno = error;
        goto werr;
    }

    /* Make sure data will not remain on the OS's output buffers */
    // 4、将临时文件的内容刷新至磁盘，最后关闭rio，确保操作系统的输出缓冲区不再保留数据
    if (fflush(fp)) goto werr;
    if (fsync(fileno(fp))) goto werr;
    if (fclose(fp)) { fp = NULL; goto werr; }
    fp = NULL;
  
    /* Use RENAME to make sure the DB file is changed atomically only
     * if the generate DB file is ok. */
    // 5、将临时文件重命名为正式文件名
    if (rename(tmpfile,filename) == -1) {
        char *str_err = strerror(errno);
        char *cwdp = getcwd(cwd,MAXPATHLEN);
        serverLog(LL_WARNING,
            &quot;Error moving temp DB file %s on the final &quot;
            &quot;destination %s (in server root dir %s): %s&quot;,
            tmpfile,
            filename,
            cwdp ? cwdp : &quot;unknown&quot;,
            str_err);
        unlink(tmpfile);
        stopSaving(0);
        return C_ERR;
    }
    // 6、将server.dirty清零，server.dirty是用了记录在上次生成rdb后有多少次数据变更，会在serverCron中用到。
    serverLog(LL_NOTICE,&quot;DB saved on disk&quot;);
    server.dirty = 0;
    server.lastsave = time(NULL);
    server.lastbgsave_status = C_OK;
    stopSaving(1);
    return C_OK;

werr:
    serverLog(LL_WARNING,&quot;Write error saving DB on disk: %s&quot;, strerror(errno));
    if (fp) fclose(fp);
    unlink(tmpfile);
    stopSaving(0);
    return C_ERR;
}

</code></pre>
<h4 id="bgsave命令源码">bgsave命令源码</h4>
<pre><code>// rdb.c
// bgsave命令---在redis-cli下调用对应的bgsave时，会触发该函数执行
/* BGSAVE [SCHEDULE] */
void bgsaveCommand(client *c) {
    int schedule = 0;

    /* The SCHEDULE option changes the behavior of BGSAVE when an AOF rewrite
     * is in progress. Instead of returning an error a BGSAVE gets scheduled. */
     // 当 AOF 重写正在进行时，SCHEDULE 选项会更改 BGSAVE 的行为。 BGSAVE 将会延时执行而不是返回错误。
    if (c-&gt;argc &gt; 1) {
        if (c-&gt;argc == 2 &amp;&amp; !strcasecmp(c-&gt;argv[1]-&gt;ptr,&quot;schedule&quot;)) {
            schedule = 1;
        } else {
            addReplyErrorObject(c,shared.syntaxerr);
            return;
        }
    }

    rdbSaveInfo rsi, *rsiptr;
    rsiptr = rdbPopulateSaveInfo(&amp;rsi);
    // 判断是否存在bgsave正在运行
    if (server.child_type == CHILD_TYPE_RDB) {
        addReplyError(c,&quot;Background save already in progress&quot;);
    } else if (hasActiveChildProcess() || server.in_exec) {
        // 如果bgsave已经在执行中了，这次执行会放到serverCron中执行
        if (schedule || server.in_exec) {
            server.rdb_bgsave_scheduled = 1;
            addReplyStatus(c,&quot;Background saving scheduled&quot;);
        } else {
            addReplyError(c,
            &quot;Another child process is active (AOF?): can't BGSAVE right now. &quot;
            &quot;Use BGSAVE SCHEDULE in order to schedule a BGSAVE whenever &quot;
            &quot;possible.&quot;);
        }
    } else if (rdbSaveBackground(SLAVE_REQ_NONE,server.rdb_filename,rsiptr) == C_OK) {
        addReplyStatus(c,&quot;Background saving started&quot;);
    } else {
        addReplyErrorObject(c,shared.err);
    }
}

// 后台执行rdbsave
int rdbSaveBackground(int req, char *filename, rdbSaveInfo *rsi) {
    pid_t childpid;

    if (hasActiveChildProcess()) return C_ERR;
    server.stat_rdb_saves++;

    server.dirty_before_bgsave = server.dirty;
    server.lastbgsave_try = time(NULL);
    // 创建子进程，redisFork函数是对fork的封装
    if ((childpid = redisFork(CHILD_TYPE_RDB)) == 0) {
        int retval;

        /* Child */
        redisSetProcTitle(&quot;redis-rdb-bgsave&quot;);
        redisSetCpuAffinity(server.bgsave_cpulist);
        // 实际上依旧是调用了rdbsave函数进行rdb文件生成操作
        retval = rdbSave(req, filename,rsi);
        if (retval == C_OK) {
            sendChildCowInfo(CHILD_INFO_TYPE_RDB_COW_SIZE, &quot;RDB&quot;);
        }
        exitFromChild((retval == C_OK) ? 0 : 1);
    } else {
        /* Parent */
        // 创建子进程失败
        if (childpid == -1) {
            server.lastbgsave_status = C_ERR;
            serverLog(LL_WARNING,&quot;Can't save in background: fork: %s&quot;,
                strerror(errno));
            return C_ERR;
        }
        // 已经存在正在运行的子进程
        serverLog(LL_NOTICE,&quot;Background saving started by pid %ld&quot;,(long) childpid);
        server.rdb_save_time_start = time(NULL);
        server.rdb_child_type = RDB_CHILD_TYPE_DISK;
        return C_OK;
    }
    return C_OK; /* unreached */
}

</code></pre>
<h4 id="自动触发">自动触发</h4>
<p>除了上述说到的两个手动生成rdb文件的命令外，redis还提供了自动生成rdb文件的机制。只需在redis.conf文件中配置对应的save选项，便可让服务器每隔一段时间自动执行一次bgsave命令。源码如下：</p>
<pre><code>// redis.conf
# Save the DB to disk.
#
# save &lt;seconds&gt; &lt;changes&gt; [&lt;seconds&gt; &lt;changes&gt; ...]
#
# Redis will save the DB if the given number of seconds elapsed and it
# surpassed the given number of write operations against the DB.
#
# Snapshotting can be completely disabled with a single empty string argument
# as in following example:
#
# save &quot;&quot;
#
# Unless specified otherwise, by default Redis will save the DB:
#   * After 3600 seconds (an hour) if at least 1 change was performed
#   * After 300 seconds (5 minutes) if at least 100 changes were performed
#   * After 60 seconds if at least 10000 changes were performed
#
# You can set these explicitly by uncommenting the following line.
#
# save 3600 1 300 100 60 10000
// 以下配置生效后，将会在对应的时刻自动触发bgsave命令执行，从而生成快照rdb文件
save 3600 1 # 服务器在3600秒内对数据库进行了至少1次修改
save 300 100 # 服务器在300秒内对数据库进行了至少100次修改
save 60 10000 # 服务器在60秒内对数据库进行了至少10000次修改

</code></pre>
<h4 id="快照数据">快照数据</h4>
<p>Redis的快照本质上是在某一时刻对内存中的全量数据进行一次写入磁盘操作。在此过程中，redis采用了写时复制(copy-on-write)技术，以保证数据修改可以同步至快照中。</p>
<p>在redis的写时复制流程中，bgsave fork子进程的时候，并不会完全复制主进程的内存数据，而是只复制必要的虚拟数据结构，并不为其分配真实的物理空间，它与父进程共享同一个物理内存空间。</p>
<p>bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。此时，如果主线程对这些数据也都是读操作，那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据，此时会给子进程分配一块物理内存空间，把要修改的数据复制一份，生成该数据的副本到子进程的物理内存空间。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。</p>
<p>写入时复制（英语：Copy-on-write，简称COW）是一种计算机程式设计领域的优化策略。其核心思想是，如果有多个调用者（callers）同时请求相同资源（如内存或磁盘上的数据存储），他们会共同取得相同的指标指向相同的资源，直到某个调用者试图修改资源的内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。这过程对其他的调用者都是透明的。此作法主要的优点是如果调用者没有修改该资源，就不会有副本（private copy）被建立，因此多个调用者只是读取操作时可以共享同一份资源。</p>
<p>快照执行间隔时间越长，那么出现意外导致的数据丢失就会越多，为了防止大量数据丢失，是不是可以尽可能的降低快照执行间隔呢？</p>
<p>其实不然，一方面，持久化是一个写入磁盘的过程，频繁将全量数据写入磁盘，会给磁盘带来很大压力，频繁执行快照也容易导致前一个快照还没有执行完，后一个又开始了，这样多个快照竞争有限的磁盘带宽，容易造成恶性循环。</p>
<p>再者，bgsave所fork出来的子进程执行操作虽然并不会阻塞父进程的操作，但是fork出子进程的操作却是由主进程完成的，会阻塞主进程，fork子进程需要拷贝进程必要的数据结构，其中有一项就是拷贝内存页表（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量CPU资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，实例越大，内存页表越大，fork阻塞时间也就越久。</p>
<p>所以快照的执行间隔需要经过严格的数据测试才能得到一个相对合理的时间。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320021.png" alt="img" loading="lazy"></figure>
<h3 id="aof持久化的实现">AOF持久化的实现</h3>
<p>从上节中可以发现，RDB形式的存储方式存在一定的问题，无论是间隔过长导致的数据丢失还是频繁操作所带来的性能消耗在一定程度上都令人无法忍受；为了解决RDB文件存储的劣势，redis还推出了另一种存储方式--AOF。</p>
<p>与RDB持久化通过保存数据库中的键值对来记录数据库状态不同，AOF持久化是通过保存Redis服务器所执行的写命令来记录数据库状态的。即，每执行一个写命令，就把该命令记录到日志文件中。</p>
<p>需要注意的是，数据写入日志文件的操作是在Redis执行命令将数据写入内存之后。这样做的好处是不会阻塞当前操作，也可以避免额外的检查开销。</p>
<p>当然，这样也会出现另外的问题。例如，当写入日志文件失败的时候，操作命令依旧会丢失，且写日志本身也是由主进程来进行处理，当数据过多的情况下，对后续执行命令存在一定的影响。</p>
<p>为了解决上述可能出现的问题，Redis利用了aof缓冲区来缓存写日志文件的操作命令。即操作命令会同步写入aof_buf中，之后便依据配置的刷盘策略进行数据写入硬盘操作。</p>
<p>Redis AOF 提供了三种回写磁盘的策略，默认：everysec。</p>
<ul>
<li>Always(同步写回): 命令写入 AOF缓冲区后调用系统 fsync操作同步到AOF文件, fsync完成后线程返回。速度最慢但最安全。</li>
<li>Everysec(每秒写回): 命令写人 AOF缓冲区后调用系统 write操作, write完成后线程返回。fsync同步文件操作由专门线程每秒调用一次。最多丢失一秒钟的数据，速度却得到了很大提升，为默认配置。</li>
<li>No(操作系统自动写回): 命令写入 AOF缓冲区后调用系统 write操作,不对AOF文件做 fsync同步,同步硬盘操作由操作系统负责,通常同步周期最长30秒。速度最快，但最不安全，一旦出现故障，未同步进硬盘的资源都将丢失。</li>
</ul>
<pre><code>// redis.conf
# The fsync() call tells the Operating System to actually write data on disk
# instead of waiting for more data in the output buffer. Some OS will really flush
# data on disk, some other OS will just try to do it ASAP.
#
# Redis supports three different modes:
#
# no: don't fsync, just let the OS flush the data when it wants. Faster.
# always: fsync after every write to the append only log. Slow, Safest.
# everysec: fsync only one time every second. Compromise.
#
# The default is &quot;everysec&quot;, as that's usually the right compromise between
# speed and data safety. It's up to you to understand if you can relax this to
# &quot;no&quot; that will let the operating system flush the output buffer when
# it wants, for better performances (but if you can live with the idea of
# some data loss consider the default persistence mode that's snapshotting),
# or on the contrary, use &quot;always&quot; that's very slow but a bit safer than
# everysec.
#
# More details please check the following article:
# http://antirez.com/post/redis-persistence-demystified.html
#
# If unsure, use &quot;everysec&quot;.

# appendfsync always
appendfsync everysec
# appendfsync no

</code></pre>
<h4 id="aof重写">AOF重写</h4>
<p>Redis中AOF命令追加随着时间的增长会导致对应的aof文件越来越大，过大的aof文件不仅让追加命令变慢，还会影响redis服务器以及宿主机(存储资源占用过多)，且还原数据时，也会十分缓慢。这个时候便需要使用AOF重写机制来缩小aof文件大小。</p>
<pre><code>redis&gt; set key v1
OK
redis&gt; set key v2
OK
redis&gt; set key v3
OK
redis&gt; set key v4
OK

</code></pre>
<p>AOF文件是以追加的方式，逐一记录接收到的写命令的。当一个键值对被多条写命令反复修改时，AOF 文件会记录相应的多条命令。如上示例，我们执行完命令后，Redis会在AOF里面追加4条命令。但实际上只需要set key v4一条命令就够了。</p>
<p>Redis首先会从数据库中读取所有的数据键值，然后用一条命令将其存入aof文件中，用以代替对同一数据键多次的写操作(如果存在多次写操作)。这就是aof重写的基本原理。</p>
<p>AOF重写时会进行大量的写操作，如果在redis主进程中来处理，将会导致长时间的进程阻塞，很显然，作为一种持久化的辅佐手段，redis是不希望它来阻塞主进程的。</p>
<p>因此，redis将AOF重写程序放在了子进程中执行，这样有两个好处：</p>
<ul>
<li>子进程进行AOF重写期间，redis主进程依旧可以继续处理命令请求。</li>
<li>子进程带有服务器进程的数据副本，使用子进程而不是线程，可以在避免使用锁的情况下，保证数据的安全性。</li>
</ul>
<p>当然，使用子进程处理aof重写，也面临着一个问题，那就是服务进程在aof重写过程中依旧在接受客户端的命令请求，而其中就可能存在写命令操作。写命令操作可能会对现有数据库值进行修改或新增，如果子进程对服务进程的新命令不作任何处理，那么将会导致服务器当前的数据库键值与重写后的aof文件所保存的数据库键值不一致。</p>
<table>
<thead>
<tr>
<th>时间</th>
<th>服务器进程</th>
<th>子进程</th>
</tr>
</thead>
<tbody>
<tr>
<td>T1</td>
<td>执行命令 set k1 v1</td>
<td></td>
</tr>
<tr>
<td>T2</td>
<td>执行命令 set k1 v2</td>
<td></td>
</tr>
<tr>
<td>T3</td>
<td>执行命令 set k1 v3</td>
<td></td>
</tr>
<tr>
<td>T4</td>
<td>创建子进程，执行aof重写</td>
<td>开始aof重写</td>
</tr>
<tr>
<td>T5</td>
<td>执行命令 set k2 v2</td>
<td>执行重写操作</td>
</tr>
<tr>
<td>T6</td>
<td>执行命令 set k3 v3</td>
<td>执行重写操作</td>
</tr>
<tr>
<td>T7</td>
<td>执行命令 set k4 v4</td>
<td>完成aof重写操作</td>
</tr>
</tbody>
</table>
<p>如上表所示，当子进程开始进行aof重写时，数据库仅有k1一个键，但当子进程完成aof重写后，数据库中已经新设置了k2，k3，k4三个键。由此可以看出，重写后的aof文件与当前服务器数据库不一致。</p>
<p>为了解决上述问题，redis设置了一个aof重写缓冲区，这个缓冲区在服务器创建子进程后开始使用。这个时候，当redis服务器执行新的写命令时，该命令会同时存储到aof缓冲区以及aof重写缓冲区中。</p>
<p>如下图所示，在子进程执行aof重写期间，服务器进程需要执行以下步骤：</p>
<ol>
<li>接受客户端传入的命令。</li>
<li>执行命令，更新内存中的值。</li>
<li>将执行命令记录到aof缓冲区。</li>
<li>将执行命令记录到aof重写后缓冲区。</li>
</ol>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320022.png" alt="" loading="lazy"></figure>
<p>当子进程完成aof重写后，它会向服务器进程发送一个信号，服务器进程接收到信号后，将会执行一个信号函数，将aof重写缓冲区中的所有内容追加到新aof文件中，这便可以保证新aof文件与当前数据库中数据状态保持一致。同时，该函数还会对新的aof文件进行重命名，原子的覆盖现有的aof文件，至此，新旧aof文件的替换便已完成。</p>
<table>
<thead>
<tr>
<th>时间</th>
<th>服务器进程</th>
<th>子进程</th>
</tr>
</thead>
<tbody>
<tr>
<td>T1</td>
<td>执行命令 set k1 v1</td>
<td></td>
</tr>
<tr>
<td>T2</td>
<td>执行命令 set k1 v2</td>
<td></td>
</tr>
<tr>
<td>T3</td>
<td>执行命令 set k1 v3</td>
<td></td>
</tr>
<tr>
<td>T4</td>
<td>创建子进程，执行aof重写</td>
<td>开始aof重写</td>
</tr>
<tr>
<td>T5</td>
<td>执行命令 set k2 v2</td>
<td>执行重写操作</td>
</tr>
<tr>
<td>T6</td>
<td>执行命令 set k3 v3</td>
<td>执行重写操作</td>
</tr>
<tr>
<td>T7</td>
<td>执行命令 set k4 v4</td>
<td>完成aof重写操作，向服务器进程发送信号</td>
</tr>
<tr>
<td>T8</td>
<td>接收子进程信号，将T5，T6，T7三个时间点的新命令追加到新aof文件中</td>
<td></td>
</tr>
<tr>
<td>T9</td>
<td>用新的aof文件覆盖已有的旧aof文件</td>
<td></td>
</tr>
</tbody>
</table>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深入Redis之内存淘汰策略]]></title>
        <id>https://philosopherzb.github.io/post/shen-ru-redis-zhi-nei-cun-tao-tai-ce-lue/</id>
        <link href="https://philosopherzb.github.io/post/shen-ru-redis-zhi-nei-cun-tao-tai-ce-lue/">
        </link>
        <updated>2023-01-07T08:19:43.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Redis内存淘汰策略。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/tree-736881_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="内存淘汰策略">内存淘汰策略</h2>
<h3 id="内存淘汰概述">内存淘汰概述</h3>
<p>redis 是内存数据库，可以通过 redis.conf 配置 maxmemory，也可以通过 congfig set 命令进行设置maxmemory，限制 redis 内存使用量。</p>
<p>当 redis 主库内存超出限制时，命令处理将会触发数据淘汰机制，淘汰（key-value）数据，直至当前内存使用量小于限制阈值。</p>
<p>由于需要淘汰掉key，所以此时redis不适合用作key-value数据库了，开启了内存淘汰的redis一般用于缓存中，因此很多人也把他叫缓存淘汰和缓存淘汰策略。</p>
<p>又因为内存淘汰的触发时机是内存使用达到阈值，为了存储新的key然后淘汰旧的key，因此也有人把内存淘汰策略叫做缓存替换策略。</p>
<p>注意： maxmemory-policy 默认为noeviction；不配置 maxmemory 的值时，将默认为0；如果需要更改maxmemory 值，需要将maxmemory-policy 设置为非noeviction 策略。</p>
<pre><code>// 文件地址： redis.config
# In short... if you have replicas attached it is suggested that you set a lower
# limit for maxmemory so that there is some free RAM on the system for replica
# output buffers (but this is not needed if the policy is 'noeviction').
#
# maxmemory &lt;bytes&gt;

# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory
# is reached. You can select one from the following behaviors:
#
# volatile-lru -&gt; Evict using approximated LRU, only keys with an expire set.
# allkeys-lru -&gt; Evict any key using approximated LRU.
# volatile-lfu -&gt; Evict using approximated LFU, only keys with an expire set.
# allkeys-lfu -&gt; Evict any key using approximated LFU.
# volatile-random -&gt; Remove a random key having an expire set.
# allkeys-random -&gt; Remove a random key, any key.
# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)
# noeviction -&gt; Don't evict anything, just return an error on write operations.

# LRU means Least Recently Used
# LFU means Least Frequently Used

# The default is:
# maxmemory-policy noeviction

</code></pre>
<table>
<thead>
<tr>
<th>配置</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>noeviction</td>
<td>不淘汰任何数据，写入操作将会返回错误，允许读操作以及删除操作</td>
</tr>
<tr>
<td>volatile-random</td>
<td>随机删除设置了过期时间的键。</td>
</tr>
<tr>
<td>allkeys-random</td>
<td>删除随机键，任何键。</td>
</tr>
<tr>
<td>volatile-ttl</td>
<td>删除最接近到期时间（较小的TTL）的键。 在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除</td>
</tr>
<tr>
<td>volatile-lru</td>
<td>在设置了过期时间的键中，使用近似的LRU算法淘汰最近最少使用的键。</td>
</tr>
<tr>
<td>allkeys-lru</td>
<td>使用近似的LRU算法淘汰长时间没有使用的键。</td>
</tr>
<tr>
<td>volatile-lfu</td>
<td>在设置了过期时间的键中，使用近似的LFU算法淘汰使用频率比较低的键。</td>
</tr>
<tr>
<td>allkeys-lfu</td>
<td>使用近似的LFU算法淘汰整个数据库的键。</td>
</tr>
</tbody>
</table>
<h3 id="不淘汰配置noeviction">不淘汰配置（noeviction）</h3>
<p>当内存淘汰策略使用noeviction时，数据并不会被删除，只是禁止了写入操作。使用这种淘汰策略时，maxmemory需要设置为0，根据不同操作位系统有以下区别。</p>
<ul>
<li>64 bit系统：maxmemory 设置为 0 表示不限制 Redis 内存使用。需要注意的是，这种不限制策略可能由于操作系统设置了虚拟内存（部分物理存储会转化为虚拟内存，两者可以进行swap）而导致性能降低。</li>
<li>32 bit系统：maxmemory 设置为 0 表示限制 Redis 内存，不能超过 3G。</li>
</ul>
<pre><code>// 文件地址： server.c
void initServer(void) {
    /* 32 bit instances are limited to 4GB of address space, so if there is
     * no explicit limit in the user provided configuration we set a limit
     * at 3 GB using maxmemory with 'noeviction' policy'. This avoids
     * useless crashes of the Redis instance for out of memory. */
    // 32位的系统会限制内存为4GB，为了防止因内存不足而导致redis崩溃，此处会将其限制为3GB 
    if (server.arch_bits == 32 &amp;&amp; server.maxmemory == 0) {
        serverLog(LL_WARNING,&quot;Warning: 32 bit instance detected but no memory limit set. Setting 3 GB maxmemory limit with 'noeviction' policy now.&quot;);
        server.maxmemory = 3072LL*(1024*1024); /* 3 GB */
        server.maxmemory_policy = MAXMEMORY_NO_EVICTION;
    }
}

</code></pre>
<h3 id="随机淘汰配置random">随机淘汰配置（random）</h3>
<p>当程序配置了volatile-random 或者 allkeys-random 两种淘汰策略中的一种时，同时也需要将64bit的操作系统的maxmemory设置为大于0。随机淘汰的实现机制比较简单，它不关注数据使用频率，直接从库中随机挑选数据进行删除操作。</p>
<pre><code>// 文件地址： evict.c
int performEvictions(void) {
        /* volatile-random and allkeys-random policy */
        else if (server.maxmemory_policy == MAXMEMORY_ALLKEYS_RANDOM ||
                 server.maxmemory_policy == MAXMEMORY_VOLATILE_RANDOM)
        {
            /* When evicting a random key, we try to evict a key for
             * each DB, so we use the static 'next_db' variable to
             * incrementally visit all DBs. */
            // 递增访问所有DB并释放key 
            for (i = 0; i &lt; server.dbnum; i++) {
                j = (++next_db) % server.dbnum;
                db = server.db+j;
                // 判断淘汰策略针对的是否allkeys, db-&gt;dict：数据库中所有key集合，db-&gt;expire：数据库中过期键集合
                dict = (server.maxmemory_policy == MAXMEMORY_ALLKEYS_RANDOM) ?
                        db-&gt;dict : db-&gt;expires;
                if (dictSize(dict) != 0) {
                    // 从dict中随机获取需要被淘汰的key
                    de = dictGetRandomKey(dict);
                    bestkey = dictGetKey(de);
                    bestdbid = j;
                    break;
                }
            }
        }
}

</code></pre>
<h3 id="最近最少使用淘汰lru">最近最少使用淘汰（LRU）</h3>
<p>LRU算法全称是最近最少使用算法（Least Recently Use），广泛的应用于缓存机制中。当缓存使用的空间达到上限后，就需要从已有的数据中淘汰一部分以维持缓存的可用性，而淘汰数据的选择就是通过LRU算法完成的。</p>
<p>LRU算法的基本思想是基于局部性原理中的时间局部性：如果一个信息量正在被访问，那么在近期该信息量很可能会被再次访问。</p>
<p>Redis LRU算法并没有精确的实现LRU算法，而是尽可能达到LRU的目的，这意味着redis无法百分百精确的淘汰最久未访问的数据项，当然，redis3.0后其采用了pool的机制，让其更加的接近LRU算法行为。</p>
<p>Redis LRU主要是通过对少量数据键进行采样，并淘汰其中最久未访问的数据键。默认样本值会被设置为5，这对于redis而言是足够好的，无论是性能还是淘汰率；如果设置为10，会更接近于LRU，但对于CPU的消耗会更大；如果设置为3，淘汰速度将会更快，但不怎么准确。</p>
<pre><code>// 文件地址： redis.config
# The default of 5 produces good enough results. 10 Approximates very closely
# true LRU but costs more CPU. 3 is faster but not very accurate.
#
# maxmemory-samples 5

</code></pre>
<p>Redis 并没有使用真正的LRU实现的原因，是因为它需要更多的内存，而内存对于redis而言是无比重要的。不过，实际上对于使用redis的程序来说，Redis LRU基本等效于真正的LRU算法。如下所示：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320019.png" alt="img" loading="lazy"></figure>
<p>在图中有三种点，形成三个不同的带。</p>
<ul>
<li>浅灰色带是被驱逐的对象。</li>
<li>灰色带是未被驱逐的对象。</li>
<li>绿色带是添加的对象。</li>
</ul>
<p>如上图所示，与 Redis 2.8 相比，Redis 3.0 的 5 个样本做得更好，但大多数最新访问的对象仍由 Redis 2.8 保留。在 Redis 3.0 中使用 10 的样本大小，该近似值非常接近 Redis 3.0 的理论性能。</p>
<pre><code>// 文件地址：server.c
// 执行命令或准备服务器以从客户端进行批量读取。
int processCommand(client *c) {
    /* Handle the maxmemory directive.
     *
     * Note that we do not want to reclaim memory if we are here re-entering
     * the event loop since there is a busy Lua script running in timeout
     * condition, to avoid mixing the propagation of scripts with the
     * propagation of DELs due to eviction. */
    // 如果配置了 server.maxmemory 并且 lua脚本没有在超时条件下运行 
    if (server.maxmemory &amp;&amp; !scriptIsTimedout()) {
        // performEvictions() 是核心的淘汰函数
        int out_of_memory = (performEvictions() == EVICT_FAIL);
   }
}  

</code></pre>
<pre><code>// 文件地址： evict.c
/* 检查内存使用是否在当前的“maxmemory”限制内。如果超过“maxmemory”，尝试通过逐出数据来释放内存（如果这样做是安全的）。
 *
 * Redis 可能突然显示超过“maxmemory”设置。如果分配量很大（如哈希表调整大小），
 * 或者即使手动调整了“maxmemory”设置，也会发生这种情况。因此，在一段可控的时间内驱逐很重要,
 * 否则 Redis 在驱逐时会变得无响应。
 *
 * 此功能的目标是改善记忆状况 - 而不是立即解决它。在某些项目已被驱逐但尚未达到“最大内存”限制的情况下，
 * 将启动一个 aeTimeProc，它将继续驱逐项目，直到没有其他可驱逐的项目。
 *
 * 这应该在执行命令之前调用。如果返回 EVICT_FAIL，则应拒绝会导致内存使用量增加的命令。
 *
 * 回报：
 * EVICT_OK - 内存正常或现在无法执行驱逐
 * EVICT_RUNNING - 内存超过限制，但驱逐仍在处理
 * EVICT_FAIL - 内存超过限制，没有什么可以驱逐的
 */
int performEvictions(void) {
}  

</code></pre>
<pre><code>// 文件地址： evict.c#performEvictions
// 获取内存状态，判断会否需要进行内存淘汰
if (getMaxmemoryState(&amp;mem_reported,NULL,&amp;mem_tofree,NULL) == C_OK) {
    result = EVICT_OK;
    goto update_metrics;
}
// 需要释放内存，但noeviction淘汰策略不允许这样做
if (server.maxmemory_policy == MAXMEMORY_NO_EVICTION) {
    result = EVICT_FAIL;  /* We need to free memory, but policy forbids. */
    goto update_metrics;
}

</code></pre>
<pre><code>// 文件地址： evict.c
/* 从maxmemory指令的角度获取内存状态：
 * 如果使用的内存低于 maxmemory 设置，则返回 C_OK。
 * 否则，如果超过内存限制，函数返回
 * C_ERR。
 *
 * 该函数可能会通过引用返回附加信息，仅当
 * 指向相应参数的指针不为 NULL。某些字段是
 * 仅在返回 C_ERR 时填充：
 *
 * 'total' 使用的总字节数。
 *（为 C_ERR 和 C_OK 填充）
 *
 * 'logical' 使用的内存量减去slaves/AOF缓冲区。
 *（返回 C_ERR 时填充）
 *
 * 'tofree' 为了回到内存限制应该释放的内存量
 *（返回 C_ERR 时填充）
 *
 * 'level' 这通常范围从 0 到 1，并报告数量
 * 当前使用的内存。如果我们超过了内存限制可能会 &gt; 1。
 *（为 C_ERR 和 C_OK 填充）
 */
int getMaxmemoryState(size_t *total, size_t *logical, size_t *tofree, float *level)

</code></pre>
<pre><code>// 文件地址： evict.c#performEvictions
// 需要被淘汰的key
sds bestkey = NULL;
// 需要被淘汰的key所属的db id
int bestdbid;
// redis db
redisDb *db;
// 字典
dict *dict;
// 哈希表节点
dictEntry *de;

// 如果淘汰策略选择了LRU或者LFU或者TTL，则进行如下数据键筛选逻辑
if (server.maxmemory_policy &amp; (MAXMEMORY_FLAG_LRU|MAXMEMORY_FLAG_LFU) ||
    server.maxmemory_policy == MAXMEMORY_VOLATILE_TTL)
{   
    // 淘汰池，是针对旧版LRU的一个优化，让其更符合真实LRU算法行为
    struct evictionPoolEntry *pool = EvictionPoolLRU;

    while(bestkey == NULL) {
        unsigned long total_keys = 0, keys;

        /* We don't want to make local-db choices when expiring keys,
         * so to start populate the eviction pool sampling keys from
         * every DB. */
        // 遍历所有数据库，选出采样key填充至淘汰池 
        for (i = 0; i &lt; server.dbnum; i++) {
            db = server.db+i;
            // 判断淘汰策略针对的是否allkeys, db-&gt;dict：数据库中所有key集合，db-&gt;expire：数据库中过期键集合
            dict = (server.maxmemory_policy &amp; MAXMEMORY_FLAG_ALLKEYS) ?
                    db-&gt;dict : db-&gt;expires;
            if ((keys = dictSize(dict)) != 0) {
                // 将需要驱逐的key填充至淘汰池，淘汰key在池内是有序的：按空闲时间升序
                evictionPoolPopulate(i, dict, db-&gt;dict, pool);
                // 已遍历检查过的key数量
                total_keys += keys;
            }
        }
        if (!total_keys) break; /* No keys to evict. */

        /* Go backward from best to worst element to evict. */
        // 从驱逐池末尾（空闲时间最长）往前遍历，
        for (k = EVPOOL_SIZE-1; k &gt;= 0; k--) {
            if (pool[k].key == NULL) continue;
            // 获取当前key所在数据库的id
            bestdbid = pool[k].dbid;

            // 如果针对的是所有数据键，则从dict中取所有数据
            if (server.maxmemory_policy &amp; MAXMEMORY_FLAG_ALLKEYS) {
                de = dictFind(server.db[bestdbid].dict,
                    pool[k].key);
            } else {
                // 如果针对的是过期数据键，则从expires中取过期数据
                de = dictFind(server.db[bestdbid].expires,
                    pool[k].key);
            }

            /* Remove the entry from the pool. */
            if (pool[k].key != pool[k].cached)
                sdsfree(pool[k].key);
            pool[k].key = NULL;
            pool[k].idle = 0;

            /* If the key exists, is our pick. Otherwise it is
             * a ghost and we need to try the next element. */
            // 如果这个节点存在，就跳出这个循环，否则尝试下一个元素。 
            if (de) {
                bestkey = dictGetKey(de);
                break;
            } else {
                /* Ghost... Iterate again. */
            }
        }
    }
}

</code></pre>
<pre><code>// 文件地址： evict.c
// 填充淘汰池
/* 
 * 这是 performEvictions() 的辅助函数，用于在每次我们想要key过期时用一些条目填充 evictionPool。
 * 
 * 添加空闲时间大于当前key之一的key。 如果有空闲条目，则一直添加key。
 * 
 * 我们按升序将键依次插入，因此空闲时间较小的键在左侧，而空闲时间较长的键在右侧。
 */
void evictionPoolPopulate(int dbid, dict *sampledict, dict *keydict, struct evictionPoolEntry *pool) {
    int j, k, count;
    // 初始化采样集合
    dictEntry *samples[server.maxmemory_samples];
    // 针对字典集进行随机采样
    count = dictGetSomeKeys(sampledict,samples,server.maxmemory_samples);
    for (j = 0; j &lt; count; j++) {
        // 这被称为空闲只是因为代码最初处理的是 LRU，但实际上只是一个分数，分数越高意味着候选者越好。
        unsigned long long idle;
        sds key;
        robj *o;
        dictEntry *de;

        de = samples[j];
        key = dictGetKey(de);

        /* 如果我们采样的字典不是主字典（而是过期的字典），我们需要在键字典中再次查找键以获得值对象。*/
        if (server.maxmemory_policy != MAXMEMORY_VOLATILE_TTL) {
            if (sampledict != keydict) de = dictFind(keydict, key);
            o = dictGetVal(de);
        }

        /* 根据策略计算空闲时间。 这被称为空闲只是因为代码最初处理的是 LRU，
         * 但实际上只是一个分数，分数越高意味着候选者越好。 */
        if (server.maxmemory_policy &amp; MAXMEMORY_FLAG_LRU) {
            idle = estimateObjectIdleTime(o);
        } else if (server.maxmemory_policy &amp; MAXMEMORY_FLAG_LFU) {
            /* 当我们使用 LRU 策略时，我们按空闲时间对密钥进行排序，以便我们从更大的空闲时间开始过期密钥。 
             * 然而，当策略是 LFU 策略时，我们有一个频率估计，我们希望首先驱逐频率较低的键。 
             * 因此，在池中，我们使用反转频率减去实际频率到最大频率 255 来放置对象。 */
            idle = 255-LFUDecrAndReturn(o);
        } else if (server.maxmemory_policy == MAXMEMORY_VOLATILE_TTL) {
            // 在这种情况下，越早过期越好。
            idle = ULLONG_MAX - (long)dictGetVal(de);
        } else {
            serverPanic(&quot;Unknown eviction policy in evictionPoolPopulate()&quot;);
        }

        /* 将元素插入池中。 首先，找到空闲时间小于我们空闲时间的第一个空桶或第一个填充桶。 */
        k = 0;
        // 遍历淘汰池，从左边开始，找到第一个空桶或者第一个空闲时间小于待选元素的桶，k是该元素的坐标
        while (k &lt; EVPOOL_SIZE &amp;&amp;
               pool[k].key &amp;&amp;
               pool[k].idle &lt; idle) k++;
        if (k == 0 &amp;&amp; pool[EVPOOL_SIZE-1].key != NULL) {
             /* 如果元素小于我们拥有的最差元素并且没有空桶，则无法插入。
             * 
             * key == 0 说明上面的while循环一次也没有进入
             * 要么第一个元素就是空的，要么所有已有元素的空闲时间都大于等于待插入元素的空闲时间（待插入元素比已有所有元素都优质）
             * 又因为数组最后一个key不为空，因为是从左边开始插入的，所以排除了第一个元素是空的
             */
            continue;
        } else if (k &lt; EVPOOL_SIZE &amp;&amp; pool[k].key == NULL) {
             /* 插入空桶，插入前无需设置 */
        } else {
           /* 插入中间，现在 k 指向比要插入的元素空闲时间大的第一个元素 */
            if (pool[EVPOOL_SIZE-1].key == NULL) {
                /* 数组末尾有空桶，将所有元素从 k 向右移动到末尾。*/
                /* 覆盖前保存 SDS */
                sds cached = pool[EVPOOL_SIZE-1].cached;
                // 注意这里不设置 pool[k], 只是给 pool[k] 腾位置
                memmove(pool+k+1,pool+k,
                    sizeof(pool[0])*(EVPOOL_SIZE-k-1));
                pool[k].cached = cached;
            } else {
                /* 右边没有可用空间？ 在 k-1 处插入 */
                k--;
                /*
                 * 将k（包含）左侧的所有元素向左移动，因此我们丢弃空闲时间较小的元素。
                 */
                sds cached = pool[0].cached; /* Save SDS before overwriting. */
                if (pool[0].key != pool[0].cached) sdsfree(pool[0].key);
                memmove(pool,pool+1,sizeof(pool[0])*k);
                pool[k].cached = cached;
            }
        }

         /*
         * 尝试重用在池条目中分配的缓存 SDS 字符串，因为分配和释放此对象的成本很高
         * 注意真正要复用的sds内存空间，避免重新申请内存，而不是它的值
         */
        int klen = sdslen(key);
        // 判断字符串长度来决定是否复用sds
        if (klen &gt; EVPOOL_CACHED_SDS_SIZE) {
            // 复制一个新的 sds 字符串并赋值
            pool[k].key = sdsdup(key);
        } else {
             /*
             * 内存拷贝函数，从数据源拷贝num个字节的数据到目标数组
             * 
             * destination：指向目标数组的指针 
             * source：指向数据源的指针 
             * num：要拷贝的字节数
             *
             */
            // 复用sds对象 
            memcpy(pool[k].cached,key,klen+1);
            // 重新设置sds长度
            sdssetlen(pool[k].cached,klen);
            // 设置真正的key
            pool[k].key = pool[k].cached;
        }
        // 设置空闲时间
        pool[k].idle = idle;
        // 设置key所在db的id
        pool[k].dbid = dbid;
    }
}

</code></pre>
<pre><code>// 文件地址： evict.c#performEvictions
// 不断循环删除key，直到释放出足够的内存
while (mem_freed &lt; (long long)mem_tofree) {
    // 需要被淘汰的key
    sds bestkey = NULL;
    if (server.maxmemory_policy &amp; (MAXMEMORY_FLAG_LRU|MAXMEMORY_FLAG_LFU) ||
            server.maxmemory_policy == MAXMEMORY_VOLATILE_TTL)
        {
            // 省略，具体可参见上一节
    }
    /* Finally remove the selected key. */
    if (bestkey) {
        db = server.db+bestdbid;
        robj *keyobj = createStringObject(bestkey,sdslen(bestkey));
         /* 我们单独计算 db*Delete() 释放的内存量。
          * 实际上，在 AOF 和复制链接中传播 DEL 所需的内存可能大于我们在删除密钥时释放的内存，
          * 但我们无法解释这一点，否则我们将永远不会退出循环。
          *
          * 与 signalModifiedKey 生成的 CSC 失效消息相同。
          *
          * AOF 和输出缓冲内存最终会被释放，所以我们只关心键空间使用的内存。 */
        // 获取已使用内存  
        delta = (long long) zmalloc_used_memory();
        // 开始监控
        latencyStartMonitor(eviction_latency);
        // 延迟释放时，异步删除键，否则同步删除
        // lazyfree的原理就是在删除对象时只是进行逻辑删除，然后把对象丢给后台，
        // 让后台线程去执行真正的destruct，避免由于对象体积过大而造成阻塞。
        if (server.lazyfree_lazy_eviction)
            dbAsyncDelete(db,keyobj);
        else
            dbSyncDelete(db,keyobj);
        // 结束监控  
        latencyEndMonitor(eviction_latency);
        latencyAddSampleIfNeeded(&quot;eviction-del&quot;,eviction_latency);
        // 计算删除key后的内存变化量
        delta -= (long long) zmalloc_used_memory();
        // 计算已释放内存
        mem_freed += delta;
        server.stat_evictedkeys++;
        signalModifiedKey(NULL,db,keyobj);
        // 通知事件，从服务器需要知道某个键已被删除
        notifyKeyspaceEvent(NOTIFY_EVICTED, &quot;evicted&quot;,
            keyobj, db-&gt;id);
        propagateDeletion(db,keyobj,server.lazyfree_lazy_eviction);
        decrRefCount(keyobj);
        keys_freed++;

        if (keys_freed % 16 == 0) {
            /* 当要释放的内存足够大时，可能会在这里花费太多时间，
             * 以至于无法足够快地将数据传递到副本，因此需要强制在循环内传输。 */
            if (slaves) flushSlavesOutputBuffers();

            /* 循环停止的条件，是已经释放了预先计算的。内存值
             * 但是，当我们在另一个线程中删除对象时，最好时不时地检查我们是否已经到达阈值（已释放足够的内存），
             * 因为“mem_freed”数量仅在 dbAsyncDelete() 调用中计算，而线程可以无时无刻的释放内存。 */
            if (server.lazyfree_lazy_eviction) {
                if (getMaxmemoryState(NULL,NULL,NULL,NULL) == C_OK) {
                    break;
                }
            }

            // 一段时间后退出循环，防止因释放大量内存而消耗过多的时间
            if (elapsedUs(evictionTimer) &gt; eviction_time_limit_us) {
                // 可以启动一个定时器来释放剩余的大量内存
                startEvictionTimeProc();
                break;
            }
        }
    } else {
        goto cant_free; /* nothing to free... */
    }   
}  

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深入Redis之过期策略]]></title>
        <id>https://philosopherzb.github.io/post/shen-ru-redis-zhi-guo-qi-ce-lue/</id>
        <link href="https://philosopherzb.github.io/post/shen-ru-redis-zhi-guo-qi-ce-lue/">
        </link>
        <updated>2022-12-31T08:15:18.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Redis过期策略。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/lago-di-limides-3025780_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="过期策略">过期策略</h2>
<h3 id="过期键删除策略">过期键删除策略</h3>
<p>在redis中，数据库键可以设置过期时间，当达到了指定的过期时间，该数据库键将会被redis删除，主要的删除策略如下：</p>
<ul>
<li>定时删除：在设置数据库键的过期时间的同时，创建一个特定的定时器，让此定时器在键的过期时间来临时，执行对应的删除操作。</li>
<li>惰性删除：此种策略不会主动地去查询过期键并删除，而是被动的执行。使用此种策略时，每次从键空间获取键时，都会检查键是否过期，过期则删除该键，否则返回。</li>
<li>定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。检查的数据库数目以及删除的过期键数量都由特定的算法所决定。</li>
</ul>
<h4 id="定时删除">定时删除</h4>
<p>定时删除策略对于内存而言是友好的。通过特定的定时器，该策略可以保证过期键会被尽快删除，从而释放过期键所占有的内存。</p>
<p>不过，该策略对于CPU时间是最不友好的：无论是创建定时器还是删除过期键操作，都会耗费不少CPU时间，尤其当定时器以及过期键十分之多的时候，这个缺点造成的影响将无法忽视。</p>
<p>例如：如果正有大量的命令请求在等待服务器处理，并且服务器当前不缺少内存，那么服务器应该优先将CPU时间用在处理客户端的命令请求上，而不是去删除过期键。</p>
<p>一般而言，不会使用该过期策略，因为对性能影响太大，且对CPU很不友好。</p>
<h4 id="惰性删除">惰性删除</h4>
<p>惰性删除策略对于CPU时间是最友好的，程序只会在取出键的时候才会对键进行过期性检查，这可以保证删除过期键的操作只会在非做不可的情况下进行(此时如果不删除，客户端收到的将是过期键),并且删除的目标仅限于当前处理的键。</p>
<p>使用这种删除策略，不会在删除其他无关的过期键上花费任何CPU时间，但同样的，它对于内存而言就非常不友好了。如果一个键已经过期，但并没有被惰性策略调用删除，那么它将一直存留在内存中，变成无法回收的垃圾数据(除非手动进行删除)。</p>
<p>可想而知，一旦这样的数据多起来，用在无用数据上的内存将会非常庞大，这对于极度依赖内存的redis来说，显然不是一个好消息。</p>
<h4 id="定期删除">定期删除</h4>
<p>从上面来看，定时删除占用太多CPU时间，影响服务器的响应时间和吞吐量；而惰性删除则浪费了大量内存，可能会造成内存泄露。为了折合上述两种策略的优势，定期删除便应运而生了。</p>
<p>定期删除策略每隔一段时间执行一次删除过期键操作，且定期时长以及删除过期键的数量都可以通过后台进行配置，从而减少CPU时耗。与此同时，定期删除也能有效减少因过期键过多而带来的内存浪费。</p>
<p>需要注意的是：使用定期删除策略，需要合理的控制其定时时长以及执行频率，防止退化为定时删除或者惰性删除。</p>
<h3 id="redis的过期键删除策略">Redis的过期键删除策略</h3>
<p>在redis中，主要使用的是惰性删除以及定期删除这两种策略；通过配合使用这两种删除策略，服务器可以很好地在合理使用CPU时间以及避免浪费内存之间取得平衡。</p>
<h4 id="惰性删除策略的实现">惰性删除策略的实现</h4>
<p>惰性删除主要由函数expireIfNeeded实现，函数内部会判断键是否过期，过期则删除，否则无操作。</p>
<pre><code>// 文件地址：db.c
int expireIfNeeded(redisDb *db, robj *key, int force_delete_expired) {
    // 键未过期，直接返回
    if (!keyIsExpired(db,key)) return 0;
  
    // 删除操作由主副本执行
    if (server.masterhost != NULL) {
        if (server.current_client == server.master) return 0;
        if (!force_delete_expired) return 1;
    }

    // 如果客户端暂停，我们将保持当前数据集不变，不过redis服务器仍会将其认为正确的状态返回给客户端。 
    // 通常在暂停结束时，redis会过期该数据键，或者将故障转移并且新的主节点将向redis发送过期消息。
    if (checkClientPauseTimeoutAndReturnIfPaused()) return 1;

    /* Delete the key */
    deleteExpiredKeyAndPropagate(db,key);
    return 1;
}

</code></pre>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320018.png" alt="img" loading="lazy"></figure>
<h4 id="定期删除策略的实现">定期删除策略的实现</h4>
<p>定期删除主要由函数activeExpireCycle实现，函数每次运行时，都从一定数量的数据库中取出一定数量的随机键进行检查，并删除其中的过期键。</p>
<p>函数中的current_db具备一些全局状态，此变量会记录当前activeExpireCycle函数检查的进度，并在下一次activeExpireCycle函数调用时，接着上一次进度进行处理。比如，当前activeExpireCycle函数在遍历3号数据库返回了，那么下一次activeExpireCycle进来时，将从4号数据库开始查找并删除过期键。</p>
<p>随着activeExpireCycle函数的不断执行，服务器中的所有数据库都会被检查一遍，到了这时，函数中的current_db变量将重置为0，表示下一次将开始新一轮的检查工作。</p>
<pre><code>// 文件地址：expire.c
#define ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP 20 /* Keys for each DB loop. */
#define ACTIVE_EXPIRE_CYCLE_FAST_DURATION 1000 /* Microseconds. */
#define ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC 25 /* Max % of CPU to use. */
#define ACTIVE_EXPIRE_CYCLE_ACCEPTABLE_STALE 10 /* % of stale keys after which
                                                   we do extra efforts. */
void activeExpireCycle(int type) {
    /* Adjust the running parameters according to the configured expire
     * effort. The default effort is 1, and the maximum configurable effort
    * is 10. */
    unsigned long
    // 通过配置的过期因子(expire effort)调整运行参数，该值默认为1，最大为10
    effort = server.active_expire_effort-1, /* Rescale from 0 to 9. */
    // 每次循环遍历的键个数
    config_keys_per_loop = ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP +
                           ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP/4*effort,
    // 快速遍历时间范围            
    config_cycle_fast_duration = ACTIVE_EXPIRE_CYCLE_FAST_DURATION +
                                 ACTIVE_EXPIRE_CYCLE_FAST_DURATION/4*effort,
    // 慢速遍历检查时间片                         
    config_cycle_slow_time_perc = ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC +
                                  2*effort,
    //  已经到期的数据/检查数据，得出一个可以接受的比例                         
    config_cycle_acceptable_stale = ACTIVE_EXPIRE_CYCLE_ACCEPTABLE_STALE-
                                    effort;

    // 此变量具有一些全局状态，以便在调用之间以增量方式继续工作。
    static unsigned int current_db = 0; /* Next DB to test. */
    // 检查是否已超时
    static int timelimit_exit = 0;      /* Time limit hit in previous call? */
    // 上一次快速检查数据的起始时间
    static long long last_fast_cycle = 0; /* When last fast cycle ran. */

    // iteration迭代检查个数，每16次循环遍历，确认一下是否检查超时
    int j, iteration = 0;
    // 每次周期性检查的数据库个数，redis默认16个库
    int dbs_per_call = CRON_DBS_PER_CALL;
    long long start = ustime(), timelimit, elapsed;

    // 如果客户端被暂停了，此时操作也需被终止。主要是为了保留现场，不允许修改相关数据
    if (checkClientPauseTimeoutAndReturnIfPaused()) return;

    if (type == ACTIVE_EXPIRE_CYCLE_FAST) {
        /* Don't start a fast cycle if the previous cycle did not exit
         * for time limit, unless the percentage of estimated stale keys is
         * too high. Also never repeat a fast cycle for the same period
         * as the fast cycle total duration itself. */
        // 如果前一个循环没有达到时间限制而退出，则不要启动快速循环，除非预估的过期键百分比太高。 
        // 同时，永远不要在与快速循环总持续时间本身相同的周期内重复快速循环 
        if (!timelimit_exit &amp;&amp;
            server.stat_expired_stale_perc &lt; config_cycle_acceptable_stale)
            return;

        // 限制快速检查频次，在两个config_cycle_fast_duration内，只能执行一次快速检查
        if (start &lt; last_fast_cycle + (long long)config_cycle_fast_duration*2)
            return;

        last_fast_cycle = start;
    }

     /* 我们通常应该在每次迭代时测试 CRON_DBS_PER_CALL，使用
      * 两个例外：
      *
      * 1) 不要测试比我们更多的数据库。
      * 2) 如果上次我们达到时间限制，我们想扫描所有数据库
      * 在这个迭代中，因为在某些数据库中需要做一些工作，我们不希望过期的键使用内存太长时间。 */
    if (dbs_per_call &gt; server.dbnum || timelimit_exit)
        dbs_per_call = server.dbnum;

    /* 我们可以在每次迭代中使用 CPU 时间的最大“config_cycle_slow_time_perc”百分比。 
     * 由于此函数以每秒 server.hz 次的频率调用，以下是我们可以在此函数中花费的最大微秒数。
     * 其中server.hz默认为10，hz是执行后台任务的频率，越大执行越频繁。
     */
    timelimit = config_cycle_slow_time_perc*1000000/server.hz/100;
    timelimit_exit = 0;
    if (timelimit &lt;= 0) timelimit = 1;
  
    // 快速检查模式下，需要更新检查周期时间
    if (type == ACTIVE_EXPIRE_CYCLE_FAST)
        timelimit = config_cycle_fast_duration; /* in microseconds. */

    // 过期键操作期间，会累积一些全局统计信息，以了解逻辑上已经过期但仍存在于数据库中的键的数量
    // 总检查数
    long total_sampled = 0;
    // 总过期数
    long total_expired = 0;

    /* Sanity: There can't be any pending commands to propagate when
     * we're in cron */
    // 在定期任务执行过程中，不允许有挂起的命令传播。 
    serverAssert(server.also_propagate.numops == 0);
    server.core_propagates = 1;
    server.propagate_no_multi = 1;

    for (j = 0; j &lt; dbs_per_call &amp;&amp; timelimit_exit == 0; j++) {
        /* 在单个循环内进行过期检查 */
        unsigned long expired, sampled;

        redisDb *db = server.db+(current_db % server.dbnum);

        // 递增db 
        current_db++;

        // 遍历数据库以检查过期数据，直到超出检查周期时间，或者过期数据比例低于阈值
        do {
            unsigned long num, slots;
            long long now, ttl_sum;
            int ttl_samples;
            iteration++;

            /* 不存在过期时间，进入下一个db */
            if ((num = dictSize(db-&gt;expires)) == 0) {
                db-&gt;avg_ttl = 0;
                break;
            }
            slots = dictSlots(db-&gt;expires);
            now = mstime();

            /* 过期存储数据结构是字典，数据经过处理后，字典存储的数据可能已经很少，
             * 但是字典还是大字典，这样遍历数据有效命中率会很低，处理起来会浪费资源，
             * 后面的访问会很快触发字典的缩容，缩容后再进行处理效率更高。
             * 因此，遇到这种情况，可以先跳过，进入下一个db处理过期数据*/
            if (slots &gt; DICT_HT_INITIAL_SIZE &amp;&amp;
                (num*100/slots &lt; 1)) break;

            /* 主要收集周期。 在具有过期集的密钥中采样随机密钥，检查过期密钥。*/
            // 过期数
            expired = 0;
            // 采样数
            sampled = 0;
            // 没有过期的数据时间差之和
            ttl_sum = 0;
            // 没有过期的数据个数
            ttl_samples = 0;
        
            // 每次检查的数据限制
            if (num &gt; config_keys_per_loop)
                num = config_keys_per_loop;

            // 最大的扫描桶数 
            long max_buckets = num*20;
            // 已被检查过的桶数
            long checked_buckets = 0;

            // 一个桶上有可能有多个数据。所以检查从两方面限制：一个是数据量，一个是桶的数量。
            while (sampled &lt; num &amp;&amp; checked_buckets &lt; max_buckets) {
                // 如果dict没有扩容，不需要检查它的第二张表
                for (int table = 0; table &lt; 2; table++) {
                    if (table == 1 &amp;&amp; !dictIsRehashing(db-&gt;expires)) break;

                    unsigned long idx = db-&gt;expires_cursor;
                    idx &amp;= DICTHT_SIZE_MASK(db-&gt;expires-&gt;ht_size_exp[table]);
                    dictEntry *de = db-&gt;expires-&gt;ht_table[table][idx];
                    long long ttl;

                    /* 扫描当前表的当前桶。 */
                    checked_buckets++;
                    while(de) {
                        /* 立即获取下一个条目，因为此条目可能会被删除。*/
                        dictEntry *e = de;
                        de = de-&gt;next;
                    
                        // 检查数据是否已经超时
                        ttl = dictGetSignedIntegerVal(e)-now;
                        // 数据过期，便进行回收处理
                        if (activeExpireCycleTryExpire(db,e,now)) expired++;
                        if (ttl &gt; 0) {
                            /* 获取尚未过期键的平均 TTL。*/
                            ttl_sum += ttl;
                            ttl_samples++;
                        }
                        sampled++;
                    }
                }
                db-&gt;expires_cursor++;
            }
            total_expired += expired;
            total_sampled += sampled;

            /* Update the average TTL stats for this database. */
            if (ttl_samples) {
                long long avg_ttl = ttl_sum/ttl_samples;

                /* 用几个样本做一个简单的平均值采样处理。
                 * 当前估计的权重为 2%，上一次的估计权重为 98%。 */
                if (db-&gt;avg_ttl == 0) db-&gt;avg_ttl = avg_ttl;
                db-&gt;avg_ttl = (db-&gt;avg_ttl/50)*49 + (avg_ttl/50);
            }

            /* 即使有很多密钥要过期，我们也不能在这里永远阻塞。 
             * 所以在给定的毫秒数之后返回调用者等待另一个活跃的过期周期。 
             * 即检查是否超时，超时便退出*/
            if ((iteration &amp; 0xf) == 0) { /* check once every 16 iterations. */
                elapsed = ustime()-start;
                if (elapsed &gt; timelimit) {
                    timelimit_exit = 1;
                    server.stat_expired_time_cap_reached_count++;
                    break;
                }
            }
            /* 如果有可接受数量的过期键（逻辑上过期但尚未回收），我们不会为当前数据库重复循环。 
             * 简而言之，即当前数据库，如果没有检查到数据，或者过期数据已经达到可接受比例
             * 就退出该数据库检查，进入到下一个数据库检查，防止重复检查。*/
        } while (sampled == 0 ||
                 (expired*100/sampled) &gt; config_cycle_acceptable_stale);
    }

    serverAssert(server.core_propagates); /* This function should not be re-entrant */

    /* 传播 DELs 命令 */
    propagatePendingCommands();

    server.core_propagates = 0;
    server.propagate_no_multi = 0;
    // 统计信息
    elapsed = ustime()-start;
    server.stat_expire_cycle_time_used += elapsed;
    latencyAddSampleIfNeeded(&quot;expire-cycle&quot;,elapsed/1000);

    /* 更新当前过期键的百分比。 此样本运行权重占 5%。 */
    double current_perc;
    if (total_sampled) {
        current_perc = (double)total_expired/total_sampled;
    } else
        current_perc = 0;
    // 通过累加每次检查的过期概率影响，保存过期数据占数据比例。  
    server.stat_expired_stale_perc = (current_perc*0.05)+
                                     (server.stat_expired_stale_perc*0.95);
}

</code></pre>
<h3 id="持久化及复制时对过期键的处理">持久化及复制时对过期键的处理</h3>
<h4 id="rdb">RDB</h4>
<p>在执行SAVE或BGSAVE命令时会创建一个新的RDB文件，后台程序运行期间会对数据库中的键进行检查，已经过期的键不会被保存到新创建的RDB文件中。</p>
<p>加载RDB文件时，需要区分主从服务器，如下：</p>
<ul>
<li>主服务器加载RDB：程序启动时，会自动检查文件中的键，未过期的键可以被载入，而过期键则会被忽略。</li>
<li>从服务器加载RDB：程序启动时，会自动加载文件中的所有数据键，无论是是否已过期；此处过期键会对从服务器造成脏数据影响，实际上，在主从服务器同步过程中，从服务器的数据会被清空，因此，过期键一般不会对从服务器造成影响。</li>
</ul>
<h4 id="aof">AOF</h4>
<p>如果redis服务器开启了AOF持久化功能，那么当数据库中的某个键过期，但还未被删除时，并不会对AOF持久化形成脏数据。</p>
<p>因为，当过期键被惰性删除或定期删除时，程序会向AOF文件追加一条DEL命令，来显式地记录该键已被删除。</p>
<p>例如，如果客户端使用了get命令获取msg时，如果发现该msg已经过期，那么服务器会从数据库中删除该键，同时追加一条DEL命令到AOF文件，随后返回客户端空值。</p>
<p>重载AOF文件时，基本流程与RDB一样，主服务器都会去检查键是否过期，过期的话将不会载入。</p>
<h4 id="复制">复制</h4>
<p>主从复制模式下，从服务器的过期键删除动作由主服务器控制。也即是说，只有当主服务器主动删除了一个过期键，并将此命令显式的同步至从服务器，从服务器才会去删除这个过期键；否则从服务器不会做任何操作。</p>
<p>这种通过主服务器控制从服务器的模式主要是为了保证主从数据的一致性。不过需要注意的是，在这种模式下，一个数据键如果已经过期，此时，一个get命令进入，并是由从服务器响应，那么从服务器并不会执行过期键操作，而是正常返回，就像这个键并未过期一般。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深入Redis之内部数据结构(下)]]></title>
        <id>https://philosopherzb.github.io/post/shen-ru-redis-zhi-nei-bu-shu-ju-jie-gou-xia/</id>
        <link href="https://philosopherzb.github.io/post/shen-ru-redis-zhi-nei-bu-shu-ju-jie-gou-xia/">
        </link>
        <updated>2022-12-17T08:11:02.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Redis内部数据结构下篇。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/saturn-341379_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="内部数据结构">内部数据结构</h2>
<h3 id="字典">字典</h3>
<p>字典（dict），又称为符号表（symbol table）、关联数组（associative array）或映射（map），是一种用于保存键值对的抽象数据结构。</p>
<p>字典（dict）是 redis 最重要的数据结构之一，使用频率与SDS不相上下，在redisDb（实现数据库键空间）、hash（底层数据结构）以及服务器内部需要用到hashmap的场景都是用dict来实现的。</p>
<p>在Redis中，字典以哈希表作为底层实现,一个哈希表有多个哈希节点,而每个哈希节点保存了一个字典的键值对。</p>
<pre><code>// 文件地址：dict.h
struct dict {
    // 特定类型处理函数
    dictType *type;
  
    // 两个hash表---&gt;用于字典扩容与缩减
    // 一般情况下，字典只使用ht[0] 哈希表, ht[1]哈希表只会对ht[0]哈希表进行rehash时使用。
    dictEntry **ht_table[2];
    // hash表的对应的键值对个数-----&gt;即已使用的空间数
    unsigned long ht_used[2];
  
    // 记录rehash进度的标志，如果为-1则表示rehash未进行
    long rehashidx; /* rehashing not in progress if rehashidx == -1 */

    /* Keep small vars at end for optimal (minimal) struct padding */
    int16_t pauserehash; /* If &gt;0 rehashing is paused (&lt;0 indicates coding error) */
    signed char ht_size_exp[2]; /* exponent of size. (size = 1&lt;&lt;exp) */
};

typedef struct dictType {
    // 计算hash值的函数
    uint64_t (*hashFunction)(const void *key);
    // 复制键的函数
    void *(*keyDup)(dict *d, const void *key);
    // 复制值的函数
    void *(*valDup)(dict *d, const void *obj);
    // 键比较函数
    int (*keyCompare)(dict *d, const void *key1, const void *key2);
    // 摧毁键的函数
    void (*keyDestructor)(dict *d, void *key);
    // 摧毁值的函数
    void (*valDestructor)(dict *d, void *obj);
    // 允许扩展的函数
    int (*expandAllowed)(size_t moreMem, double usedRatio);
    /* Allow a dictEntry to carry extra caller-defined metadata.  The
     * extra memory is initialized to 0 when a dictEntry is allocated. */
     // 额外的元数据
    size_t (*dictEntryMetadataBytes)(dict *d);
} dictType;

typedef struct dictEntry {
    // 键
    void *key;
    // 值
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    // 指向下个hash表的节点，形成链表
    // 这个指针可以将多个哈希值相同的键值对连接在一起，解决键冲突问题----&gt;典型的拉链法解决hash冲突
    struct dictEntry *next;     /* Next entry in the same hash bucket. */
    // 记录了 dictEntryMetadataBytes() 函数返回的字节数
    void *metadata[];           /* An arbitrary number of bytes (starting at a
                                 * pointer-aligned address) of size as returned
                                 * by dictType's dictEntryMetadataBytes(). */
} dictEntry;

</code></pre>
<h4 id="rehash">rehash</h4>
<p>Redis中的字典使用hash表作为底层数据结构，并且使用了拉链法来解决hash冲突的问题；所谓拉链法指的是多个hash值相同的key分配到同一节点上时，列表节点会自动使用链表拉长自己的实体。</p>
<p>如上述源码所见，每个hash表节点都有一个next指针，该指针便指向了同一个hash桶中的下一个实体，通过不断拉长链表可以解决hash键冲突的问题。</p>
<p>对于使用拉链法来解决碰撞问题的hash表来说，hash表的性能依赖于它的大小（size 属性）和它所保存的节点的数量（used 属性）之间的比率：</p>
<ul>
<li>比率在 1:1 时，哈希表的性能最好；</li>
<li>如果节点数量比hash表的大小要大很多的话，那么hash表就会退化成多个链表，hash表本身的性能优势就不再存在；</li>
</ul>
<p>为了在字典的键值对不断增多的情况下保持良好的性能，字典需要对所使用的哈希表（ht[0]）进行 rehash 操作：在不修改任何键值对的情况下，对哈希表进行扩容，尽量将比率维持在 1:1 左右。</p>
<p>dictAdd 在每次向字典添加新键值对之前，都会对哈希表 ht[0] 进行检查，对于 ht[0] 的 size 和 used 属性，如果它们之间的比率 ratio = used / size 满足以下任何一个条件的话， rehash 过程就会被激活：</p>
<ul>
<li>自然 rehash ：ratio &gt;= 1 ，且变量 dict_can_resize （执行后台任务时，会将其置为假，避免执行自然rehash，从而减少程序对内存的碰撞）为真。</li>
<li>强 制 rehash ： ratio 大 于 变 量 dict_force_resize_ratio （dict_force_resize_ratio = 5 ）。</li>
</ul>
<pre><code>// 文件地址：dict.c
/* Using dictEnableResize() / dictDisableResize() we make possible to
 * enable/disable resizing of the hash table as needed. This is very important
 * for Redis, as we use copy-on-write and don't want to move too much memory
 * around when there is a child performing saving operations.
 *
 * Note that even when dict_can_resize is set to 0, not all resizes are
 * prevented: a hash table is still allowed to grow if the ratio between
 * the number of elements and the buckets &gt; dict_force_resize_ratio. */
// 通过 dict_can_resize  参数可以控制非强制rehash的执行
static int dict_can_resize = 1;
// 如果达到了强制rehash比率 dict_force_resize_ratio = 5 那么即使dict_can_resize 为假也可以执行rehash
static unsigned int dict_force_resize_ratio = 5;

/* 检查dict是否需要扩容 */
static int _dictExpandIfNeeded(dict *d)
{
    /* 已经处于渐进式rehash流程中，直接返回ok */
    if (dictIsRehashing(d)) return DICT_OK;

    /* If the hash table is empty expand it to the initial size. */
    if (DICTHT_SIZE(d-&gt;ht_size_exp[0]) == 0) return dictExpand(d, DICT_HT_INITIAL_SIZE);

    /* If we reached the 1:1 ratio, and we are allowed to resize the hash
     * table (global setting) or we should avoid it but the ratio between
     * elements/buckets is over the &quot;safe&quot; threshold, we resize doubling
     * the number of buckets. */
    //  DICTHT_SIZE(d-&gt;ht_size_exp[0] 会将size左移一位，即乘上2
    if (d-&gt;ht_used[0] &gt;= DICTHT_SIZE(d-&gt;ht_size_exp[0]) &amp;&amp;
        (dict_can_resize ||
         d-&gt;ht_used[0]/ DICTHT_SIZE(d-&gt;ht_size_exp[0]) &gt; dict_force_resize_ratio) &amp;&amp;
        dictTypeExpandAllowed(d))
    {
        return dictExpand(d, d-&gt;ht_used[0] + 1);
    }
    return DICT_OK;
}

</code></pre>
<pre><code>// 文件地址：dict.c
// rehash的过程比较简单，主要是利用ht[1]进行复制操作；具体如下：
// 1. 创建一个比 ht[0]-&gt;table 更大/更小的 ht[1]-&gt;table
// 2. 将 ht[0]-&gt;table 中的所有键值对迁移到 ht[1]-&gt;table，此过程中会重新计算key的hash值
// 3. 将原有 ht[0] 的数据清空，并将 ht[1] 替换为新的 ht[0]，腾出ht[1]以方便下次rehash操作
int dictRehash(dict *d, int n) {
    int empty_visits = n*10; /* Max number of empty buckets to visit. */
    if (!dictIsRehashing(d)) return 0;

    while(n-- &amp;&amp; d-&gt;ht_used[0] != 0) {
        dictEntry *de, *nextde;

        /* Note that rehashidx can't overflow as we are sure there are more
         * elements because ht[0].used != 0 */
        assert(DICTHT_SIZE(d-&gt;ht_size_exp[0]) &gt; (unsigned long)d-&gt;rehashidx);
        while(d-&gt;ht_table[0][d-&gt;rehashidx] == NULL) {
            d-&gt;rehashidx++;
            if (--empty_visits == 0) return 1;
        }
        de = d-&gt;ht_table[0][d-&gt;rehashidx];
        /* Move all the keys in this bucket from the old to the new hash HT */
        // 循环计算ht[0]中的数据并将其放入ht[1]中
        while(de) {
            uint64_t h;

            nextde = de-&gt;next;
            /* Get the index in the new hash table */
            h = dictHashKey(d, de-&gt;key) &amp; DICTHT_SIZE_MASK(d-&gt;ht_size_exp[1]);
            de-&gt;next = d-&gt;ht_table[1][h];
            d-&gt;ht_table[1][h] = de;
            d-&gt;ht_used[0]--;
            d-&gt;ht_used[1]++;
            de = nextde;
        }
        d-&gt;ht_table[0][d-&gt;rehashidx] = NULL;
        d-&gt;rehashidx++;
    }

    /* Check if we already rehashed the whole table... */
    // 当完成了迁移操作后
    if (d-&gt;ht_used[0] == 0) {
        // 释放ht[0]空间
        zfree(d-&gt;ht_table[0]);
        /* Copy the new ht onto the old one */
        // 将ht[1]赋值给ht[0]，腾出ht[1]以方便下次rehash操作
        d-&gt;ht_table[0] = d-&gt;ht_table[1];
        d-&gt;ht_used[0] = d-&gt;ht_used[1];
        d-&gt;ht_size_exp[0] = d-&gt;ht_size_exp[1];
        _dictReset(d, 1);
        d-&gt;rehashidx = -1;
        return 0;
    }

    /* More to rehash... */
    return 1;
}

</code></pre>
<h4 id="渐进式rehash">渐进式rehash</h4>
<p>从上一节可以看出rehash的过程就是循环将ht[0]的数据重新分布到ht[1]中的操作，需要注意的是，此过程并非一次性，集中式的完成，而是分为多次，渐进式的完成。之所以这样做，主要是由于当数据量过大时，集中式的一次性rehash会导致程序整个阻塞，这非常不友好。</p>
<p>为了解决一次性扩容耗时过多的情况，redis将扩容操作穿插在插入操作的过程中，分批完成。当负载因子触达阈值之后，只申请新空间，但并不将老的数据搬移到新散列表中。当有新数据要插入时，将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。</p>
<p>Redis为了解决这个问题采用渐进式rehash方式。以下是Redis渐进式rehash的详细步骤:</p>
<ol>
<li>为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。</li>
<li>在字典中维持一个索引计数器变量 rehashidx ，并将它的值设置为 0 ，表示 rehash 工作正式开始。</li>
<li>在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ，当 rehash 工作完成之后，程序将 rehashidx 属性的值增一。</li>
<li>随着字典操作的不断执行，最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ，这时程序将 rehashidx 属性的值设为 -1 ，表示 rehash 操作已完成。</li>
</ol>
<h3 id="跳跃表">跳跃表</h3>
<p>跳跃表（简称跳表，skiplist），是用于有序元素序列快速搜索查找的一个数据结构，跳表是一个随机化的数据结构，实质就是一种可以进行二分查找的有序链表。</p>
<p>跳表在原有的有序链表上面增加了多级索引，通过索引来实现快速查找。跳表不仅能提高搜索性能，同时也可以提高插入和删除操作的性能。它在性能上和红黑树，AVL树不相上下，但是跳表的原理非常简单，实现也比红黑树简单很多。</p>
<p>跳跃表支持平均O(logN)，最坏O(N)复杂度的节点查找，还可以通过顺序性操作来批量处理节点。</p>
<p>Redis使用跳跃表作为有序集合键的底层实现之一,如果一个有序集合包含的元素数量比较多,又或者有序集合中元素的成员是比较长的字符串时, Redis就会使用跳跃表来作为有序集合健的底层实现。</p>
<p>跳表在原有的有序链表上面增加了多级索引这种操作的本质是以空间换时间。当数据量较多，或者原始链表对象非常大时，使用这种数据结果将会带来显著的性能提升。</p>
<pre><code>// 文件地址：server.h
typedef struct zskiplistNode {
    // 元素，存储实际的数据对象
    sds ele;
    // 分值，用于排序
    double score;
    // 后退指针
    struct zskiplistNode *backward;
    // 跳表层
    struct zskiplistLevel {
        // 前进指针
        struct zskiplistNode *forward;
        // 跨度
        unsigned long span;
    } level[];
} zskiplistNode;

typedef struct zskiplist {
    // 头尾节点，通过这两个节点，程序定位表头和表位的时间复杂度为O(1)
    struct zskiplistNode *header, *tail;
    // 节点数量，此参数可以让程序以O(1)的复杂度返回跳跃表的长度
    unsigned long length;
    // 当前表内节点的最大层数
    int level;
} zskiplist;

</code></pre>
<h4 id="跳跃表的实现">跳跃表的实现</h4>
<p>Redis的跳跃表由zskiplistNode和zskiplist两个结构共同组成。其中 zskiplistNode结构用于表示跳跃表节点,而 zskiplist结构则用于保存跳跃表节点的相关信息,比如节点的数量,以及指向表头节点和表尾节点的指针等等。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320017.png" alt="img" loading="lazy"></figure>
<p>上图展示了一个跳跃表示例,其中最左边的是 zskiplist结构,该结构包含以下属性。</p>
<ul>
<li>header: 指向跳跃表的表头节点，通过这个指针程序定位表头节点的时间复杂度就为O(1)</li>
<li>tail: 指向跳跃表的表尾节点,通过这个指针程序定位表尾节点的时间复杂度就为O(1)</li>
<li>level: 记录目前跳跃表内,层数最大的那个节点的层数(表头节点的层数不计算在内)，通过这个属性可以在O(1)的时间复杂度内获取层高最好的节点的层数。</li>
<li>length: 记录跳跃表的长度,也即是,跳跃表目前包含节点的数量(表头节点不计算在内)，通过这个属性，程序可以在O(1)的时间复杂度内返回跳跃表的长度。</li>
</ul>
<p>结构右方的是四个 zskiplistNode结构,该结构包含以下属性。</p>
<ul>
<li>
<p>层(level):</p>
<p>节点中用L1、L2、L3等字样标记节点的各个层,L1代表第一层,L2代表第二层,以此类推。</p>
<p>每个层都带有两个属性: 前进指针和跨度。前进指针用于访问位于表尾方向的其他节点,而跨度则记录了前进指针所指向节点和当前节点的距离(跨度越大、距离越远)。在上图中,连线上带有数字的箭头就代表前进指针,而那个数字就是跨度。当程序从表头向表尾进行遍历时,访问会沿着层的前进指针进行。</p>
<p>跳跃表节点的level数组可以包含多个元素，每个元素都包含一个指向其他节点的指针，程序可以通过这些层来加快访问其他节点的速度，一般来说，层的数量越多，访问其他节点的速度就越快。</p>
<p>每次创建一个新跳跃表节点的时候,程序都根据幂次定律(powerlaw,越大的数出现的概率越小)随机生成一个介于1和32之间的值作为level数组的大小,这个大小就是层的“高度”。</p>
</li>
<li>
<p>后退(backward)指针：</p>
<p>节点中用BW字样标记节点的后退指针,它指向位于当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。与前进指针所不同的是每个节点只有一个后退指针，因此每次只能后退一个节点。</p>
</li>
<li>
<p>分值(score):</p>
<p>节点的分值是一个double类型的浮点数，就如上图中的各个节点的1.0、1.0和3.0便是该节点所保存的分值。在跳跃表中,节点按各自所保存的分值从小到大排列。</p>
<p>在同一个跳跃表中，分值字段允许重复；分值相同的节点将按照元素在字典序中的大小进行排序，元素对象较小的节点会排在前面(靠近表头的方向),而元素对象较大的节点则会排在后面(靠近表尾的方向)。</p>
</li>
<li>
<p>元素(ele):</p>
<p>各个节点中的o1、o2和o3是节点所保存的实体元素。需要注意的是，在同一个跳跃表中,各个节点保存的实体元素必须是唯一的。</p>
</li>
</ul>
<h3 id="整数集合">整数集合</h3>
<p>整数集合(intset)是集合键的底层实现之一；当一个集合只包含整数值元素,并且这个集合的元素数量不多时, Redis 就会使用整数集合作为集合键的底层实现。</p>
<p>整数集合并非一个基础的数据结构，而是由Redis专门设计的用来保存整数值的集合抽象数据结构，它可以保存int16_t、int32_t或者int64_t的整数值，并且可以保证集合中不会出现重复元素。</p>
<pre><code>// 文件地址：intset.h
typedef struct intset {
    // 编码方式
    uint32_t encoding;
    // 集合保存的元素数量
    uint32_t length;
    // 保存元素的数组
    int8_t contents[];
} intset;

</code></pre>
<ul>
<li>contents数组是整数集合的底层实现，整数集合的每个元素都是 contents数组的个数组项(item),各个项在数组中按值的大小从小到大有序地排列,并且数组中不包含任何重复项。</li>
<li>length属性记录了整数集合包含的元素数量，也即contents数组的长度。</li>
<li>intset结构虽然将contents属性声明为int8_t类型的数组,但实际上 contents数组并不保存任何int8_t类型的值, contents数组的真正类型取决于encoding属性的值。encoding属性的值包括：INTSET_ENC_INT16、INTSET_ENC_INT32以及INTSET_ENC_INT64。</li>
</ul>
<h4 id="整数集合升级">整数集合升级</h4>
<p>每当我们要将一个新元素添加到整数集合里面,并且新元素的类型比整数集合现有所有元素的类型都要长时,整数集合需要先进行升级,然后才能将新元素添加到整数集合里面。</p>
<p>升级整数集合并添加新元素主要分三步来进行。</p>
<ol>
<li>根据新元素的类型,扩展整数集合底层数组的空间大小,并为新元素分配空间。</li>
<li>将底层数组现有的所有元素都转换成与新元素相同的类型,并将类型转换后的元素放置到正确的位上,而且在放置元素的过程中,需要继续维持底层数组的有序性质不变。</li>
<li>将新元素添加到底层数组里面。</li>
</ol>
<h4 id="升级好处">升级好处</h4>
<ul>
<li>
<p>提升灵活性</p>
<p>因为C语言是静态类型语言,为了避免类型错误,一般情况下不会将两种不同类型的值放在同一个数据结构里面。</p>
<p>例如,一般只使用int16_t类型的数组来保存int16_t类型的值,只使用int32_t类型的数组来保存int32_t类型的值,诸如此类。但是,因为整数集合可以通过自动升级底层数组来适应新元素,所以在Redis中可以随意地将int16_t、int32_t或者int64_t类型的整数添加到集合中,而不必担心出现类型错误,这种做法非常灵活。</p>
</li>
<li>
<p>节约内存</p>
<p>要让一个数组可以同时保存int16_t、int32_t、int64_t三种类型的值,最简单的做法就是直接使用int64t类型的数组作为整数集合的底层实现。不过这样一来,即使添加到整数集合里面的都是int16_t类型或者int32_t类型的值,数组都需要使用int64_t类型的空间去保存它们,从而出现浪费内存的情况。</p>
<p>而整数集合现在的做法既可以让集合能同时保存三种不同类型的值,又可以确保升级操作只会在有需要的时候进行,这可以尽量节省内存。</p>
<p>例如，如果我们一直只向整数集合添加int16_t类型的值,那么整数集合的底层实现就会一直是int16_t类型的数组,只有在我们要将int32_t类型或者int64_t类型的值添加到集合时,程序才会对数组进行升级。</p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深入Redis之内部数据结构(上)]]></title>
        <id>https://philosopherzb.github.io/post/shen-ru-redis-zhi-nei-bu-shu-ju-jie-gou-shang/</id>
        <link href="https://philosopherzb.github.io/post/shen-ru-redis-zhi-nei-bu-shu-ju-jie-gou-shang/">
        </link>
        <updated>2022-12-03T08:03:13.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Redis内部数据结构上篇。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/desert-65310_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="内部数据结构">内部数据结构</h2>
<h3 id="概要">概要</h3>
<p>Redis是一个使用ANSI C编写的开源、支持网络、基于内存、可选持久性的键值对存储数据库。从2015年6月开始，Redis的开发由Redis Labs赞助，而2013年5月至2015年6月期间，其开发由Pivotal赞助。在2013年5月之前，其开发由VMware赞助。根据月度排行网站DB-Engines.com的数据显示，Redis是最流行的键值对存储数据库。</p>
<p>Redis 和其他很多 key-value 数据库的不同之处在于，Redis 不仅支持简单的字符串键值对，还提供了一系列数据结构类型值，比如列表、哈希、集合和有序集，并在这些数据结构类型上定义了一套强大的 API。</p>
<p>依靠这些数据结构类型值，Redis 可以很轻易地完成其他只支持字符串键值对的 key-value 数据库很难（或者无法）完成的任务。</p>
<p>在 Redis 的内部，数据结构类型值由高效的数据结构和算法进行支持，并且在 Redis 自身的构建当中，也大量用到了这些数据结构。以下将详细介绍其中动态字符串、链表、字典、跳跃表等数据结构。</p>
<h3 id="简单动态字符串">简单动态字符串</h3>
<p>在 C 语言中，字符串实际上是使用空字符 \0 结尾的一维字符数组。因此，\0 是用于标记字符串的结束。空字符（Null character）又称结束符，缩写 NUL，是一个数值为 0 的控制字符，\0 是转义字符，意思是告诉编译器，这不是字符 0，而是空字符。</p>
<p>Redis虽然是使用了ANSI C 语言编写的开源项目，但redis中的字符串却并没有直接采用 C 语言中的字符串，而是自己定义了一套抽象类型，名为simple dynamic string, 简称SDS。本质上是对 C 语言中的char进行一层封装。而其之所以使用SDS的原因如下所示：</p>
<table>
<thead>
<tr>
<th>C字符串</th>
<th>SDS</th>
</tr>
</thead>
<tbody>
<tr>
<td>获取字符串长度的复杂度为 O(N)</td>
<td>获取字符串长度的复杂度为 O(1)</td>
</tr>
<tr>
<td>操作字符串函数不安全，可能造成缓冲区溢出</td>
<td>安全的操作字符串API，避免缓冲区溢出</td>
</tr>
<tr>
<td>修改字符串长度 N 次必然需要执行 N 次内存重分配</td>
<td>修改字符串长度 N 次最多需要执行 N 次内存重分配</td>
</tr>
<tr>
<td>只能保存文本数据</td>
<td>可以保存文本以及图片、音频、视频、压缩文件这样的二进制数据。</td>
</tr>
</tbody>
</table>
<p>SDS作为一种抽象的数据结构，主要由以下两部分构成。注意其中定义了五种结构，主要是为了让不同的字符串使用不同长度的struct，从而节省内存。</p>
<pre><code>// 文件地址：sds.h
// 定义char 别名：sds
typedef char *sds;
// 实际上有五种结构体，但其实现都一致，所以此处仅以其中一种作为说明。
struct __attribute__ ((__packed__)) sdshdr64 {
    // 记录buf数组中已使用的字节数量，计算字符串长度时，复杂度可降为O(1)
    uint64_t len; /* used */
    // 分配的buf数组长度，不包括头和空字符结尾，减少内存重分配次数
    uint64_t alloc; /* excluding the header and null terminator */
    // 标志位， 最低3位表示struct类型，另外5个位没有使用
    unsigned char flags; /* 3 lsb of type, 5 unused bits */
    // 字符数组，用于存储实际的字符串
    char buf[];
};

// 此处定义对应上述结构体中的 flags
#define SDS_TYPE_5  0
#define SDS_TYPE_8  1
#define SDS_TYPE_16 2
#define SDS_TYPE_32 3
#define SDS_TYPE_64 4

</code></pre>
<p>SDS 遵循 C 字符串以空字符结尾的惯例， 保存空字符的 1 字节空间不计算在 SDS 的 len 属性里面，并且为空字符分配额外的 1 字节空间，以及添加空字符到字符串末尾等操作都是由 SDS 函数自动完成的，所以这个空字符对于 SDS 的使用者来说是完全透明的。简单例子如下所示：</p>
<pre><code>// 以存储redis为例，结构最终形式如下：
struct sdshdr {
    len = 5
    alloc = 8
    flags = 1
    buf = &quot;redis\0&quot;  // 可以发现buf的实际长度为 buf + 1 因为多了一个结束符
};

</code></pre>
<h4 id="sds-优势">SDS 优势</h4>
<p>传统的C字符串并没有记录字符串长度，而想获取字符串长度时就需要遍历一遍字符串，复杂度为O(N),导致随着字符串长度变长，获取字符串长度的操作复杂度也会增加。</p>
<p>针对这种情况，redis在SDS结构体中设计了len字段，主要用来存储字符串的长度，因此每次获取length时，只需要查询len字段即可，复杂度为O(1)。</p>
<p>SDS另一个优势则是在于其对于追加操作的优化，原有的C字符串对于字符串的追加操作容易造成缓冲区溢出或者重分配次数过多等问题。而SDS则是依靠alloc字段来降低了重分配次数以及可能的缓冲区溢出。</p>
<p>在 C 语言中字符串的长度和底层数组的长度之间存在着关联性（n字符串---&gt;n+1数组，多出的1个用于存储空字符串），每当增长或者缩短一个 C 字符串时，程序都要对保存这个 C 字符串的数组进行一次内存重分配操作：</p>
<ul>
<li>
<p>如果程序执行的是增长字符串的操作，比如拼接操作（append），那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小 —— 如果忘了这一步就会产生缓冲区溢出。</p>
</li>
<li>
<p>如果程序执行的是缩短字符串的操作，比如截断操作（trim），那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间 —— 如果忘了这一步就会产生内存泄漏。</p>
<p>为了减少重分配次数过多的情形，redis增加了alloc字段（已分配数组长度）来解除字符串长度和底层数组长度之间的关联，通过已分配长度字段alloc，SDS实现了空间预分配及惰性空间释放两种优化策略。</p>
</li>
<li>
<p>空间预分配：在redis追加字符串时，SDS不仅仅只是单纯的附加字符串，而且还为alloc字段增加了额外的空间，这样，当下次追加操作进入时，会判断预留空间是否充足，充足便可以直接进行追加，而无需扩展数组长度，避免了缓冲区溢出的问题。同时通过这种技术，SDS将连续增长N次字符串所需的重分配次数从必定N次降为了最多N次。</p>
</li>
<li>
<p>惰性空间释放：在redis截断字符串时，其并不立即使用内存重分配来回收缩短后多出来的字节，而是把缩短后的长度记录在SDS中的len属性中，剩余空间用于未来扩展字符串用。当然也可以手动指定函数释放空闲空间。</p>
</li>
</ul>
<pre><code>// 文件地址：sds.c
// 空间预分配核心函数
// 其中greedy参数控制着是否要分配超出实际所需的空间：即原本只需分配10字节，如果开启了此参数，则会分配20+1字节。
sds _sdsMakeRoomFor(sds s, size_t addlen, int greedy) {
    void *sh, *newsh;
    // 返回可用空间，实际上就是sds中的alloc - len的结果值
    size_t avail = sdsavail(s);
    size_t len, newlen, reqlen;
    char type, oldtype = s[-1] &amp; SDS_TYPE_MASK;
    int hdrlen;
    size_t usable;

    // 如果剩余空间足够，则不会进行预分配操作
    if (avail &gt;= addlen) return s;

    len = sdslen(s);
    // sds的起始位置
    sh = (char*)s-sdsHdrSize(oldtype);
    reqlen = newlen = (len+addlen);
    assert(newlen &gt; len);   /* Catch size_t overflow */
    // 增加额外空间
    if (greedy == 1) {
        // 如果修改后的字符串长度小于SDS_MAX_PREALLOC(1MB)，则分配与原字符串长度一致的额外空间
        // 例如：修改后的字符串长度为10字节，那么此处会为其分配额外的10字节空间，相当于一共分配了20字节空间
        // 实际上是21字节，因为还有一个额外的字节用于存储空字符
        if (newlen &lt; SDS_MAX_PREALLOC)
            newlen *= 2;
        else
            // 否则分配的额外空间 + 1MB +1byte
            newlen += SDS_MAX_PREALLOC;
    }

    // 重新获取结构体----》因为原有的可能表达不了增加后的容量 
    type = sdsReqType(newlen);

    /* Don't use type 5: the user is appending to the string and type 5 is
     * not able to remember empty space, so sdsMakeRoomFor() must be called
     * at every appending operation. */
    if (type == SDS_TYPE_5) type = SDS_TYPE_8;
    // 获取类型对应的长度 
    hdrlen = sdsHdrSize(type);
    assert(hdrlen + newlen + 1 &gt; reqlen);  /* Catch size_t overflow */
    if (oldtype==type) {
        // sh为开始地址，在开始地址的基础上为其分配更多的空间。
        newsh = s_realloc_usable(sh, hdrlen+newlen+1, &amp;usable);
        if (newsh == NULL) return NULL;
        // s指向sds的开始位置
        s = (char*)newsh+hdrlen;
    } else {
        /* Since the header size changes, need to move the string forward,
         * and can't use realloc */
        // 当类型发生变化时，地址内容不可复用，便需要寻找新的可用空间地址
        newsh = s_malloc_usable(hdrlen+newlen+1, &amp;usable);
        if (newsh == NULL) return NULL;
        // 复制原来的字符串到新的sds上
        memcpy((char*)newsh+hdrlen, s, len+1);
        // 释放之前的空间
        s_free(sh);
        // s指向sds的开始位置
        s = (char*)newsh+hdrlen;
        s[-1] = type;
        // 设置len长度值
        sdssetlen(s, len);
    }
    usable = usable-hdrlen-1;
    if (usable &gt; sdsTypeMaxSize(type))
        usable = sdsTypeMaxSize(type);
    // 分配alloc的值  
    sdssetalloc(s, usable);
    // 返回新的sds
    return s;
}

</code></pre>
<pre><code>// 文件地址：sds.c
// 以sdstrim函数简述惰性空间释放
// 从整个函数逻辑可以看出，其并没有调用任何重分配函数，仅仅只是修改了sds中的len字段值。
// 由此可见，惰性空间释放，实际上并没有将已分配的空间释放出来。这也是为了方便后续存在追加操作时，一定程度上无需扩容。
sds sdstrim(sds s, const char *cset) {
    char *end, *sp, *ep;
    size_t len;

    sp = s;
    ep = end = s+sdslen(s)-1;
    while(sp &lt;= end &amp;&amp; strchr(cset, *sp)) sp++;
    while(ep &gt; sp &amp;&amp; strchr(cset, *ep)) ep--;
    len = (ep-sp)+1;
    if (s != sp) memmove(s, sp, len);
    // 补齐末尾的空字符
    s[len] = '\0';
    // 设置sds中的len字段值
    sdssetlen(s,len);
    return s;
}

// 手动释放空闲空间的核心函数
sds sdsRemoveFreeSpace(sds s) {
    void *sh, *newsh;
    char type, oldtype = s[-1] &amp; SDS_TYPE_MASK;
    int hdrlen, oldhdrlen = sdsHdrSize(oldtype);
    // 返回字符串长度
    size_t len = sdslen(s);
    // 返回可用空间，实际上就是sds中的alloc - len的结果值
    size_t avail = sdsavail(s);
    sh = (char*)s-oldhdrlen;

    // 如果可用空间为0表示没有可释放的空闲空间
    if (avail == 0) return s;

    /* Check what would be the minimum SDS header that is just good enough to
     * fit this string. */
    type = sdsReqType(len);
    hdrlen = sdsHdrSize(type);

    /* If the type is the same, or at least a large enough type is still
     * required, we just realloc(), letting the allocator to do the copy
     * only if really needed. Otherwise if the change is huge, we manually
     * reallocate the string to use the different header type. */
    if (oldtype==type || type &gt; SDS_TYPE_8) {
        // 用的实际上还是原来的sds开始地址
        newsh = s_realloc(sh, oldhdrlen+len+1);
        if (newsh == NULL) return NULL;
        // 类型不变的情况下，s指针也不变
        s = (char*)newsh+oldhdrlen;
    } else {
        // 当类型发生变化时，地址内容不可复用，便需要寻找新的可用空间地址
        newsh = s_malloc(hdrlen+len+1);
        if (newsh == NULL) return NULL;
        // 复制原来的字符串到新的sds上
        memcpy((char*)newsh+hdrlen, s, len+1);
        // 释放之前的空间
        s_free(sh);
        // s指向sds的开始位置
        s = (char*)newsh+hdrlen;
        s[-1] = type;
        // 设置len值
        sdssetlen(s, len);
    }
    // 设置alloc值
    sdssetalloc(s, len);
    // 返回新的sds
    return s;
}

</code></pre>
<h3 id="链表">链表</h3>
<p>链表提供了高效的节点重排能力以及顺序性的节点访问方式，并且可以通过可以快速的增删节点来调整链表的长度。在 C 语言中并没有内置链表这种数据结构，因此redis构建了自己的链表实现。</p>
<p>Redis中的列表对象在版本3.2之前，其底层数据结构由ziplist和linkedlist实现，但是从3.2版本开始，重新引入了一个 quicklist 的数据结构，列表的底层都由quicklist实现。</p>
<p>在3.2版本之前，当列表对象中元素的长度比较小或者数量比较少的时候，采用ziplist来存储，当列表对象中元素的长度比较大或者数量比较多的时候，则会转而使用双向列表linkedlist来存储。</p>
<pre><code>// 文件地址：t_list.c---》版本3.2及之后
// 可以看出，如果底层数据结构不是quicklist 便会直接抛出异常：Unknown list encoding
void listTypePush(robj *subject, robj *value, int where) {
    if (subject-&gt;encoding == OBJ_ENCODING_QUICKLIST) {
        int pos = (where == LIST_HEAD) ? QUICKLIST_HEAD : QUICKLIST_TAIL;
        if (value-&gt;encoding == OBJ_ENCODING_INT) {
            char buf[32];
            ll2string(buf, 32, (long)value-&gt;ptr);
            quicklistPush(subject-&gt;ptr, buf, strlen(buf), pos);
        } else {
            quicklistPush(subject-&gt;ptr, value-&gt;ptr, sdslen(value-&gt;ptr), pos);
        }
    } else {
        serverPanic(&quot;Unknown list encoding&quot;);
    }
}

</code></pre>
<pre><code>// 文件地址：t_list.c---》版本3.2之前
// 可以看出，其底层数据结构主要由ziplist和linkedlist实现
void listTypePush(robj *subject, robj *value, int where) {
    /* Check if we need to convert the ziplist */
    listTypeTryConversion(subject,value);
    if (subject-&gt;encoding == REDIS_ENCODING_ZIPLIST &amp;&amp;
        ziplistLen(subject-&gt;ptr) &gt;= server.list_max_ziplist_entries)
            listTypeConvert(subject,REDIS_ENCODING_LINKEDLIST);

    if (subject-&gt;encoding == REDIS_ENCODING_ZIPLIST) {
        int pos = (where == REDIS_HEAD) ? ZIPLIST_HEAD : ZIPLIST_TAIL;
        value = getDecodedObject(value);
        subject-&gt;ptr = ziplistPush(subject-&gt;ptr,value-&gt;ptr,sdslen(value-&gt;ptr),pos);
        decrRefCount(value);
    } else if (subject-&gt;encoding == REDIS_ENCODING_LINKEDLIST) {
        if (where == REDIS_HEAD) {
            listAddNodeHead(subject-&gt;ptr,value);
        } else {
            listAddNodeTail(subject-&gt;ptr,value);
        }
        incrRefCount(value);
    } else {
        redisPanic(&quot;Unknown list encoding&quot;);
    }
}

</code></pre>
<h4 id="linkedlist">linkedList</h4>
<p>双向链表linkedList在3.2版本之前作为列表键的底层数据结构之外，还被用于发布订阅、慢查询、监视器等功能中。</p>
<p>双向链表linkedlist便于在表的两端进行push和pop操作，在插入节点上复杂度很低，但是它的内存开销比较大。首先，它在每个节点上除了要保存数据之外，还要额外保存两个指针；其次，双向链表的各个节点是单独的内存块，地址不连续，节点多了容易产生内存碎片。</p>
<pre><code>// 文件地址：adlist.h
typedef struct listNode {
    // 前驱节点
    struct listNode *prev;
    // 后继节点
    struct listNode *next;
    // 实际存储的值
    void *value;
} listNode;

typedef struct list {
    // 表头指针
    listNode *head;
    // 表尾指针
    listNode *tail;
    // 复制函数
    void *(*dup)(void *ptr);
    // 释放函数
    void (*free)(void *ptr);
    // 对比函数
    int (*match)(void *ptr, void *key);
    // 节点数量，即链表长度
    unsigned long len;
} list;

</code></pre>
<p>从上述源码可以看出，listNode 中使用了一个 void 类型的指针保存其内存，说明双向链表对节点所保存的值的类型不做任何限制。</p>
<p>对于不同类型的值，有时候便需要不同的函数来进行处理，由于链表内部并不知道 value 保存的内容的真实情况， 因此需要用户指定三个函数用于处理这个 value，这三个函数分别是：</p>
<ul>
<li>dup：复制一个 value，若没有提供此函数，链表内部在需要复制的时候将会直接复制 value 指针的值</li>
<li>free：释放一个 value，若没有提供此函数，则不进行 free 操作</li>
<li>match：比较两个 value 的值，返回非 0 值表示两个 value 相等，若不提供此函数则直接比较两个 value 指针是否相等</li>
</ul>
<p>list 结构里面维护了一个表示其节点个数的变量 len，因此 Redis 在查询 list 的长度时可以直接使用这个值，而不用遍历整个列表，这也使得查询长度操作的复杂度为O(1)。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320014.png" alt="img" loading="lazy"></figure>
<p>从上述结构图中可以看出，Redis链表结构的主要特性如下:</p>
<ul>
<li>双向：链表节点带有前驱、后继指针获取某个节点的前驱、后继节点的时间复杂度为0(1)。</li>
<li>无环: 链表为非循环链表表头节点的前驱指针和表尾节点的后继指针都指向NULL，对链表的访问以NULL为终点。</li>
<li>带表头指针和表尾指针：通过list结构中的head和tail指针，获取表头和表尾节点的时间复杂度都为O(1)。</li>
<li>带链表长度计数器：通过list结构的len属性获取节点数量的时间复杂度为O(1)。</li>
<li>多态：链表节点使用void*指针保存节点的值，并且可以通过list结构的dup、free、match三个属性为节点值设置类型特定函数，所以链表可以用来保存各种不同类型的值。</li>
</ul>
<h4 id="ziplist">zipList</h4>
<p>压缩列表ziplist 是一个经过特殊编码的双向链表，旨在提高内存效率。它允许在 O(1) 时间内对列表的任一侧进行push和pop操作。不过，由于每个操作都需要重新分配 ziplist 使用的内存，因此实际复杂度与 ziplist 使用的内存量有关。</p>
<p>ziplist存储在一段连续的内存上，所以存储效率很高。但是，它不利于修改操作，插入和删除操作需要频繁的申请和释放内存。特别是当ziplist长度很长的时候，一次realloc可能会导致大批量的数据拷贝。</p>
<p>zipList的构成一般如下表所示：</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>zlbytes</td>
<td>uint32_t</td>
<td>用于保存压缩列表的长度（单位为字节），包括 4 字节的 zlbytes 本身。通过值可以快速调整整个压缩列表的大小，而无需遍历整个压缩列表</td>
</tr>
<tr>
<td>ztail</td>
<td>uint32_t</td>
<td>用于记录列表中最后一个节点的偏移。通过这个值可以快速的从队列末尾弹出一个节点，而无需遍历整个列表</td>
</tr>
<tr>
<td>zllen</td>
<td>uint16_t</td>
<td>记录压缩列表中节点的数量。当节点数量超过或者等于 UINT16_MAX(65535) 时，需要遍历整个列表才能知道节点的数量（一旦列表到达 UINT16_MAX，即使后面将部分节点从列表中删除，使得列表节点数小于 UINT16_MAX 这个字段依然会保持 UINT16_MAX 不变）</td>
</tr>
<tr>
<td>zlend</td>
<td>uint8_t</td>
<td>是一个特殊的节点，用于表示压缩列表的末尾。它的值为 255。其他普通的节点不会以 255 开头。因此可以通过检查节点的地一个字节是否等于 255 从而知道是否已经到达列表的末尾</td>
</tr>
</tbody>
</table>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320015.png" alt="" loading="lazy"></figure>
<pre><code>// 文件地址：ziplist.c
// 创建ziplist时，便已经指定了各个字段的长度。
#define ZIPLIST_HEADER_SIZE     (sizeof(uint32_t)*2+sizeof(uint16_t))
#define ZIPLIST_ENTRY_END(zl)   ((zl)+intrev32ifbe(ZIPLIST_BYTES(zl))-1)
unsigned char *ziplistNew(void) {
    unsigned int bytes = ZIPLIST_HEADER_SIZE+ZIPLIST_END_SIZE;
    unsigned char *zl = zmalloc(bytes);
    ZIPLIST_BYTES(zl) = intrev32ifbe(bytes);
    ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(ZIPLIST_HEADER_SIZE);
    ZIPLIST_LENGTH(zl) = 0;
    zl[bytes-1] = ZIP_END;
    return zl;
}

</code></pre>
<p>压缩列表中的压缩节点entry可以保存一个字节数组或者一个整数值。每个压缩列表节点都已两个字段开头：</p>
<ul>
<li>第一个字段是上一个节点的长度（prevlen），通过这个字段可以从后往前遍历列表。</li>
<li>第二个字段是该节点的编码方式（encoding）。指示了本节点的节点类型是字节数组类型（String）还是整数类型（Integer）。如果是字节数组类型（String）还包含了字节数组的长度。</li>
</ul>
<pre><code>// 通常，压缩节点的结构如下所示。
&lt;prevlen&gt; &lt;encoding&gt; &lt;entry-data&gt;
// 对于一些很小的数字，会在 encoding 字段包含了数字本身。在这种情况下不需要 &lt;entry-data&gt; 
&lt;prevlen&gt; &lt;encoding&gt;

</code></pre>
<pre><code>// 因为压缩列表本质只是字节数组，为了方便操作，redis还定义了zlentry结构体
// 操作时，将每个节点包含信息按照一定规则写入zlentry中。
typedef struct zlentry {
    unsigned int prevrawlensize; /* Bytes used to encode the previous entry len*/
    unsigned int prevrawlen;     /* Previous entry len. */
    unsigned int lensize;        /* Bytes used to encode this entry type/len.
                                    For example strings have a 1, 2 or 5 bytes
                                    header. Integers always use a single byte.*/
    unsigned int len;            /* Bytes used to represent the actual entry.
                                    For strings this is just the string length
                                    while for integers it is 1, 2, 3, 4, 8 or
                                    0 (for 4 bit immediate) depending on the
                                    number range. */
    unsigned int headersize;     /* prevrawlensize + lensize. */
    unsigned char encoding;      /* Set to ZIP_STR_* or ZIP_INT_* depending on
                                    the entry encoding. However for 4 bits
                                    immediate integers this can assume a range
                                    of values and must be range-checked. */
    unsigned char *p;            /* Pointer to the very start of the entry, that
                                    is, this points to prev-entry-len field. */
} zlentry;  

</code></pre>
<p>对于压缩节点中的prevlen字段有着两种编码方式：</p>
<ul>
<li>前节点长度小于254时，占用1字节，用来表示前节点长度</li>
<li>前节点长度大于等于254时，占用5字节，其中第1个字节为特殊值0xFE(254)，后面4字节用来表示实际长度</li>
</ul>
<pre><code>// 前节点长度小于254时
&lt;prevlen from 0 to 253&gt; &lt;encoding&gt; &lt;entry&gt;
// 前节点长度大于等于254时
0xFE &lt;4 bytes unsigned little endian prevlen&gt; &lt;encoding&gt; &lt;entry&gt;

</code></pre>
<pre><code>// 文件地址：ziplist.c
// 返回前一个节点的编码长度
#define ZIP_DECODE_PREVLENSIZE(ptr, prevlensize) do {
    if ((ptr)[0] &lt; ZIP_BIG_PREVLEN) {
        (prevlensize) = 1;                                                 
    } else {                                                               
        (prevlensize) = 5;                                                 
    }                                                                      
} while(0)

#define ZIP_DECODE_PREVLEN(ptr, prevlensize, prevlen) do {                 
    ZIP_DECODE_PREVLENSIZE(ptr, prevlensize);                              
    if ((prevlensize) == 1) {                                              
        (prevlen) = (ptr)[0];                                              
    } else { /* prevlensize == 5 */                                        
        (prevlen) = ((ptr)[4] &lt;&lt; 24) |                                     
                    ((ptr)[3] &lt;&lt; 16) |                                     
                    ((ptr)[2] &lt;&lt;  8) |                                     
                    ((ptr)[1]);                                            
    }                                                                      
} while(0)

</code></pre>
<p>对于压缩节点encoding字段而言，针对不同数据类型会有不同的表现形式，即entry的编码字段取决于entry的内容。</p>
<ul>
<li>当entry是字符串时，编码第一个字节的前 2 位将保存用于存储字符串长度的编码类型，然后是字符串的实际长度。</li>
<li>当条目是整数时，前 2 位都设置为 1。接下来的 2 位用于指定在此标头之后将存储哪种整数。</li>
<li>不同类型和编码的概述如下。第一个字节总是足以确定条目的类型。</li>
</ul>
<table>
<thead>
<tr>
<th>编码</th>
<th>大小</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>00pppppp</td>
<td>1 字节</td>
<td>保存小于等于 63 个字符的字节数组（6 比特）。&quot;pppppp&quot; 表示 6 比特大小的无符号整数长度</td>
</tr>
<tr>
<td>01pppppp</td>
<td>qqqqqqqq</td>
<td>2 字节</td>
</tr>
<tr>
<td>10000000</td>
<td>qqqqqqqq</td>
<td>rrrrrrrr</td>
</tr>
<tr>
<td>11000000</td>
<td>1 字节</td>
<td>保存 2 字节有符号整数（int16_t）的编码</td>
</tr>
<tr>
<td>11010000</td>
<td>1 字节</td>
<td>保存 4 字节有符号整数（int32_t）的编码</td>
</tr>
<tr>
<td>11100000</td>
<td>1 字节</td>
<td>保存 8 字节有符号整数（int64_t）的编码</td>
</tr>
<tr>
<td>11110000</td>
<td>1 字节</td>
<td>保存 3 字节有符号整数（24 位）的编码</td>
</tr>
<tr>
<td>11111110</td>
<td>1 字节</td>
<td>保存 1 字节有符号整数（int8_t）的编码</td>
</tr>
<tr>
<td>1111xxxx</td>
<td>1 字节</td>
<td>（xxxx 取值为 0001 到 1101）保存 0 到 12 的整数编码，xxxx 为 0001 时表示保存的是 0,依此类推</td>
</tr>
<tr>
<td>11111111</td>
<td>1 字节</td>
<td>表示压缩列表的末尾节点</td>
</tr>
</tbody>
</table>
<p>注意：每个压缩节点会存在prevlen属性，用来记录前置节点的长度，根据前置节点长度还分为两种情况，当长度大于等于254时，占用空间会从1字节扩大到5字节。所谓连锁更新，就是多个长度处于250字节~253字节之间的连续节点，当一个节点更新后，导致下一个节点prevlen由1字节变为5字节，从而导致下下一个节点prevlen值增大，产生连锁反应。因为连锁更新在最坏情况下需要对压缩列表执行N次空间预分配，而每次空间预分配的最坏复杂度为O(n)，所以连锁更新的最坏复杂度为O(n<sup>2)。不过，虽然连锁更新的复杂度高，但出现的几率较低，需要多个特殊长度的节点，所以插入删除命令的平均复杂度均为O(n)，最坏复杂度为O(n</sup>2)。对应的连锁更新函数为：__ziplistCascadeUpdate</p>
<h4 id="quicklist">quickList</h4>
<p>前面两种结构各有优缺点，ziplist适合数据量少而linkedlist适合大数据量，那么有没有一种能够兼容这两种数据结构的新型数据结构呢？答案是有的，从redis3.2版本开始，引入了一个quicklist的数据结构，其兼顾了前面两种数据结构的优点，现已成为list对象的底层实现。</p>
<p>实际上quicklist是ziplist和linkedlist的混合体，它将linkedlist按段切分，每一段都会使用ziplist来紧凑存储，多个ziplist之间使用双向指针进行串联。</p>
<pre><code>// 文件地址：quicklist.h
typedef struct quicklistNode {
    struct quicklistNode *prev; /* 指向上一个节点 */
    struct quicklistNode *next; /* 指向下一个节点 */
    unsigned char *entry;  /* 当节点保存的是压缩 ziplist 时，指向 quicklistLZF，否则指向 ziplist */
    size_t sz;            /* ziplist 的大小（字节为单位） */
    unsigned int count : 16;     /* ziplist 的项目个数 */
    unsigned int encoding : 2;   /* RAW==1 or LZF==2 */
    unsigned int container : 2;  /* PLAIN==1 or PACKED==2 */
    unsigned int recompress : 1; /* 这个节点是否之前是被压缩的 */
    unsigned int attempted_compress : 1;  /* 节点太小，不压缩 */
    unsigned int extra : 10; /* 未使用，保留字段 */
} quicklistNode;

// 当 quicklistNode 指向的 ziplist 被压缩的情况下，上述的 entry成员会指向如下的 quicklistLZF 结构
typedef struct quicklistLZF {
    unsigned int sz; /* LZF 压缩后的ziplist大小*/
    char compressed[]; /* 是个柔性数组（flexible array member），存放压缩后的ziplist字节数组。*/
} quicklistLZF;

typedef struct quicklist {
    quicklistNode *head;  /* 头指针 */
    quicklistNode *tail;  /* 尾指针 */
    unsigned long count;        /* 所有ziplist数据项的个数总和。 */
    unsigned long len;          /* quicklistNodes总数 */
    signed int fill : QL_FILL_BITS;       /* 16bit，ziplist大小设置，存放list-max-ziplist-size参数的值。 */
    unsigned int compress : QL_COMP_BITS; /*  16bit，节点压缩深度设置，存放list-compress-depth参数的值;0=off */
    unsigned int bookmark_count: QL_BM_BITS;
    quicklistBookmark bookmarks[];
} quicklist;

</code></pre>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320016.png" alt="img" loading="lazy"></figure>
<p>注意：</p>
<ul>
<li>quicklist 不会对过小的 ziplist 进行压缩，这个值为 MIN_COMPRESS_BYTES，目前是 48</li>
<li>quicklist 在对 ziplist 进行压缩后，会对比压缩后的长度和未压缩的长度，若压缩后的长度 - 未压缩的长度 &lt; 8（MIN_COMPRESS_IMPROVE）（lzf-&gt;sz + MIN_COMPRESS_IMPROVE &gt;= node-&gt;sz），则使用未压缩的数据</li>
<li>quicklist 的头节点和尾节点在任何时候都不会被压缩，因此可以保证将数据插入列表的头或者尾是高效的。</li>
<li>插入一个数据到压缩的节点时，需要先对节点的 ziplist 整个进行解压，插入后再次进行压缩。</li>
<li>对中间某个已满的节点进行插入操作会导致此节点（假设此节点为 node）的 ziplist 在插入点进行分裂，此时这个 ziplist 会变成两个 ziplist（新建一个 quicklistNode 保存多出来的 ziplist，假设此节点为 newnode），然后将数据插入 node 指向的 ziplist 的末尾，然后对 node 进行 _quicklistMergeNodes 操作。</li>
</ul>
<pre><code>// 文件地址：quicklist.c
// 压缩时，不过对小于48字节的数据进行压缩，同时会开辟新的空间压缩ziplist数据，并释放原来的node空间，最后指向压缩后的数据并修改其他属性值
// 解压操作与压缩操作基本一致
/* Minimum listpack size in bytes for attempting compression. */
#define MIN_COMPRESS_BYTES 48
/* Compress the listpack in 'node' and update encoding details.
 * Returns 1 if listpack compressed successfully.
 * Returns 0 if compression failed or if listpack too small to compress. */
REDIS_STATIC int __quicklistCompressNode(quicklistNode *node) {
#ifdef REDIS_TEST
    node-&gt;attempted_compress = 1;
#endif

    /* validate that the node is neither
     * tail nor head (it has prev and next)*/
    assert(node-&gt;prev &amp;&amp; node-&gt;next);

    node-&gt;recompress = 0;
    /* 小于MIN_COMPRESS_BYTES=48的数据不会被压缩 */
    if (node-&gt;sz &lt; MIN_COMPRESS_BYTES)
        return 0;

    quicklistLZF *lzf = zmalloc(sizeof(*lzf) + node-&gt;sz);

    /* Cancel if compression fails or doesn't compress small enough */
    if (((lzf-&gt;sz = lzf_compress(node-&gt;entry, node-&gt;sz, lzf-&gt;compressed,
                                 node-&gt;sz)) == 0) ||
        lzf-&gt;sz + MIN_COMPRESS_IMPROVE &gt;= node-&gt;sz) {
        /* lzf_compress aborts/rejects compression if value not compressible. */
        zfree(lzf);
        return 0;
    }
    lzf = zrealloc(lzf, sizeof(*lzf) + lzf-&gt;sz);
    zfree(node-&gt;entry);
    node-&gt;entry = (unsigned char *)lzf;
    node-&gt;encoding = QUICKLIST_NODE_ENCODING_LZF;
    return 1;
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RocketMQ源码解读之主从机制]]></title>
        <id>https://philosopherzb.github.io/post/rocketmq-yuan-ma-jie-du-zhi-zhu-cong-ji-zhi/</id>
        <link href="https://philosopherzb.github.io/post/rocketmq-yuan-ma-jie-du-zhi-zhu-cong-ji-zhi/">
        </link>
        <updated>2022-11-19T05:55:18.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Rocketmq源码之主从机制。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/reservoir-50681_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="主从复制">主从复制</h2>
<h3 id="简介">简介</h3>
<p>为了提供消息消费的高可用性，避免broker单点故障导致其上的消息无法及时消费，rocketmq引入了broker主备机制，即消息到达主服务器后需要同步至从服务器；这样，如果主服务器broker宕机了，消费者依然可以从slave服务器拉取消息。</p>
<p>rocketmq高可用相关类如下所示：</p>
<ul>
<li>HAService: RocketMQ 主从同步核心实现类</li>
<li>HAService$AcceptSocketService : HA Master 端监听客户端连接实现类</li>
<li>HAService$GroupTransferService ：主从同步通知实现类。</li>
<li>HAService$HAClient: HA Client 端实现类。</li>
<li>HA Connection: HA Master 服务端 HA 连接对象的封装，与 Broker 从服务器的网络读写类</li>
<li>HAConnection$ReadSocketService: HA Master 网络读实现类</li>
<li>HAConnection$WriteSocketServicce: HA Master 网络写实现类</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320012.png" alt="img" loading="lazy"></figure>
<h3 id="haservice工作机制">HAService工作机制</h3>
<p>高可用的核心处理类HAService，本节主要介绍其启动加载内容。rocketmq HA 的大致实现原理如下：</p>
<ol>
<li>主服务器启动，并在特定端口监听从服务器的连接。</li>
<li>从服务器主动连接主服务器，主服务器接受客户端的连接，并建立相应的tcp连接。</li>
<li>从服务器主动向主服务器发送待拉取消息的偏移量，主服务器解析请求并返回消息给从服务器。</li>
<li>从服务器保存消息，并不断发送新的消息同步请求。</li>
</ol>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320013.png" alt="" loading="lazy"></figure>
<pre><code>// 文件地址：org.apache.rocketmq.store.ha.HAService
// ha 服务启动
public void start() throws Exception {
    this.acceptSocketService.beginAccept();
    this.acceptSocketService.start();
    this.groupTransferService.start();
    this.haClient.start();
}

</code></pre>
<h3 id="acceptsocketservice">AcceptSocketService</h3>
<p>AcceptSocketService 是 HAService的内部类，主要负责实现master端监听salve连接。</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.ha.HAService#AcceptSocketService 
public void beginAccept() throws Exception {
    // 创建服务端socket通道，基于NIO
    this.serverSocketChannel = ServerSocketChannel.open();
    // 创建selector--》事件选择器，基于NIO
    this.selector = RemotingUtil.openSelector();
    this.serverSocketChannel.socket().setReuseAddress(true);
    // 绑定监听端口
    this.serverSocketChannel.socket().bind(this.socketAddressListen);
    // 设置非阻塞
    this.serverSocketChannel.configureBlocking(false);
    // 以OP_ACCEPT（网络连接事件）进行注册
    this.serverSocketChannel.register(this.selector, SelectionKey.OP_ACCEPT);
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.ha.HAService#AcceptSocketService 
public void run() {
    log.info(this.getServiceName() + &quot; service started&quot;);

    while (!this.isStopped()) {
        try {
            // 每隔1s处理一次连接就绪事件
            this.selector.select(1000);
            Set&lt;SelectionKey&gt; selected = this.selector.selectedKeys();

            if (selected != null) {
                for (SelectionKey k : selected) {
                    if ((k.readyOps() &amp; SelectionKey.OP_ACCEPT) != 0) {
                        // 创建一个SocketChannel
                        SocketChannel sc = ((ServerSocketChannel) k.channel()).accept();

                        if (sc != null) {
                            HAService.log.info(&quot;HAService receive new connection, &quot;
                                + sc.socket().getRemoteSocketAddress());

                            try {
                                // 同时为每个连接创建一个HAConnection ，主要负责M-S数据同步逻辑
                                HAConnection conn = new HAConnection(HAService.this, sc);
                                conn.start();
                                HAService.this.addConnection(conn);
                            } catch (Exception e) {
                                log.error(&quot;new HAConnection exception&quot;, e);
                                sc.close();
                            }
                        }
                    } else {
                        log.warn(&quot;Unexpected ops in select &quot; + k.readyOps());
                    }
                }

                selected.clear();
            }
        } catch (Exception e) {
            log.error(this.getServiceName() + &quot; service has exception.&quot;, e);
        }
    }

    log.info(this.getServiceName() + &quot; service end&quot;);
}

</code></pre>
<h3 id="grouptransferservice">GroupTransferService</h3>
<p>GroupTransferService是 HAService的内部类，主要负责主从同步阻塞。在主从同步中，消息发送者将消息刷盘后，需要继续等待新消息被传输到从服务器，即需要等待数据主从同步的结果。而在这个同步过程中便需要用到GroupTransferService进行阻塞处理。</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.ha.HAService#GroupTransferService
private void doWaitTransfer() {
    if (!this.requestsRead.isEmpty()) {
        for (CommitLog.GroupCommitRequest req : this.requestsRead) {
            boolean transferOK = HAService.this.push2SlaveMaxOffset.get() &gt;= req.getNextOffset();
            long deadLine = req.getDeadLine();
            while (!transferOK &amp;&amp; deadLine - System.nanoTime() &gt; 0) {
                this.notifyTransferObject.waitForRunning(1000);
                 // 同步完成的判断依据：salve中已复制的最大偏移量是否大于等于生产者发送消息后服务端返回的下一条消息的起始偏移量 
                transferOK = HAService.this.push2SlaveMaxOffset.get() &gt;= req.getNextOffset();
            }
            // 根据上述同步判断来决定是否唤醒发送者线程
            req.wakeupCustomer(transferOK ? PutMessageStatus.PUT_OK : PutMessageStatus.FLUSH_SLAVE_TIMEOUT);
        }

        this.requestsRead = new LinkedList&lt;&gt;();
    }
}

</code></pre>
<h3 id="haclient">HAClient</h3>
<p>HAClient是主从同步slave端的核心实现类。以下从其run函数开始分析其工作原理。</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.ha.HAService#HAClient
// 首先，salve服务器需要与master服务器进行连接
private boolean connectMaster() throws ClosedChannelException {
    // 当socketChannel为空时尝试连接master
    if (null == socketChannel) {
        // 获取master服务器地址
        String addr = this.masterAddress.get();
        if (addr != null) {
             // 将master地址转为socket地址  
            SocketAddress socketAddress = RemotingUtil.string2SocketAddress(addr);
            if (socketAddress != null) {
                // 创建socket通道
                this.socketChannel = RemotingUtil.connect(socketAddress);
                if (this.socketChannel != null) {
                    // 以OP_READ（网络读事件）注册
                    this.socketChannel.register(this.selector, SelectionKey.OP_READ);
                }
            }
        }
        // 初始化currentReportedOffset为commitlog文件的最大偏移量
        this.currentReportedOffset = HAService.this.defaultMessageStore.getMaxPhyOffset();
        // 上次写入时间戳的值设置为当前时间戳
        this.lastWriteTimestamp = System.currentTimeMillis();
    }

    return this.socketChannel != null;
}

// 是否将当前待拉取偏移量反馈给master，默认心跳间隔5s，可通过haSendHeartbeatInterval进行配置
private boolean isTimeToReportOffset() {
    long interval =
        HAService.this.defaultMessageStore.getSystemClock().now() - this.lastWriteTimestamp;
    boolean needHeart = interval &gt; HAService.this.defaultMessageStore.getMessageStoreConfig()
        .getHaSendHeartbeatInterval();

    return needHeart;
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.ha.HAService#HAClient
// 向master反馈待拉取偏移量
// 此函数包含两种含义：对于salve而言是发送下次待拉取偏移量；
// 对master而言既可以认为是salve本次请求的待拉取偏移量，也可以认为是salve消息同步的ACK确认消息
private boolean reportSlaveMaxOffset(final long maxOffset) {
    // 读写模式切换，或许可以使用ByteBuffer的flip函数来实现
    this.reportOffset.position(0);
    this.reportOffset.limit(8);
    this.reportOffset.putLong(maxOffset);
    this.reportOffset.position(0);
    this.reportOffset.limit(8);

    // 此处循环写入的原因是由于NIO是一个非阻塞IO，一次写入不一定会将ByteBuffer可读字节全部写入 
    for (int i = 0; i &lt; 3 &amp;&amp; this.reportOffset.hasRemaining(); i++) {
        try {
            this.socketChannel.write(this.reportOffset);
        } catch (IOException e) {
            log.error(this.getServiceName()
                + &quot;reportSlaveMaxOffset this.socketChannel.write exception&quot;, e);
            return false;
        }
    }

    lastWriteTimestamp = HAService.this.defaultMessageStore.getSystemClock().now();
    return !this.reportOffset.hasRemaining();
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.ha.HAService#HAClient
// 处理网络读请求，即master返回的消息
private boolean processReadEvent() {
    int readSizeZeroTimes = 0;
    // 循环判断readByteBuffer是否还有剩余空间
    while (this.byteBufferRead.hasRemaining()) {
        try {
            // 获取读字节数
            int readSize = this.socketChannel.read(this.byteBufferRead);
            if (readSize &gt; 0) {
                // 重置读取到0字节的次数
                readSizeZeroTimes = 0;
                // 将读取到的所有消息追加到内存映射文件中
                boolean result = this.dispatchReadRequest();
                if (!result) {
                    log.error(&quot;HAClient, dispatchReadRequest error&quot;);
                    return false;
                }
            } else if (readSize == 0) {
                if (++readSizeZeroTimes &gt;= 3) {
                    break;
                }
            } else {
                log.info(&quot;HAClient, processReadEvent read socket &lt; 0&quot;);
                return false;
            }
        } catch (IOException e) {
            log.info(&quot;HAClient, processReadEvent read socket exception&quot;, e);
            return false;
        }
    }

    return true;
}

</code></pre>
<h3 id="haconnection">HAConnection</h3>
<p>Master服务器接收到Salve服务器的请求后，会将主从服务器的连接封装成 HAConnection 对象，以此来实现主从服务器之间的读写操作。下面主要分析其读写操作源码。</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.ha.HAConnection#ReadSocketService
// 读操作核心处理函数processReadEvent
private boolean processReadEvent() {
    int readSizeZeroTimes = 0;
    // 如果readByteBuffer没有剩余空间，说明position=limit=capacity
    if (!this.byteBufferRead.hasRemaining()) {
        // flip函数将会让position=0，limit=capacity
        this.byteBufferRead.flip();
        // 设置processPosition 为0，表示从头开始处理
        this.processPosition = 0;
    }
    // 循环读取 
    while (this.byteBufferRead.hasRemaining()) {
        try {
            int readSize = this.socketChannel.read(this.byteBufferRead);
            if (readSize &gt; 0) {
                readSizeZeroTimes = 0;
                this.lastReadTimestamp = HAConnection.this.haService.getDefaultMessageStore().getSystemClock().now();
                // 存在读数据且本次读取内容大于8，说明收到了从服务器的一条拉取消息请求
                if ((this.byteBufferRead.position() - this.processPosition) &gt;= 8) {
                    int pos = this.byteBufferRead.position() - (this.byteBufferRead.position() % 8);
                    long readOffset = this.byteBufferRead.getLong(pos - 8);
                    this.processPosition = pos;

                    HAConnection.this.slaveAckOffset = readOffset;
                    if (HAConnection.this.slaveRequestOffset &lt; 0) {
                        HAConnection.this.slaveRequestOffset = readOffset;
                        log.info(&quot;slave[&quot; + HAConnection.this.clientAddr + &quot;] request offset &quot; + readOffset);
                    } else if (HAConnection.this.slaveAckOffset &gt; HAConnection.this.haService.getDefaultMessageStore().getMaxPhyOffset()) {
                        log.warn(&quot;slave[{}] request offset={} greater than local commitLog offset={}. &quot;,
                                HAConnection.this.clientAddr,
                                HAConnection.this.slaveAckOffset,
                                HAConnection.this.haService.getDefaultMessageStore().getMaxPhyOffset());
                        return false;
                    }
                    // 通知由于同步等待HA复制结果而阻塞的消息发送线程
                    HAConnection.this.haService.notifyTransferSome(HAConnection.this.slaveAckOffset);
                }
            } else if (readSize == 0) {
                // 读取字节数等于0，重复执行三次
                if (++readSizeZeroTimes &gt;= 3) {
                    break;
                }
            } else {
                // 结束本次循环，服务器将关闭该连接
                log.error(&quot;read socket[&quot; + HAConnection.this.clientAddr + &quot;] &lt; 0&quot;);
                return false;
            }
        } catch (IOException e) {
            log.error(&quot;processReadEvent exception&quot;, e);
            return false;
        }
    }

    return true;
}
</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.ha.HAConnection#WriteSocketService
// 写操作核心处理函数run
public void run() {
    HAConnection.log.info(this.getServiceName() + &quot; service started&quot;);

    while (!this.isStopped()) {
        try {
            // 每1秒都会执行一次写操作
            this.selector.select(1000);
            // slaveRequestOffset=-1，表示master还未收到salve的请求，将跳过本次循环
            if (-1 == HAConnection.this.slaveRequestOffset) {
                Thread.sleep(10);
                continue;
            }
            // nextTransferFromWhere=-1，表示初次进行数据传输
            if (-1 == this.nextTransferFromWhere) {
                // 当slaveRequestOffset=0时，会从当前commitlog文件获取最大偏移量
                // nextTransferFromWhere 表示下次数据传输点
                if (0 == HAConnection.this.slaveRequestOffset) {
                    long masterOffset = HAConnection.this.haService.getDefaultMessageStore().getCommitLog().getMaxOffset();
                    masterOffset =
                        masterOffset
                            - (masterOffset % HAConnection.this.haService.getDefaultMessageStore().getMessageStoreConfig()
                            .getMappedFileSizeCommitLog());

                    if (masterOffset &lt; 0) {
                        masterOffset = 0;
                    }
                    // 赋值给nextTransferFromWhere 
                    this.nextTransferFromWhere = masterOffset;
                } else {
                    // 否则将从服务器拉取偏移量赋值给nextTransferFromWhere 
                    this.nextTransferFromWhere = HAConnection.this.slaveRequestOffset;
                }

                log.info(&quot;master transfer data from &quot; + this.nextTransferFromWhere + &quot; to slave[&quot; + HAConnection.this.clientAddr
                    + &quot;], and slave request &quot; + HAConnection.this.slaveRequestOffset);
            }
            // 判断上次写事件是否将消息全部写入客户端
            if (this.lastWriteOver) {
                // 获取当前系统时间与上次最后写入时间的差值
                long interval =
                    HAConnection.this.haService.getDefaultMessageStore().getSystemClock().now() - this.lastWriteTimestamp;
                // 如果该差值大于HA心跳间隔，HA心跳间隔由haSendHeartbeatInterval控制，默认5s
                if (interval &gt; HAConnection.this.haService.getDefaultMessageStore().getMessageStoreConfig()
                    .getHaSendHeartbeatInterval()) {

                    // Build Header
                    this.byteBufferHeader.position(0);
                    this.byteBufferHeader.limit(headerSize);
                    this.byteBufferHeader.putLong(this.nextTransferFromWhere);
                    this.byteBufferHeader.putInt(0);
                    this.byteBufferHeader.flip();
                    // 此处将发送一个心跳包，主要是为了避免长链接由于空闲而被关闭
                    this.lastWriteOver = this.transferData();
                    if (!this.lastWriteOver)
                        continue;
                }
            } else {
                // 未写入，则继续传输上一次数据
                this.lastWriteOver = this.transferData();
                // 如果消息仍未传输完成，则终止此次循环
                if (!this.lastWriteOver)
                    continue;
            }
            // 传输消息到从服务器
            // 根据从服务器请求的待拉取偏移量，查找该偏移量之后的所有可读数据
            SelectMappedBufferResult selectResult =
                HAConnection.this.haService.getDefaultMessageStore().getCommitLogData(this.nextTransferFromWhere);
            // 存在可读数据
            if (selectResult != null) {
                // 判断消息总长度是否大于HA配置的一次同步任务最大传输字节数，如果是的话，消息将会被截取，进行部分发送
                int size = selectResult.getSize();
                // HA一次同步任务最大字节数由haTransferBatchSize控制，默认32k
                if (size &gt; HAConnection.this.haService.getDefaultMessageStore().getMessageStoreConfig().getHaTransferBatchSize()) {
                    size = HAConnection.this.haService.getDefaultMessageStore().getMessageStoreConfig().getHaTransferBatchSize();
                }

                long thisOffset = this.nextTransferFromWhere;
                this.nextTransferFromWhere += size;
                // 限制传输大小
                selectResult.getByteBuffer().limit(size);
                this.selectMappedBufferResult = selectResult;

                // Build Header
                this.byteBufferHeader.position(0);
                this.byteBufferHeader.limit(headerSize);
                this.byteBufferHeader.putLong(thisOffset);
                this.byteBufferHeader.putInt(size);
                this.byteBufferHeader.flip();
                // 传输数据 
                this.lastWriteOver = this.transferData();
            } else {
                // 未匹配到数据时，通知所有线程等待100ms
                HAConnection.this.haService.getWaitNotifyObject().allWaitForRunning(100);
            }
        } catch (Exception e) {

            HAConnection.log.error(this.getServiceName() + &quot; service has exception.&quot;, e);
            break;
        }
    }
     // 将等待线程移除 
    HAConnection.this.haService.getWaitNotifyObject().removeFromWaitingThreadTable();
    if (this.selectMappedBufferResult != null) {
        this.selectMappedBufferResult.release();
    }
    this.makeStop();
    readSocketService.makeStop();
    haService.removeConnection(HAConnection.this);
    SelectionKey sk = this.socketChannel.keyFor(this.selector);
    if (sk != null) {
        sk.cancel();
    }
    try {
        // 关闭socket
        this.selector.close();
        this.socketChannel.close();
    } catch (IOException e) {
        HAConnection.log.error(&quot;&quot;, e);
    }

    HAConnection.log.info(this.getServiceName() + &quot; service end&quot;);
}

</code></pre>
<h2 id="读写分离">读写分离</h2>
<h3 id="简介-2">简介</h3>
<p>本节主要介绍消费者拉取消息时，从服务器如何参与负载，即主写从读机制。</p>
<p>RocketMQ根据MessageQueue查找broker地址的唯一依据时brokerName，从RocketMQ的的broker组织结构中得知同一组broker(M-S)服务器，它们的brokerName相同，但是brokerId不同，主服务器的brokerId为0，从服务器的则大于0。</p>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.PullAPIWrapper#pullKernelImpl
// 提供基于brokeName获取broker信息的函数
FindBrokerResult findBrokerResult =
    this.mQClientFactory.findBrokerAddressInSubscribe(mq.getBrokerName(),
        this.recalculatePullFromWhichNode(mq), false);

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.factory.MQClientInstance
public FindBrokerResult findBrokerAddressInSubscribe(
    final String brokerName,
    final long brokerId,
    // 是否仅返回broker信息
    final boolean onlyThisBroker
) {
    String brokerAddr = null;
    boolean slave = false;
    boolean found = false;
    // 从缓存表中获取broker信息
    HashMap&lt;Long/* brokerId */, String/* address */&gt; map = this.brokerAddrTable.get(brokerName);
    if (map != null &amp;&amp; !map.isEmpty()) {
        // 通过brokerId获取broker地址信息
        brokerAddr = map.get(brokerId);
        slave = brokerId != MixAll.MASTER_ID;
        found = brokerAddr != null;
        // 未找到且salve为true，则获取 brokerId + 1 的 brokerAddr 
        if (!found &amp;&amp; slave) {
            brokerAddr = map.get(brokerId + 1);
            found = brokerAddr != null;
        }
        // 未找到且onlyThisBroker设置为false，则从列表中随机选一个brokerAddr
        if (!found &amp;&amp; !onlyThisBroker) {
            Entry&lt;Long, String&gt; entry = map.entrySet().iterator().next();
            brokerAddr = entry.getValue();
            slave = entry.getKey() != MixAll.MASTER_ID;
            found = true;
        }
    }

    if (found) {
        // 组装返回FindBrokerResult，其中的salve表示是否从从服务求获取broker结果信息
        return new FindBrokerResult(brokerAddr, slave, findBrokerVersion(brokerName, brokerAddr));
    }

    return null;
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore#getMessage
// maxOffsetPy：当前主服务器存储文件的最大偏移量
// maxPhyOffsetPulling：此次拉取消息的最大偏移量
// diff：对于拉取线程而言，则是当前未被拉取到消费者端的消息长度
long diff = maxOffsetPy - maxPhyOffsetPulling;
// TOTAL_PHYSICAL_MEMORY_SIZE：rocketmq所在服务器的总内存大小
// AccessMessageInMemoryMaxRatio：rocketmq所能使用的最大内存比例，超过该比例，消息会被置换出内存
// memory：消息常驻内存的大小，超过该大小，rocketmq会将旧的消息置换回磁盘
// 如果diff &gt; memory，表示当前需要拉取的消息已经超过了常驻内存的大小，此时一般是主服务器繁忙，会建议从从服务器拉取消息
long memory = (long) (StoreUtil.TOTAL_PHYSICAL_MEMORY_SIZE
    * (this.messageStoreConfig.getAccessMessageInMemoryMaxRatio() / 100.0));
getResult.setSuggestPullingFromSlave(diff &gt; memory);

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.broker.processor.PullMessageProcessor#processRequest
// 如果主服务器繁忙，则下次将从从服务器拉取消息。
if (getMessageResult.isSuggestPullingFromSlave()) {
    // 设值SuggestWhichBrokerId的值为配置文件中的whichBrokerWhenConsumeSlowly属性，默认为1
    // 一个master即使拥有多个salve，消息负载时也只会选择其中一个salve
    responseHeader.setSuggestWhichBrokerId(subscriptionGroupConfig.getWhichBrokerWhenConsumeSlowly());
} else {
    responseHeader.setSuggestWhichBrokerId(MixAll.MASTER_ID);
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RocketMQ源码解读之消息消费(下)]]></title>
        <id>https://philosopherzb.github.io/post/rocketmq-yuan-ma-jie-du-zhi-xiao-xi-xiao-fei-xia/</id>
        <link href="https://philosopherzb.github.io/post/rocketmq-yuan-ma-jie-du-zhi-xiao-xi-xiao-fei-xia/">
        </link>
        <updated>2022-11-05T05:45:19.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Rocketmq源码之消息消费下篇。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/mountain-4810958_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="消息消费">消息消费</h2>
<h3 id="消费过程">消费过程</h3>
<p>在上篇文章中，PullMessageService负责消息拉取，并将其存入ProcessQueue队列中，随后便通过ConsumeMessageService#submitConsumeRequest将其提交至消费者线程池进行具体的消费处理。在整个消费过程中，消息拉取与消息消费实现了解耦。</p>
<p>ConsumeMessageService提供了两种实现，并发消费及顺序消费。两种实现大致上的流程一致，不过也存在一些细微的差别，主要在于顺序消息在创建消息队列拉取任务时需要在Broker端锁定消息队列。即在使用顺序消息时需指定MessageQueueSelector。</p>
<p>本节主要以并发消费的源码来简单介绍其中的实现。</p>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.ConsumeMessageService
// 核心函数
void submitConsumeRequest(
    // 消息列表，默认一次从服务器最多拉取32条
    final List&lt;MessageExt&gt; msgs,
    // 消息处理队列
    final ProcessQueue processQueue,
    // 消息所属的消息队列
    final MessageQueue messageQueue,
    // 是否转发至消费线程池，并发消费时此参数可忽略
    final boolean dispathToConsume);

</code></pre>
<h4 id="消息消费-2">消息消费</h4>
<p>消费者并发消费的核心函数在ConsumeMessageConcurrentlyService#submitConsumeRequest，具体逻辑如下：</p>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService
public void submitConsumeRequest(
    final List&lt;MessageExt&gt; msgs,
    final ProcessQueue processQueue,
    final MessageQueue messageQueue,
    final boolean dispatchToConsume) {
    // 消息批次，默认为1  
    final int consumeBatchSize = this.defaultMQPushConsumer.getConsumeMessageBatchMaxSize();
     // msgs.size()默认为32条，受DefaultMQPushConsumer#pullBatchSize属性控制
    // 当 msgs.size()小于等于consumeBatchSize 
    if (msgs.size() &lt;= consumeBatchSize) {
        // 将提交消息放入ConsumeRequest 
        ConsumeRequest consumeRequest = new ConsumeRequest(msgs, processQueue, messageQueue);
        try {
            // 提交至消费者线程池
            this.consumeExecutor.submit(consumeRequest);
        } catch (RejectedExecutionException e) {
            // 提交过程中出现拒绝提交异常，则延迟5秒再提交。
            // consumeExecutor使用的是LinkedBlockingQueue，不设置大小的情况下LinkedBlockingQueue为无界队列，因此一般不会出现拒绝提交异常
            this.submitConsumeRequestLater(consumeRequest);
        }
    } else {
        // 当 msgs.size()大于consumeBatchSize 时，采用分页提交，每页页数为consumeBatchSize
        for (int total = 0; total &lt; msgs.size(); ) {
            List&lt;MessageExt&gt; msgThis = new ArrayList&lt;MessageExt&gt;(consumeBatchSize);
            for (int i = 0; i &lt; consumeBatchSize; i++, total++) {
                if (total &lt; msgs.size()) {
                    msgThis.add(msgs.get(total));
                } else {
                    break;
                }
            }
            // 将提交消息放入ConsumeRequest 
            ConsumeRequest consumeRequest = new ConsumeRequest(msgThis, processQueue, messageQueue);
            try {
                // 提交至消费者线程池
                this.consumeExecutor.submit(consumeRequest);
            } catch (RejectedExecutionException e) {
                for (; total &lt; msgs.size(); total++) {
                    msgThis.add(msgs.get(total));
                }
                // 提交过程中出现拒绝提交异常，则延迟5秒再提交 
                this.submitConsumeRequestLater(consumeRequest);
            }
        }
    }
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService#ConsumeRequest
// 具体的逻辑处理线程类
public void run() {
    // 如果processQueue的dropped设置为true，则停止对该队列的消费操作
    // 在消息队列重平衡期间，此值可能会被设置true
    if (this.processQueue.isDropped()) {
        log.info(&quot;the message queue not be able to consume, because it's dropped. group={} {}&quot;, ConsumeMessageConcurrentlyService.this.consumerGroup, this.messageQueue);
        return;
    }
    // 获取并发消费监听器
    MessageListenerConcurrently listener = ConsumeMessageConcurrentlyService.this.messageListener;
    // 通过messageQueue获取并发消费上下文
    ConsumeConcurrentlyContext context = new ConsumeConcurrentlyContext(messageQueue);
    ConsumeConcurrentlyStatus status = null;
    // 重置重试主题名，当存在重试主题（RETRY_TOPIC）或者namespace时，需要重置主题名
    defaultMQPushConsumerImpl.resetRetryAndNamespace(msgs, defaultMQPushConsumer.getConsumerGroup());

    ConsumeMessageContext consumeMessageContext = null;
    if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) {
        consumeMessageContext = new ConsumeMessageContext();
        consumeMessageContext.setNamespace(defaultMQPushConsumer.getNamespace());
        consumeMessageContext.setConsumerGroup(defaultMQPushConsumer.getConsumerGroup());
        consumeMessageContext.setProps(new HashMap&lt;String, String&gt;());
        consumeMessageContext.setMq(messageQueue);
        consumeMessageContext.setMsgList(msgs);
        consumeMessageContext.setSuccess(false);
        // 在消息消费之前，执行增强处理逻辑
        ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.executeHookBefore(consumeMessageContext);
    }

    long beginTimestamp = System.currentTimeMillis();
    boolean hasException = false;
    ConsumeReturnType returnType = ConsumeReturnType.SUCCESS;
    try {
        if (msgs != null &amp;&amp; !msgs.isEmpty()) {
            for (MessageExt msg : msgs) {
                MessageAccessor.setConsumeStartTimeStamp(msg, String.valueOf(System.currentTimeMillis()));
            }
        }
        // 通过监听器的consumeMessage函数处理具体的消息消费
        status = listener.consumeMessage(Collections.unmodifiableList(msgs), context);
    } catch (Throwable e) {
        log.warn(String.format(&quot;consumeMessage exception: %s Group: %s Msgs: %s MQ: %s&quot;,
            RemotingHelper.exceptionSimpleDesc(e),
            ConsumeMessageConcurrentlyService.this.consumerGroup,
            msgs,
            messageQueue), e);
        hasException = true;
    }
    // 设置返回状态returnType 
    long consumeRT = System.currentTimeMillis() - beginTimestamp;
    if (null == status) {
        if (hasException) {
            returnType = ConsumeReturnType.EXCEPTION;
        } else {
            returnType = ConsumeReturnType.RETURNNULL;
        }
    } else if (consumeRT &gt;= defaultMQPushConsumer.getConsumeTimeout() * 60 * 1000) {
        returnType = ConsumeReturnType.TIME_OUT;
    } else if (ConsumeConcurrentlyStatus.RECONSUME_LATER == status) {
        returnType = ConsumeReturnType.FAILED;
    } else if (ConsumeConcurrentlyStatus.CONSUME_SUCCESS == status) {
        returnType = ConsumeReturnType.SUCCESS;
    }

    if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) {
        consumeMessageContext.getProps().put(MixAll.CONSUME_CONTEXT_TYPE, returnType.name());
    }

    if (null == status) {
        log.warn(&quot;consumeMessage return null, Group: {} Msgs: {} MQ: {}&quot;,
            ConsumeMessageConcurrentlyService.this.consumerGroup,
            msgs,
            messageQueue);
        status = ConsumeConcurrentlyStatus.RECONSUME_LATER;
    }

    if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) {
        consumeMessageContext.setStatus(status.toString());
        consumeMessageContext.setSuccess(ConsumeConcurrentlyStatus.CONSUME_SUCCESS == status);
        // 在消息消费之前，执行增强处理逻辑
        ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.executeHookAfter(consumeMessageContext);
    }

    ConsumeMessageConcurrentlyService.this.getConsumerStatsManager()
        .incConsumeRT(ConsumeMessageConcurrentlyService.this.consumerGroup, messageQueue.getTopic(), consumeRT);
    // 执行消息结果处理之前，校验processQueue是否被丢弃；
    // 因为在前述处理步骤中，如果出现消费队列重平衡或原先的消费者宕机而将该队列分配给新的消费者，那么应用程序认为此消息可能会重复消费
    if (!processQueue.isDropped()) {
        ConsumeMessageConcurrentlyService.this.processConsumeResult(status, context, this);
    } else {
        log.warn(&quot;processQueue is dropped without process consume result. messageQueue={}, msgs={}&quot;, messageQueue, msgs);
    }
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService
public void processConsumeResult(
    final ConsumeConcurrentlyStatus status,
    final ConsumeConcurrentlyContext context,
    final ConsumeRequest consumeRequest
) {
    // 定义ack返回值 
    int ackIndex = context.getAckIndex();

    if (consumeRequest.getMsgs().isEmpty())
        return;

    switch (status) {
        case CONSUME_SUCCESS:
            // 当ackIndex 大于消息数时，将其值设置为消息数
            if (ackIndex &gt;= consumeRequest.getMsgs().size()) {
                ackIndex = consumeRequest.getMsgs().size() - 1;
            }
            int ok = ackIndex + 1;
            int failed = consumeRequest.getMsgs().size() - ok;
            this.getConsumerStatsManager().incConsumeOKTPS(consumerGroup, consumeRequest.getMessageQueue().getTopic(), ok);
            this.getConsumerStatsManager().incConsumeFailedTPS(consumerGroup, consumeRequest.getMessageQueue().getTopic(), failed);
            break;
        case RECONSUME_LATER:
            // 重试时，需要将其设置为-1，主要是为了下次发送ACK做准备
            ackIndex = -1;
            this.getConsumerStatsManager().incConsumeFailedTPS(consumerGroup, consumeRequest.getMessageQueue().getTopic(),
                consumeRequest.getMsgs().size());
            break;
        default:
            break;
    }

    switch (this.defaultMQPushConsumer.getMessageModel()) {
        case BROADCASTING:
            // 广播模式中，无需发送ACK，并针对重试消息打印失败日志
            for (int i = ackIndex + 1; i &lt; consumeRequest.getMsgs().size(); i++) {
                MessageExt msg = consumeRequest.getMsgs().get(i);
                log.warn(&quot;BROADCASTING, the message consume failed, drop it, {}&quot;, msg.toString());
            }
            break;
        case CLUSTERING:
            // 集群模式
            List&lt;MessageExt&gt; msgBackFailed = new ArrayList&lt;MessageExt&gt;(consumeRequest.getMsgs().size());
            // 消费成功时，ackIndex + 1 = consumeRequest.getMsgs().size()，故不会发送ACK
            // 消息重试消费时，针对每批消息都需要发送ACK给客户端
            for (int i = ackIndex + 1; i &lt; consumeRequest.getMsgs().size(); i++) {
                MessageExt msg = consumeRequest.getMsgs().get(i);
                boolean result = this.sendMessageBack(msg, context);
                // 当ACK消息发送失败时，消息会统计进msgBackFailed
                if (!result) {
                    msg.setReconsumeTimes(msg.getReconsumeTimes() + 1);
                    msgBackFailed.add(msg);
                }
            }
            // 针对msgBackFailed数据，会延迟5s再次进行消费
            if (!msgBackFailed.isEmpty()) {
                consumeRequest.getMsgs().removeAll(msgBackFailed);
                this.submitConsumeRequestLater(msgBackFailed, consumeRequest.getProcessQueue(), consumeRequest.getMessageQueue());
            }
            break;
        default:
            break;
    }

    // 从ProcessQueue中移除该批消息，此处返回的offset为当前队列中的最小偏移量。
    long offset = consumeRequest.getProcessQueue().removeMessage(consumeRequest.getMsgs());
    if (offset &gt;= 0 &amp;&amp; !consumeRequest.getProcessQueue().isDropped()) {
        // 使用上述offset更新消息消费进度，以便消费者重启后能从上一次消费进度开始消费，防止重复消费。
        // 值得注意的是：监听器返回RECONSUME_LATER时，消息消费进度依然会向前推进；
        // 因为当返回RECONSUME_LATER时，rocketmq会创建一条与原先属性完全一致的消息，并生成一个新的msgId，该消息会存入commitlog，这相当于一条全新的消息，自然也拥有全新的offset。
        this.defaultMQPushConsumerImpl.getOffsetStore().updateOffset(consumeRequest.getMessageQueue(), offset, true);
    }
}

</code></pre>
<h4 id="消息确认ack">消息确认（ACK）</h4>
<p>当消息监听器返回的消费结果为RECONSUME_LATER，则需要将这些消息发送给Broker延迟消息。如果ACK发送失败，将延迟5s后提交至线程池继续进行消费。</p>
<pre><code>// 文件地址：org.apache.rocketmq.common.protocol.header.ConsumerSendMsgBackRequestHeader
// ConsumerSendMsgBackRequestHeader核心属性
// 消息物理偏移量
private Long offset;
// 消费组名
private String group;
// 延迟级别，固定为1-18
private Integer delayLevel;
// 原消息id
private String originMsgId;
// 原消息主题
private String originTopic;
// 单位模型
private boolean unitMode = false;
// 最大重消费次数，默认为16次
private Integer maxReconsumeTimes;

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.common.subscription.SubscriptionGroupConfig
// 获取消费组的订阅信息，如果不存在将会返回错误 SUBSCRIPTION_GROUP_NOT_EXIST
// 核心属性如下
// 消费组名
private String groupName;
// 是否可消费，默认true
private boolean consumeEnable = true;
// 是否允许从最小offset开始消费，默认true
private boolean consumeFromMinEnable = true;
// 是否能以广播的形式消费，默认true
private boolean consumeBroadcastEnable = true;
// 重试队列数，默认1，即每个broker上一个重试队列
private int retryQueueNums = 1;
// 最大重试次数，默认为16次
private int retryMaxTimes = 16;
// masterId
private long brokerId = MixAll.MASTER_ID;
// 当（主broker）消息阻塞时，将转向该brokerId的服务器上拉取消息，默认为1
private long whichBrokerWhenConsumeSlowly = 1;
// 当消息发送变化时是否立即进行消息队列重平衡
private boolean notifyConsumerIdsChangedEnable = true;

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.broker.processor.SendMessageProcessor#asyncConsumerSendMsgBack
// 创建重试主题，格式：%RETRY% + 消费组名
String newTopic = MixAll.getRetryTopic(requestHeader.getGroup());
// 从重试队列中随机选择一个
int queueIdInt = ThreadLocalRandom.current().nextInt(99999999) % subscriptionGroupConfig.getRetryQueueNums();

// 根据ConsumerSendMsgBackRequestHeader中的offset从commitlog中获取消息
MessageExt msgExt = this.brokerController.getMessageStore().lookMessageByOffset(requestHeader.getOffset());
if (null == msgExt) {
    response.setCode(ResponseCode.SYSTEM_ERROR);
    response.setRemark(&quot;look message by offset failed, &quot; + requestHeader.getOffset());
    return CompletableFuture.completedFuture(response);
}
// 获取重试主题
final String retryTopic = msgExt.getProperty(MessageConst.PROPERTY_RETRY_TOPIC);
if (null == retryTopic) {
    MessageAccessor.putProperty(msgExt, MessageConst.PROPERTY_RETRY_TOPIC, msgExt.getTopic());
}
msgExt.setWaitStoreMsgOK(false);

// 当消息次数超过maxReconsumeTimes时，主题名将再次改变，格式：%DLQ% + 消费组名；此主题权限为只写，如需再操作则需人工干预
newTopic = MixAll.getDLQTopic(requestHeader.getGroup());
queueIdInt = ThreadLocalRandom.current().nextInt(99999999) % DLQ_NUMS_PER_GROUP;

// 根据原来的消息创建一条新的内部消息进行存储，此处也对应了上小节的updateOffset操作
MessageExtBrokerInner msgInner = new MessageExtBrokerInner();
msgInner.setTopic(newTopic);
msgInner.setBody(msgExt.getBody());
msgInner.setFlag(msgExt.getFlag());
MessageAccessor.setProperties(msgInner, msgExt.getProperties());
msgInner.setPropertiesString(MessageDecoder.messageProperties2String(msgExt.getProperties()));
msgInner.setTagsCode(MessageExtBrokerInner.tagsString2tagsCode(null, msgExt.getTags()));

msgInner.setQueueId(queueIdInt);
msgInner.setSysFlag(msgExt.getSysFlag());
msgInner.setBornTimestamp(msgExt.getBornTimestamp());
msgInner.setBornHost(msgExt.getBornHost());
msgInner.setStoreHost(msgExt.getStoreHost());
msgInner.setReconsumeTimes(msgExt.getReconsumeTimes() + 1);

String originMsgId = MessageAccessor.getOriginMessageId(msgExt);
MessageAccessor.setOriginMessageId(msgInner, UtilAll.isBlank(originMsgId) ? msgExt.getMsgId() : originMsgId);
msgInner.setPropertiesString(MessageDecoder.messageProperties2String(msgExt.getProperties()));
// 此处的消息重试主要依托于定时机制来实现。
CompletableFuture&lt;PutMessageResult&gt; putMessageResult = this.brokerController.getMessageStore().asyncPutMessage(msgInner);

</code></pre>
<h4 id="消费进度管理">消费进度管理</h4>
<p>当消息被消费完毕后，需要记录其已被消费，以防止出现重复消费。在RocketMQ中消费模式分为广播和集群，因此对应的消费进度也分两种情况。</p>
<ul>
<li>广播模式：同一个消费组下的消费者都需要消费主题下的所有消息，也就是说组内的消费者对于消息的消费行为是彼此独立，互不干扰的。在这种情况下消费进度最好与消费者绑定并独立存储。</li>
<li>集群模式：同一个消费组下的消费者共享主题下的所有消息，同一条消息在同一时间只能被组内的一个消费者消费，并且随着消息队列的动态变化而重新负载（重平衡）。在这种情况下消费进度最好存储在所有消费者都能访问到的地方（例如broker）。</li>
</ul>
<pre><code>// 文件地址：org.apache.rocketmq.client.consumer.store.OffsetStore
/**
 * 将消费进度从存储文件加载至内存中
 */
void load() throws MQClientException;

/**
 * 更新内存中的消费进度
 * 参数 increaseOnly 为true时表示offset必须大于内存中当前的消费进度才更新
 */
void updateOffset(final MessageQueue mq, final long offset, final boolean increaseOnly);

/**
 * 依据类型获取消费进度
 * ReadOffsetType: READ_FROM_MEMORY(内存),READ_FROM_STORE(存储),MEMORY_FIRST_THEN_STORE(先内存后存储)
 */
long readOffset(final MessageQueue mq, final ReadOffsetType type);

/**
 * Persist all offsets,may be in local storage or remote name server
 */
void persistAll(final Set&lt;MessageQueue&gt; mqs);

/**
 * 持久化消费进度---》本地（广播模式）或远程服务broker（集群模式）
 */
void persist(final MessageQueue mq);

/**
 * 移除消费进度
 */
void removeOffset(MessageQueue mq);

/**
 * 克隆主题下所有队列的消费进度
 */
Map&lt;MessageQueue, Long&gt; cloneOffsetTable(String topic);

/**
 * 更新存储在broker上的消费进度（集群模式）
 */
void updateConsumeOffsetToBroker(MessageQueue mq, long offset, boolean isOneway) throws RemotingException,
    MQBrokerException, InterruptedException, MQClientException;

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.client.consumer.store.LocalFileOffsetStore
// 广播模式中，消费进度是存储在消费者本地，具体实现逻辑如下：
// 核心属性
// 消费进度存储目录，可通过 -Drocketmq.client.localOffsetStoreDir 指定存储目录，默认存储在主目录下
public final static String LOCAL_OFFSET_STORE_DIR = System.getProperty(
    &quot;rocketmq.client.localOffsetStoreDir&quot;,
    System.getProperty(&quot;user.home&quot;) + File.separator + &quot;.rocketmq_offsets&quot;);
// 消息客户端
private final MQClientInstance mQClientFactory;
// 消费组名
private final String groupName;
// 消费进度存储文件路径
private final String storePath;
// 消费进度存储在内存中的对象
private ConcurrentMap&lt;MessageQueue, AtomicLong&gt; offsetTable =
    new ConcurrentHashMap&lt;MessageQueue, AtomicLong&gt;();

// 此函数是由MQClientInstance#startScheduledTask定时任务开启，每5s会执行一次
// 持久化存储消费进度    
public void persistAll(Set&lt;MessageQueue&gt; mqs) {
    if (null == mqs || mqs.isEmpty())
        return;
    // 从内存中获取消费进度信息，并将其封装至OffsetSerializeWrapper 
    OffsetSerializeWrapper offsetSerializeWrapper = new OffsetSerializeWrapper();
    for (Map.Entry&lt;MessageQueue, AtomicLong&gt; entry : this.offsetTable.entrySet()) {
        if (mqs.contains(entry.getKey())) {
            AtomicLong offset = entry.getValue();
            offsetSerializeWrapper.getOffsetTable().put(entry.getKey(), offset);
        }
    }
    // 将其转为json
    String jsonString = offsetSerializeWrapper.toJson(true);
    if (jsonString != null) {
        try {
            // 持久化至文件中
            MixAll.string2File(jsonString, this.storePath);
        } catch (IOException e) {
            log.error(&quot;persistAll consumer offset Exception, &quot; + this.storePath, e);
        }
    }
} 

</code></pre>
<p>集群模式中，消费进度存储在消息服务端broker。无论是广播模式还是集群模式，消费进度的管理逻辑基本一致，只不过广播是操作本地存储，集群操作远程存储。集群模式大致实现原理图如下所示：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320010.png" alt="img" loading="lazy"></figure>
<h3 id="定时消息机制">定时消息机制</h3>
<p>定时消息是指消息发送到broker之后，并不会立刻被消费者所感知，而是等候特定时间后才能被消费。RocketMQ只支持固定的延迟级别：1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h 总共18个级别，使用时，可以通过设置msg.setDelayLevel(level)来发送延迟消息。对level设值时会出现以下是那种情形：</p>
<ul>
<li>level == 0，消息为非延迟消息</li>
<li>1&lt;=level&lt;=maxLevel，消息延迟特定时间，例如level==1，延迟1s</li>
<li>level &gt; maxLevel，则level== maxLevel，例如level==20，延迟2h</li>
</ul>
<p>定时任务设计的关键：单独创建一个主题 SCHEDULE_TOPIC_XXXX 作为定时任务专属，该主题下的队列数量等于 MessageStoreConfig#messageDelayLevel 配置的延迟级别数量，其对应关系为：消息队列id = 延迟级别 - 1。ScheduleMessageService 会为每个延迟级别创建一个定时Timer，并根据延迟级别对应的延迟时间进行任务调度。</p>
<p>在消息发送时，如果消息的延迟级别delayLevel大于0，那么便会将其发送至内部的延迟主题的延迟队列下，之后再根据上述步骤进行任务处理。</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.schedule.ScheduleMessageService
public boolean load() {
    // 加载消费进度存储文件至内存中
    boolean result = super.load();
    // 将 1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h 映射为 1-18
    result = result &amp;&amp; this.parseDelayLevel();
    // 赋值 offsetTable---》用于在内存中管理延迟消费进度
    result = result &amp;&amp; this.correctDelayOffset();
    return result;
}

public void start() {
    // CAS加锁启动
    if (started.compareAndSet(false, true)) {
        super.load();
        this.deliverExecutorService = new ScheduledThreadPoolExecutor(this.maxDelayLevel, new ThreadFactoryImpl(&quot;ScheduleMessageTimerThread_&quot;));
        // 是否启用异步延迟
        if (this.enableAsyncDeliver) {
            this.handleExecutorService = new ScheduledThreadPoolExecutor(this.maxDelayLevel, new ThreadFactoryImpl(&quot;ScheduleMessageExecutorHandleThread_&quot;));
        }
        // 遍历所有的延迟级别
        for (Map.Entry&lt;Integer, Long&gt; entry : this.delayLevelTable.entrySet()) {
            Integer level = entry.getKey();
            Long timeDelay = entry.getValue();
            // 根据延迟级别从offsetTable中获取消费进度；这里说明了一个延迟级别对应一个消息队列
            Long offset = this.offsetTable.get(level);、
            // offsetTable 中不存在消费进度便直接使用0
            if (null == offset) {
                offset = 0L;
            }

            // 每个定时任务第一次启动时都默认延迟1s执行一次，随后才会使用对应的延迟时间执行
            if (timeDelay != null) {
                if (this.enableAsyncDeliver) {
                    this.handleExecutorService.schedule(new HandlePutResultTask(level), FIRST_DELAY_TIME, TimeUnit.MILLISECONDS);
                }
                this.deliverExecutorService.schedule(new DeliverDelayedMessageTimerTask(level, offset), FIRST_DELAY_TIME, TimeUnit.MILLISECONDS);
            }
        }

        // 创建定时任务，默认每10s持久化一次延迟队列的消费进度
        this.deliverExecutorService.scheduleAtFixedRate(new Runnable() {
            @Override
            public void run() {
                try {
                    if (started.get()) {
                        ScheduleMessageService.this.persist();
                    }
                } catch (Throwable e) {
                    log.error(&quot;scheduleAtFixedRate flush exception&quot;, e);
                }
            }
        }, 10000, this.defaultMessageStore.getMessageStoreConfig().getFlushDelayOffsetInterval(), TimeUnit.MILLISECONDS);
    }
}

// 延迟级别与消息队列的映射关系：消息队列id = 延迟级别 - 1
public static int queueId2DelayLevel(final int queueId) {
    return queueId + 1;
}
public static int delayLevel2QueueId(final int delayLevel) {
    return delayLevel - 1;
}

</code></pre>
<h4 id="定时调度逻辑">定时调度逻辑</h4>
<p>在 ScheduleMessageService#start 函数启动时，会为每个延迟任务创建一个对应的调度任务，而调度任务的核心函数便是ScheduleMessageService$DeliverDelayedMessageTimerTask#executeOnTimeup。整体流程如下：</p>
<ol>
<li>在消息发送时，如果消息的延迟级别delayLevel大于0，则改变消息主题为 SCHEDULE_TOPIC_XXX，消息队列为延迟级别减1。</li>
<li>消息首先会经由commitlog转发至 SCHEDULE_TOPIC_XXX 的消费队列0。</li>
<li>定时任务Time每隔1s根据上次拉取偏移量从消费队列中拉取出所有的消息。</li>
<li>根据消息物理偏移量及消息大小从commitlog中取出对应的延迟消息。</li>
<li>根据取出的消息属性重建新的消息对象，并转发至真正的原主题及原消息队列，此过程会清除延迟级别，同时会再次存入commitlog。</li>
<li>订阅了原主题的消费者正常消费消息。</li>
</ol>
<pre><code>// 文件地址：org.apache.rocketmq.store.schedule.ScheduleMessageService#DeliverDelayedMessageTimerTask
public void executeOnTimeup() {
    // 根据延迟主题和队列id查询消费队列
    ConsumeQueue cq =
        ScheduleMessageService.this.defaultMessageStore.findConsumeQueue(TopicValidator.RMQ_SYS_SCHEDULE_TOPIC,
            delayLevel2QueueId(delayLevel));
    // 消费队列不存在， 说明目前不存在该延迟级别的消息
    if (cq == null) {
         // 等待100ms后，再次执行该延迟级别的任务
        this.scheduleNextTimerTask(this.offset, DELAY_FOR_A_WHILE);
        return;
    }
    // 根据offset从消费队列中获取所有有效消息
    SelectMappedBufferResult bufferCQ = cq.getIndexBuffer(this.offset);
    // 未找到有效消息
    if (bufferCQ == null) {
        long resetOffset;
        if ((resetOffset = cq.getMinOffsetInQueue()) &gt; this.offset) {
            log.error(&quot;schedule CQ offset invalid. offset={}, cqMinOffset={}, queueId={}&quot;,
                this.offset, resetOffset, cq.getQueueId());
        } else if ((resetOffset = cq.getMaxOffsetInQueue()) &lt; this.offset) {
            log.error(&quot;schedule CQ offset invalid. offset={}, cqMaxOffset={}, queueId={}&quot;,
                this.offset, resetOffset, cq.getQueueId());
        } else {
            resetOffset = this.offset;
        }
        // 等待100ms后，再次执行该延迟级别的任务
        this.scheduleNextTimerTask(resetOffset, DELAY_FOR_A_WHILE);
        return;
    }

    long nextOffset = this.offset;
    try {
        int i = 0;
        ConsumeQueueExt.CqExtUnit cqExtUnit = new ConsumeQueueExt.CqExtUnit();
        // 遍历消费队列，每个消费队列条目为20个字节。
        for (; i &lt; bufferCQ.getSize() &amp;&amp; isStarted(); i += ConsumeQueue.CQ_STORE_UNIT_SIZE) {
            // 解析偏移量
            long offsetPy = bufferCQ.getByteBuffer().getLong();
            // 解析消息大小
            int sizePy = bufferCQ.getByteBuffer().getInt();
            // 解析tag信息
            long tagsCode = bufferCQ.getByteBuffer().getLong();
            if (cq.isExtAddr(tagsCode)) {
                if (cq.getExt(tagsCode, cqExtUnit)) {
                    tagsCode = cqExtUnit.getTagsCode();
                } else {
                    //can't find ext content.So re compute tags code.
                    log.error(&quot;[BUG] can't find consume queue extend file content!addr={}, offsetPy={}, sizePy={}&quot;,
                        tagsCode, offsetPy, sizePy);
                    long msgStoreTime = defaultMessageStore.getCommitLog().pickupStoreTimestamp(offsetPy, sizePy);
                    tagsCode = computeDeliverTimestamp(delayLevel, msgStoreTime);
                }
            }

            long now = System.currentTimeMillis();
            long deliverTimestamp = this.correctDeliverTimestamp(now, tagsCode);
            nextOffset = offset + (i / ConsumeQueue.CQ_STORE_UNIT_SIZE);
            // 比较延迟任务执行时间戳与当前时间戳，尽量保证在指定的延迟时间执行任务
            long countdown = deliverTimestamp - now;
            if (countdown &gt; 0) {
                // 等待100ms后，再次执行该延迟级别的任务
                this.scheduleNextTimerTask(nextOffset, DELAY_FOR_A_WHILE);
                return;
            }
            // 从commitlog中获取消息
            MessageExt msgExt = ScheduleMessageService.this.defaultMessageStore.lookMessageByOffset(offsetPy, sizePy);
            // 没有消息时，略过本次循环
            if (msgExt == null) {
                continue;
            }
            // 根据消息重建新的消息对象，清除消息的延迟级别，并恢复消息原先的消息主题和消费队列。
            MessageExtBrokerInner msgInner = ScheduleMessageService.this.messageTimeup(msgExt);
            // 判断是否为事务的半消息主题
            if (TopicValidator.RMQ_SYS_TRANS_HALF_TOPIC.equals(msgInner.getTopic())) {
                log.error(&quot;[BUG] the real topic of schedule msg is {}, discard the msg. msg={}&quot;,
                    msgInner.getTopic(), msgInner);
                continue;
            }
        
            // 开始投递消息，此过程中消息会再次存入commitlog中，并转发至对应的消息主题及消费队列以供消费者消费，同时也会更新延迟队列拉取进度。
            boolean deliverSuc;
            if (ScheduleMessageService.this.enableAsyncDeliver) {
                deliverSuc = this.asyncDeliver(msgInner, msgExt.getMsgId(), offset, offsetPy, sizePy);
            } else {
                deliverSuc = this.syncDeliver(msgInner, msgExt.getMsgId(), offset, offsetPy, sizePy);
            }

            if (!deliverSuc) {
                // 等待100ms后，再次执行该延迟级别的任务
                this.scheduleNextTimerTask(nextOffset, DELAY_FOR_A_WHILE);
                return;
            }
        }

        nextOffset = this.offset + (i / ConsumeQueue.CQ_STORE_UNIT_SIZE);
    } catch (Exception e) {
        log.error(&quot;ScheduleMessageService, messageTimeup execute error, offset = {}&quot;, nextOffset, e);
    } finally {
        bufferCQ.release();
    }
    // 等待100ms后，再次执行该延迟级别的任务
    this.scheduleNextTimerTask(nextOffset, DELAY_FOR_A_WHILE);
}

// 延迟指定时间后，再次执行this.delayLevel延迟级别的任务
public void scheduleNextTimerTask(long offset, long delay) {
    ScheduleMessageService.this.deliverExecutorService.schedule(new DeliverDelayedMessageTimerTask(
        this.delayLevel, offset), delay, TimeUnit.MILLISECONDS);
}

</code></pre>
<h3 id="消息过滤机制">消息过滤机制</h3>
<p>RocketMQ支持两种过滤模式：表达式过滤和类过滤，其中表达式过滤又分为TAG和SQL92。表达式过滤中TAG实现最为简单，就是给消息定义一个标签，消费者会根据tag进行匹配；SQL92则是依据SQL条件过滤表达式来过滤对应的消息属性；类过滤模式中允许提交一个过滤类至filterServer，消费者将从filterServer拉取消息，此过程中当消息经过filterServer时会执行对应的过滤逻辑。</p>
<h4 id="表达式tag过滤">表达式TAG过滤</h4>
<p>消息发送者在发送消息时如果设置了tag属性，那么该字段值将会一同存储在commitlog中，但在消费队列中会用8字节来存储tag的hashcode，这是因为consumerQueue采用了定长的设计，以此来加快加载速度。</p>
<p>broker端拉取消息时，仅仅只会对比tag的hashcode，如果匹配则返回，否则忽略该消息。consumer收到消息后，会再次对比tag的实际值以进行二次过滤。</p>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl
// 消费者订阅主题与消息过滤表达式
public void subscribe(String topic, String subExpression) throws MQClientException {
    try {
        // 根据主题及表达式构建订阅信息
        SubscriptionData subscriptionData = FilterAPI.buildSubscriptionData(topic, subExpression);
        // 将上述信息加入rebalanceImpl，以便后续进行消息重平衡
        this.rebalanceImpl.getSubscriptionInner().put(topic, subscriptionData);
        if (this.mQClientFactory != null) {
            // 发送心跳
            this.mQClientFactory.sendHeartbeatToAllBrokerWithLock();
        }
    } catch (Exception e) {
        throw new MQClientException(&quot;subscription exception&quot;, e);
    }
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl#pullMessage
String subExpression = null;
boolean classFilter = false;
SubscriptionData sd = this.rebalanceImpl.getSubscriptionInner().get(pullRequest.getMessageQueue().getTopic());
if (sd != null) {
    if (this.defaultMQPushConsumer.isPostSubscriptionWhenPull() &amp;&amp; !sd.isClassFilterMode()) {
        subExpression = sd.getSubString();
    }

    classFilter = sd.isClassFilterMode();
}
// 根据表达式过滤和了过滤构建系统拉取标签
int sysFlag = PullSysFlag.buildSysFlag(
    commitOffsetEnable, // commitOffset
    true, // suspend
    subExpression != null, // subscription
    classFilter // class filter
);

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.broker.processor.PullMessageProcessor#processRequest
// 根据消息表达式构建订阅信息
subscriptionData = FilterAPI.build(
    requestHeader.getTopic(), requestHeader.getSubscription(), requestHeader.getExpressionType()
);
// 类过滤模式下，使用consumerFilterData 
if (!ExpressionType.isTagType(subscriptionData.getExpressionType())) {
    consumerFilterData = ConsumerFilterManager.build(
        requestHeader.getTopic(), requestHeader.getConsumerGroup(), requestHeader.getSubscription(),
        requestHeader.getExpressionType(), requestHeader.getSubVersion()
    );
    assert consumerFilterData != null;
}

// 初始化消息过滤对象
MessageFilter messageFilter;
if (this.brokerController.getBrokerConfig().isFilterSupportRetry()) {
    // 支持对重试主题的过滤
    messageFilter = new ExpressionForRetryMessageFilter(subscriptionData, consumerFilterData,
        this.brokerController.getConsumerFilterManager());
} else {
    messageFilter = new ExpressionMessageFilter(subscriptionData, consumerFilterData,
        this.brokerController.getConsumerFilterManager());
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore#getMessage
// 根据consumerQueue过滤消息
if (messageFilter != null
    &amp;&amp; !messageFilter.isMatchedByConsumeQueue(isTagsCodeLegal ? tagsCode : null, extRet ? cqExtUnit : null)) {
    if (getResult.getBufferTotalSize() == 0) {
        status = GetMessageStatus.NO_MATCHED_MESSAGE;
    }

    continue;
}

// 如果通过consumerQueue过滤成功，则从commitlog中加载整个消息体，根据消息属性进行过滤
if (messageFilter != null
    &amp;&amp; !messageFilter.isMatchedByCommitLog(selectResult.getByteBuffer().slice(), null)) {
    if (getResult.getBufferTotalSize() == 0) {
        status = GetMessageStatus.NO_MATCHED_MESSAGE;
    }
    // release...
    selectResult.release();
    continue;
}

</code></pre>
<h4 id="类过滤模式">类过滤模式</h4>
<p>类过滤模式是指在broker端运行一个或多个filterServer，rocketmq允许消费者自定义过滤实现类并将其上传至filterServer上；消费者向filterServer拉取消息，filterServer将消费者的拉取请求转发至broker，然后对返回的消息进行过滤操作，最终将过滤后的消息返回给消费者。大体流程如下：</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320011.png" alt="" loading="lazy"></figure>
<p>filterServer注册至broker的流程与发送者或消费者的注册流程基本一致，都是定时向broker发送心跳包。broker端接收到心跳后，会维护filterServer的属性信息，并定时监控其状态。随后broker会与所有存活的nameServer进行通信，将filterServer信息存储至nameServer上，方便后续消费者从nameServer获取filterServer的相关信息。</p>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.PullAPIWrapper#pullKernelImpl
// 消息拉取时，如果发现消息过滤模式为classFilter，那么便会变更brokerAddr地址为filterServer的地址
if (PullSysFlag.hasClassFilterFlag(sysFlagInner)) {
    brokerAddr = computePullFromWhichFilterServer(mq.getTopic(), brokerAddr);
}

private String computePullFromWhichFilterServer(final String topic, final String brokerAddr)
    throws MQClientException {
     // 获取消息主题路由table  
    ConcurrentMap&lt;String, TopicRouteData&gt; topicRouteTable = this.mQClientFactory.getTopicRouteTable();
    if (topicRouteTable != null) {
        // 通过主题名获取对应的主题路由信息
        TopicRouteData topicRouteData = topicRouteTable.get(topic);
        // 从中获取所有的filterServer列表
        List&lt;String&gt; list = topicRouteData.getFilterServerTable().get(brokerAddr);
        // 随机选出一个filterServer返回
        if (list != null &amp;&amp; !list.isEmpty()) {
            return list.get(randomNum() % list.size());
        }
    }

    throw new MQClientException(&quot;Find Filter Server Failed, Broker Addr: &quot; + brokerAddr + &quot; topic: &quot;
        + topic, null);
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RocketMQ源码解读之消息消费(上)]]></title>
        <id>https://philosopherzb.github.io/post/rocketmq-yuan-ma-jie-du-zhi-xiao-xi-xiao-fei-shang/</id>
        <link href="https://philosopherzb.github.io/post/rocketmq-yuan-ma-jie-du-zhi-xiao-xi-xiao-fei-shang/">
        </link>
        <updated>2022-10-29T05:37:44.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Rocketmq源码之消息消费上篇。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/desert-74781_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="消息消费">消息消费</h2>
<h3 id="简介">简介</h3>
<p>RocketMQ中是以组的模式进行消息消费的，一个消费组内包含多个消费者，每个消费组可订阅多个主题，消费组之间有集群模式和广播模式两种消费方式。</p>
<ul>
<li>
<p>集群模式：主题下的同一条消息只允许被消费组中的一个消费者所消费。</p>
</li>
<li>
<p>广播模式：主题下的同一消息可被组内的所有消息消费一次。</p>
<p>RocketMQ中消息服务器与消费者之间的消息传递也有两种方式：推模式，拉模式。</p>
</li>
<li>
<p>拉模式（pull）：消费者主动从broker拉取消息进行消费。</p>
</li>
<li>
<p>推模式（push）：指消息到达broker后会推送至消费者，实际上是对拉模式的一个封装，本质上仍然是拉模式。</p>
</li>
</ul>
<h3 id="启动流程">启动流程</h3>
<p>以DefaultMQPushConsumerImpl#start为例，简单介绍消费者启动流程</p>
<pre><code>// 文件地址：org.apache.rocketmq.client.consumer.DefaultMQPushConsumer
public synchronized void start() throws MQClientException {
    switch (this.serviceState) {
        case CREATE_JUST:
            log.info(&quot;the consumer [{}] start beginning. messageModel={}, isUnitMode={}&quot;, this.defaultMQPushConsumer.getConsumerGroup(),
                this.defaultMQPushConsumer.getMessageModel(), this.defaultMQPushConsumer.isUnitMode());
            this.serviceState = ServiceState.START_FAILED;
            // 检查配置信息
            this.checkConfig();
            // 复制订阅信息
            this.copySubscription();

            // 集群模式下，改变消费者的instanceName为进程ID
            if (this.defaultMQPushConsumer.getMessageModel() == MessageModel.CLUSTERING) {
                // 如果instanceName为“DEFAULT”时，此处改变函数将会将instanceName更改为进程id + 当前纳秒
                // this.instanceName = UtilAll.getPid() + &quot;#&quot; + System.nanoTime();
                this.defaultMQPushConsumer.changeInstanceNameToPID();
            }

            // 创建MQClientInstance实例，整个JVM中只存在一个MQClientManager实例，其中维护了一个factoryTable用于存储clientId与MQClientInstance
            // private ConcurrentMap&lt;String/* clientId */, MQClientInstance&gt; factoryTable 
            // 也就是说一个clientId只会存在一个至于对应的MQClientInstance
            this.mQClientFactory = MQClientManager.getInstance().getOrCreateMQClientInstance(this.defaultMQPushConsumer, this.rpcHook);
        
            // 设置rebalanceImpl属性
            this.rebalanceImpl.setConsumerGroup(this.defaultMQPushConsumer.getConsumerGroup());
            this.rebalanceImpl.setMessageModel(this.defaultMQPushConsumer.getMessageModel());
            this.rebalanceImpl.setAllocateMessageQueueStrategy(this.defaultMQPushConsumer.getAllocateMessageQueueStrategy());
            this.rebalanceImpl.setmQClientFactory(this.mQClientFactory);

            // 封装拉模式API 
            this.pullAPIWrapper = new PullAPIWrapper(
                mQClientFactory,
                this.defaultMQPushConsumer.getConsumerGroup(), isUnitMode());
            this.pullAPIWrapper.registerFilterMessageHook(filterMessageHookList);

            // 初始化消息消费进度  
            if (this.defaultMQPushConsumer.getOffsetStore() != null) {
                this.offsetStore = this.defaultMQPushConsumer.getOffsetStore();
            } else {
                // 不存在storeOffset时，表示是第一次消费
                switch (this.defaultMQPushConsumer.getMessageModel()) {
                    case BROADCASTING:
                        // 广播模式中，offset存储在消费者端
                        this.offsetStore = new LocalFileOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup());
                        break;
                    case CLUSTERING:
                        // 集群模式中，offset存储在broker
                        this.offsetStore = new RemoteBrokerOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup());
                        break;
                    default:
                        break;
                }
                this.defaultMQPushConsumer.setOffsetStore(this.offsetStore);
            }
            this.offsetStore.load();

            // 判断是否顺序消息，来创建对应的consumeMessageService并启动---&gt;consumeMessageService内部维护了一个线程池用于消费消息
            if (this.getMessageListenerInner() instanceof MessageListenerOrderly) {
                this.consumeOrderly = true;
                this.consumeMessageService =
                    new ConsumeMessageOrderlyService(this, (MessageListenerOrderly) this.getMessageListenerInner());
            } else if (this.getMessageListenerInner() instanceof MessageListenerConcurrently) {
                this.consumeOrderly = false;
                this.consumeMessageService =
                    new ConsumeMessageConcurrentlyService(this, (MessageListenerConcurrently) this.getMessageListenerInner());
            }

            this.consumeMessageService.start();
        
            // 将当前消费加入MQClientInstance中进行管理，方便后续调用网络请求，发送心跳包等
            boolean registerOK = mQClientFactory.registerConsumer(this.defaultMQPushConsumer.getConsumerGroup(), this);
            if (!registerOK) {
                this.serviceState = ServiceState.CREATE_JUST;
                this.consumeMessageService.shutdown(defaultMQPushConsumer.getAwaitTerminationMillisWhenShutdown());
                throw new MQClientException(&quot;The consumer group[&quot; + this.defaultMQPushConsumer.getConsumerGroup()
                    + &quot;] has been created before, specify another name please.&quot; + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL),
                    null);
            }
            // 启动 MQClientInstance
            mQClientFactory.start();
            log.info(&quot;the consumer [{}] start OK.&quot;, this.defaultMQPushConsumer.getConsumerGroup());
            this.serviceState = ServiceState.RUNNING;
            break;
        case RUNNING:
        case START_FAILED:
        case SHUTDOWN_ALREADY:
            throw new MQClientException(&quot;The PushConsumer service state not OK, maybe started once, &quot;
                + this.serviceState
                + FAQUrl.suggestTodo(FAQUrl.CLIENT_SERVICE_NOT_OK),
                null);
        default:
            break;
    }
    // 如果订阅改变了，便更新主题订阅信息
    this.updateTopicSubscribeInfoWhenSubscriptionChanged();
    // 检查broker中的客户端信息
    this.mQClientFactory.checkClientInBroker();
    // 发送心跳
    this.mQClientFactory.sendHeartbeatToAllBrokerWithLock();
    // 立刻进行重平衡操作
    this.mQClientFactory.rebalanceImmediately();
}

private void copySubscription() throws MQClientException {
    try {
        // 获取订阅内容
        Map&lt;String, String&gt; sub = this.defaultMQPushConsumer.getSubscription();
        if (sub != null) {
            for (final Map.Entry&lt;String, String&gt; entry : sub.entrySet()) {
                final String topic = entry.getKey();
                final String subString = entry.getValue();
                SubscriptionData subscriptionData = FilterAPI.buildSubscriptionData(topic, subString);
                // 按照主题分组存入rebalanceImpl---&gt;消费者负载均衡的核心类
                this.rebalanceImpl.getSubscriptionInner().put(topic, subscriptionData);
            }
        }
        // 配置内部监听器
        if (null == this.messageListenerInner) {
            this.messageListenerInner = this.defaultMQPushConsumer.getMessageListener();
        }

        switch (this.defaultMQPushConsumer.getMessageModel()) {
            case BROADCASTING:
                break;
            case CLUSTERING:
                // 集群模式下，订阅重试主题消息。RocketMQ消息重试是以组为单位的--&gt;重试主题格式：%RETRY% + 消费组名
                final String retryTopic = MixAll.getRetryTopic(this.defaultMQPushConsumer.getConsumerGroup());
                SubscriptionData subscriptionData = FilterAPI.buildSubscriptionData(retryTopic, SubscriptionData.SUB_ALL);
                // 按照重试主题消费组分组存入rebalanceImpl---&gt;消费者负载均衡的核心类
                this.rebalanceImpl.getSubscriptionInner().put(retryTopic, subscriptionData);
                break;
            default:
                break;
        }
    } catch (Exception e) {
        throw new MQClientException(&quot;subscription exception&quot;, e);
    }
}

</code></pre>
<h3 id="消息拉取">消息拉取</h3>
<p>消息消费有两种模式，此节将以集群模式来简单介绍push模式的消息消费。</p>
<p>需要注意的是，rocketmq并没有实现真正意义上的推模式，本质上仍然是消费者主动向服务器拉取消息，只不过整个拉取过程被封装在了后台处理。</p>
<p>在此过程中，如果消息还未到达消息队列且未开启长轮询机制，那么会在服务端等待shortPollingTimeMills=1000时间后（相当于挂起）再去判断消息是否已到达消息队列；如果开启了长轮询机制longPollingEnable=true，rocketmq会每隔5s轮询检查一次消息是否到达，在消息到达时便会立即通知挂起线程验证该消息是否为可消费消息，如果是则从commitlog中提取消息并返回给客户端，否则便一直挂起直到超时（push模式默认15s；pull模式可通过设置DefaultMQPullConsumer#brokerSuspendMaxTimeMillis，该值默认20s不建议修改）。</p>
<p>长轮询核心源码：org.apache.rocketmq.broker.longpolling.PullRequestHoldService</p>
<p>整个消息拉取过程为分三步：客户端封装消息拉取请求，消息服务器查找并返回消息，客户端处理返回消息。大体流程如下图所示：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320009.png" alt="" loading="lazy"></figure>
<h4 id="客户端封装消息拉取请求">客户端封装消息拉取请求</h4>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl
public void pullMessage(final PullRequest pullRequest) {
    // 获取ProcessQueue ---&gt; ProcessQueue 本质上是对messageQueue的封装
    final ProcessQueue processQueue = pullRequest.getProcessQueue();
    // 当前处理队列已被丢弃则直接终止拉取操作
    if (processQueue.isDropped()) {
        log.info(&quot;the pull request[{}] is dropped.&quot;, pullRequest.toString());
        return;
    }
    // 更新最后一次拉取时间戳为当前时间戳
    pullRequest.getProcessQueue().setLastPullTimestamp(System.currentTimeMillis());

    try {
        // 检查服务状态是否处于RUNNING
        this.makeSureStateOK();
    } catch (MQClientException e) {
        log.warn(&quot;pullMessage exception, consumer state not ok&quot;, e);
        this.executePullRequestLater(pullRequest, pullTimeDelayMillsWhenException);
        return;
    }
    // 如果当前消费者被挂起，则将拉取任务延迟1s再放入PullMessageService的拉取任务队列中，之后结束拉取操作
    if (this.isPause()) {
        log.warn(&quot;consumer was paused, execute pull request later. instanceName={}, group={}&quot;, this.defaultMQPushConsumer.getInstanceName(), this.defaultMQPushConsumer.getConsumerGroup());
        this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_SUSPEND);
        return;
    }
  
    // 获取已缓存的消息数 
    long cachedMessageCount = processQueue.getMsgCount().get();
    // 获取已缓存的消息大小--》单位MB
    long cachedMessageSizeInMiB = processQueue.getMsgSize().get() / (1024 * 1024);
  
    // 如果processQueue当前处理的消息条数超过了pullThresholdForQueue=1000时，将触发流控
    // 将拉取任务延迟50ms再放入PullMessageService的拉取任务队列中，并终止拉取操作
    if (cachedMessageCount &gt; this.defaultMQPushConsumer.getPullThresholdForQueue()) {
        this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL);
        if ((queueFlowControlTimes++ % 1000) == 0) {
            log.warn(
                &quot;the cached message count exceeds the threshold {}, so do flow control, minOffset={}, maxOffset={}, count={}, size={} MiB, pullRequest={}, flowControlTimes={}&quot;,
                this.defaultMQPushConsumer.getPullThresholdForQueue(), processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), cachedMessageCount, cachedMessageSizeInMiB, pullRequest, queueFlowControlTimes);
        }
        return;
    }
    // 如果processQueue当前处理的消息大小超过了pullThresholdForQueue=100MB时，将触发流控
    // 将拉取任务延迟50ms再放入PullMessageService的拉取任务队列中，并终止拉取操作
    if (cachedMessageSizeInMiB &gt; this.defaultMQPushConsumer.getPullThresholdSizeForQueue()) {
        this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL);
        if ((queueFlowControlTimes++ % 1000) == 0) {
            log.warn(
                &quot;the cached message size exceeds the threshold {} MiB, so do flow control, minOffset={}, maxOffset={}, count={}, size={} MiB, pullRequest={}, flowControlTimes={}&quot;,
                this.defaultMQPushConsumer.getPullThresholdSizeForQueue(), processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), cachedMessageCount, cachedMessageSizeInMiB, pullRequest, queueFlowControlTimes);
        }
        return;
    }
  
    // 非顺序消费
    if (!this.consumeOrderly) {
        // 如果processQueue当前处理的消息最大偏移量与最小偏移量的间距超过了consumeConcurrentlyMaxSpan=2000时，将触发流控
        // 将拉取任务延迟50ms再放入PullMessageService的拉取任务队列中，并终止拉取操作
        if (processQueue.getMaxSpan() &gt; this.defaultMQPushConsumer.getConsumeConcurrentlyMaxSpan()) {
            this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL);
            if ((queueMaxSpanFlowControlTimes++ % 1000) == 0) {
                log.warn(
                    &quot;the queue's messages, span too long, so do flow control, minOffset={}, maxOffset={}, maxSpan={}, pullRequest={}, flowControlTimes={}&quot;,
                    processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), processQueue.getMaxSpan(),
                    pullRequest, queueMaxSpanFlowControlTimes);
            }
            return;
        }
    } else {
        // 顺序消息，需要判断是否已加锁
        if (processQueue.isLocked()) {
            // 是否上次加锁的数据
            if (!pullRequest.isPreviouslyLocked()) {
                long offset = -1L;
                try {
                    // 获取偏移量
                    offset = this.rebalanceImpl.computePullFromWhereWithException(pullRequest.getMessageQueue());
                } catch (Exception e) {
                    this.executePullRequestLater(pullRequest, pullTimeDelayMillsWhenException);
                    log.error(&quot;Failed to compute pull offset, pullResult: {}&quot;, pullRequest, e);
                    return;
                }
                boolean brokerBusy = offset &lt; pullRequest.getNextOffset();
                log.info(&quot;the first time to pull message, so fix offset from broker. pullRequest: {} NewOffset: {} brokerBusy: {}&quot;,
                    pullRequest, offset, brokerBusy);
                if (brokerBusy) {
                    log.info(&quot;[NOTIFYME]the first time to pull message, but pull request offset larger than broker consume offset. pullRequest: {} NewOffset: {}&quot;,
                        pullRequest, offset);
                }
                // 重置前锁与下一次offset 
                pullRequest.setPreviouslyLocked(true);
                pullRequest.setNextOffset(offset);
            }
        } else {
            // 未加锁顺序消息不允许消费----》为了保证顺序
            // 将拉取任务延迟3s再放入PullMessageService的拉取任务队列中，并终止拉取操作
            this.executePullRequestLater(pullRequest, pullTimeDelayMillsWhenException);
            log.info(&quot;pull message later because not locked in broker, {}&quot;, pullRequest);
            return;
        }
    }
    // 获取订阅信息
    final SubscriptionData subscriptionData = this.rebalanceImpl.getSubscriptionInner().get(pullRequest.getMessageQueue().getTopic());
    // 订阅信息为空
    if (null == subscriptionData) {
        // 将拉取任务延迟3s再放入PullMessageService的拉取任务队列中，并终止拉取操作
        this.executePullRequestLater(pullRequest, pullTimeDelayMillsWhenException);
        log.warn(&quot;find the consumer's subscription failed, {}&quot;, pullRequest);
        return;
    }

    final long beginTimestamp = System.currentTimeMillis();
    // 针对pull回调处理 ----》在后续 客户端处理返回消息 小节中详细介绍，此处暂且省略
    PullCallback pullCallback = new PullCallback() {};

    // 判断 commitOffsetEnable 是否启用
    boolean commitOffsetEnable = false;
    long commitOffsetValue = 0L;
    // 集群模式下，从broker读取commitOffset，如果大于0表示commitOffset可用
    if (MessageModel.CLUSTERING == this.defaultMQPushConsumer.getMessageModel()) {
        commitOffsetValue = this.offsetStore.readOffset(pullRequest.getMessageQueue(), ReadOffsetType.READ_FROM_MEMORY);
        if (commitOffsetValue &gt; 0) {
            commitOffsetEnable = true;
        }
    }
    // 消息过滤模式---》subExpression（TAG，SQL92） 和 classFilter 
    String subExpression = null;
    boolean classFilter = false;
    SubscriptionData sd = this.rebalanceImpl.getSubscriptionInner().get(pullRequest.getMessageQueue().getTopic());
    if (sd != null) {
        if (this.defaultMQPushConsumer.isPostSubscriptionWhenPull() &amp;&amp; !sd.isClassFilterMode()) {
            subExpression = sd.getSubString();
        }

        classFilter = sd.isClassFilterMode();
    }
    // 构建系统标记
    int sysFlag = PullSysFlag.buildSysFlag(
        commitOffsetEnable, // commitOffset
        true, // suspend
        subExpression != null, // subscription
        classFilter // class filter
    );
    try {
        // 与服务端进行交互拉取消息
        this.pullAPIWrapper.pullKernelImpl(
            pullRequest.getMessageQueue(),
            subExpression,
            subscriptionData.getExpressionType(),
            subscriptionData.getSubVersion(),
            pullRequest.getNextOffset(),
            this.defaultMQPushConsumer.getPullBatchSize(),
            sysFlag,
            commitOffsetValue,
            BROKER_SUSPEND_MAX_TIME_MILLIS,
            CONSUMER_TIMEOUT_MILLIS_WHEN_SUSPEND,
            CommunicationMode.ASYNC,
            pullCallback
        );
    } catch (Exception e) {
        log.error(&quot;pullKernelImpl exception&quot;, e);
        // 将拉取任务延迟3s再放入PullMessageService的拉取任务队列中
        this.executePullRequestLater(pullRequest, pullTimeDelayMillsWhenException);
    }
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.PullAPIWrapper
public PullResult pullKernelImpl(
    // 从哪个消息队列拉取消息
    final MessageQueue mq,
    // 消息过滤表达式
    final String subExpression,
    // 消息表达式类型--》TAG 和 SQL92
    final String expressionType,
    // 订阅版本号
    final long subVersion,
    // 拉取偏移量 
    final long offset,
    // 本次拉取最大消息条数，默认32条
    final int maxNums,
    // 系统标记
    final int sysFlag,
    // 当前MessageQueue的消费进度
    final long commitOffset,
    // 消息拉取过程中允许broker挂起时间，默认15s
    final long brokerSuspendMaxTimeMillis,
    // 消息拉取超时时间，默认30s
    final long timeoutMillis,
    // 消息拉取模式，默认异步拉取
    final CommunicationMode communicationMode,
    // 从broker拉取消息后的回调函数
    final PullCallback pullCallback
) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {
    // 获取broker地址，主要是为了下面进行远程通讯
    FindBrokerResult findBrokerResult =
        this.mQClientFactory.findBrokerAddressInSubscribe(mq.getBrokerName(),
            this.recalculatePullFromWhichNode(mq), false);
    if (null == findBrokerResult) {
        this.mQClientFactory.updateTopicRouteInfoFromNameServer(mq.getTopic());
        findBrokerResult =
            this.mQClientFactory.findBrokerAddressInSubscribe(mq.getBrokerName(),
                this.recalculatePullFromWhichNode(mq), false);
    }

    if (findBrokerResult != null) {
        {
            // check version
            if (!ExpressionType.isTagType(expressionType)
                &amp;&amp; findBrokerResult.getBrokerVersion() &lt; MQVersion.Version.V4_1_0_SNAPSHOT.ordinal()) {
                throw new MQClientException(&quot;The broker[&quot; + mq.getBrokerName() + &quot;, &quot;
                    + findBrokerResult.getBrokerVersion() + &quot;] does not upgrade to support for filter message by &quot; + expressionType, null);
            }
        }
        int sysFlagInner = sysFlag;

        if (findBrokerResult.isSlave()) {
            sysFlagInner = PullSysFlag.clearCommitOffsetFlag(sysFlagInner);
        }
        // 拼接请求数据
        PullMessageRequestHeader requestHeader = new PullMessageRequestHeader();
        requestHeader.setConsumerGroup(this.consumerGroup);
        requestHeader.setTopic(mq.getTopic());
        requestHeader.setQueueId(mq.getQueueId());
        requestHeader.setQueueOffset(offset);
        requestHeader.setMaxMsgNums(maxNums);
        requestHeader.setSysFlag(sysFlagInner);
        requestHeader.setCommitOffset(commitOffset);
        requestHeader.setSuspendTimeoutMillis(brokerSuspendMaxTimeMillis);
        requestHeader.setSubscription(subExpression);
        requestHeader.setSubVersion(subVersion);
        requestHeader.setExpressionType(expressionType);
    
        String brokerAddr = findBrokerResult.getBrokerAddr();
        // 如果为类过滤模式，则需要根据主题名称和broker地址找到注册在Broker上的filterServer地址，随后将从filterServer上拉取消息
        if (PullSysFlag.hasClassFilterFlag(sysFlagInner)) {
            brokerAddr = computePullFromWhichFilterServer(mq.getTopic(), brokerAddr);
        }
        // 拉取数据
        PullResult pullResult = this.mQClientFactory.getMQClientAPIImpl().pullMessage(
            brokerAddr,
            requestHeader,
            timeoutMillis,
            communicationMode,
            pullCallback);

        return pullResult;
    }

    throw new MQClientException(&quot;The broker[&quot; + mq.getBrokerName() + &quot;] not exist&quot;, null);
}

</code></pre>
<h4 id="消息服务器组装消息并返回">消息服务器组装消息并返回</h4>
<p>根据消息拉取命令code: RequestCode.PULL_MESSAGE，可以从broker端找到对应的处理函数。</p>
<pre><code>// 文件地址：org.apache.rocketmq.broker.processor.PullMessageProcessor#processRequest
// 查找消息
final GetMessageResult getMessageResult =
    this.brokerController.getMessageStore().getMessage(requestHeader.getConsumerGroup(), requestHeader.getTopic(),
        requestHeader.getQueueId(), requestHeader.getQueueOffset(), requestHeader.getMaxMsgNums(), messageFilter);

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore#getMessage
// 消息获取状态，默认NO_MESSAGE_IN_QUEUE
GetMessageStatus status = GetMessageStatus.NO_MESSAGE_IN_QUEUE;
// 待查找的起始偏移量
long nextBeginOffset = offset;
// 最小偏移量
long minOffset = 0;
// 最大偏移量
long maxOffset = 0;
// 消息获取结果，当查找到消息时再初始化
GetMessageResult getResult = null;
// 当前文件最大的偏移量
final long maxOffsetPy = this.commitLog.getMaxOffset();
// 根据主题及queueId寻找对应的ConsumeQueue 
ConsumeQueue consumeQueue = findConsumeQueue(topic, queueId);

// 当前消费队列中没有消息
if (maxOffset == 0) {
    status = GetMessageStatus.NO_MESSAGE_IN_QUEUE;
    nextBeginOffset = nextOffsetCorrection(offset, 0);
} 
// 待拉取消息偏移量小于队列的最小偏移量
else if (offset &lt; minOffset) {
    status = GetMessageStatus.OFFSET_TOO_SMALL;
    nextBeginOffset = nextOffsetCorrection(offset, minOffset);
} 
// 待拉取消息偏移量等于队列的最大偏移量--》直接从待拉取消息偏移量开始拉取
else if (offset == maxOffset) {
    status = GetMessageStatus.OFFSET_OVERFLOW_ONE;
    nextBeginOffset = nextOffsetCorrection(offset, offset);
} 
// 待拉取消息偏移量大于队列的最大偏移量
else if (offset &gt; maxOffset) {
    status = GetMessageStatus.OFFSET_OVERFLOW_BADLY;
    if (0 == minOffset) {
        nextBeginOffset = nextOffsetCorrection(offset, minOffset);
    } else {
        nextBeginOffset = nextOffsetCorrection(offset, maxOffset);
    }
}

// 填充获取消息的结果信息
getResult.setStatus(status);
getResult.setNextBeginOffset(nextBeginOffset);
getResult.setMaxOffset(maxOffset);
getResult.setMinOffset(minOffset);

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore
// 修正下次拉取offset
private long nextOffsetCorrection(long oldOffset, long newOffset) {
    // 默认使用传入的oldOffset作为下一次拉取偏移量
    long nextOffset = oldOffset;
    // 如果broker不会从节点或者OffsetCheckInSlave为true，则使用传入的newOffset最为下一次拉取偏移量
    if (this.getMessageStoreConfig().getBrokerRole() != BrokerRole.SLAVE || this.getMessageStoreConfig().isOffsetCheckInSlave()) {
        nextOffset = newOffset;
    }
    return nextOffset;
}

</code></pre>
<h4 id="客户端处理返回消息">客户端处理返回消息</h4>
<p>核心在于回调函数的处理，其中包括成功与失败。</p>
<pre><code>// 文件地址：org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl#pullMessage
PullCallback pullCallback = new PullCallback() {
    @Override
    public void onSuccess(PullResult pullResult) {
        if (pullResult != null) {
            // 获取拉取消息的结果信息
            pullResult = DefaultMQPushConsumerImpl.this.pullAPIWrapper.processPullResult(pullRequest.getMessageQueue(), pullResult,
                subscriptionData);

            switch (pullResult.getPullStatus()) {
                case FOUND:
                    // 更新下次拉取偏移量
                    long prevRequestOffset = pullRequest.getNextOffset();
                    pullRequest.setNextOffset(pullResult.getNextBeginOffset());
                    // 更新拉取间隔时间
                    long pullRT = System.currentTimeMillis() - beginTimestamp;
                    DefaultMQPushConsumerImpl.this.getConsumerStatsManager().incPullRT(pullRequest.getConsumerGroup(),
                        pullRequest.getMessageQueue().getTopic(), pullRT);

                    long firstMsgOffset = Long.MAX_VALUE;
                    // 当MsgFoundList为空时----》由于TAG模式过滤，可能会导致MsgFoundList为空
                    if (pullResult.getMsgFoundList() == null || pullResult.getMsgFoundList().isEmpty()) {
                        // 立即将拉取任务放入PullMessageService的拉取任务队列中，以便PullMessageService及时唤醒并再次执行拉取任务
                        DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest);
                    } else {
                        firstMsgOffset = pullResult.getMsgFoundList().get(0).getQueueOffset();

                        DefaultMQPushConsumerImpl.this.getConsumerStatsManager().incPullTPS(pullRequest.getConsumerGroup(),
                            pullRequest.getMessageQueue().getTopic(), pullResult.getMsgFoundList().size());
                        // 将拉取到的消息存入processQueue
                        boolean dispatchToConsume = processQueue.putMessage(pullResult.getMsgFoundList());
                        // 提交至consumeMessageService，以便消费者进行消费---》此函数异步执行
                        DefaultMQPushConsumerImpl.this.consumeMessageService.submitConsumeRequest(
                            pullResult.getMsgFoundList(),
                            processQueue,
                            pullRequest.getMessageQueue(),
                            dispatchToConsume);

                        // 判断PullInterval是否大于0，如果是，则等待该时间间隔再执行下次拉取，否则立即执行拉取操作
                        // 此逻辑主要是为了实现持续拉取，以达到准实时拉取消息的效果
                        if (DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval() &gt; 0) {
                            DefaultMQPushConsumerImpl.this.executePullRequestLater(pullRequest,
                                DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval());
                        } else {
                            DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest);
                        }
                    }

                    if (pullResult.getNextBeginOffset() &lt; prevRequestOffset
                        || firstMsgOffset &lt; prevRequestOffset) {
                        log.warn(
                            &quot;[BUG] pull message result maybe data wrong, nextBeginOffset: {} firstMsgOffset: {} prevRequestOffset: {}&quot;,
                            pullResult.getNextBeginOffset(),
                            firstMsgOffset,
                            prevRequestOffset);
                    }

                    break;
                case NO_NEW_MSG:
                case NO_MATCHED_MSG:
                    // 消息不匹配的情况下，纠正偏移量然后立即执行下次拉取操作
                    pullRequest.setNextOffset(pullResult.getNextBeginOffset());

                    DefaultMQPushConsumerImpl.this.correctTagsOffset(pullRequest);

                    DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest);
                    break;
                case OFFSET_ILLEGAL:
                    log.warn(&quot;the pull request offset illegal, {} {}&quot;,
                        pullRequest.toString(), pullResult.toString());
                    pullRequest.setNextOffset(pullResult.getNextBeginOffset());
                    // 当offset非法时，丢弃该ProcessQueue---&gt;即该ProcessQueue中的消息将停止被消费
                    pullRequest.getProcessQueue().setDropped(true);
                    // 延迟执行任务
                    DefaultMQPushConsumerImpl.this.executeTaskLater(new Runnable() {

                        @Override
                        public void run() {
                            try {
                                // 依据服务端下次校验的偏移量NextOffset去更新内存中的消息消费进度
                                DefaultMQPushConsumerImpl.this.offsetStore.updateOffset(pullRequest.getMessageQueue(),
                                    pullRequest.getNextOffset(), false);
                                // 尝试持久化该消息队列
                                DefaultMQPushConsumerImpl.this.offsetStore.persist(pullRequest.getMessageQueue());
                                // 从rebalanceImpl的处理队列中将其移除---&gt;这意味着不再从该消息队列拉取消息，直到下次队列重平衡为止
                                DefaultMQPushConsumerImpl.this.rebalanceImpl.removeProcessQueue(pullRequest.getMessageQueue());

                                log.warn(&quot;fix the pull request offset, {}&quot;, pullRequest);
                            } catch (Throwable e) {
                                log.error(&quot;executeTaskLater Exception&quot;, e);
                            }
                        }
                    }, 10000);
                    break;
                default:
                    break;
            }
        }
    }

    @Override
    public void onException(Throwable e) {
        if (!pullRequest.getMessageQueue().getTopic().startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) {
            log.warn(&quot;execute the pull request exception&quot;, e);
        }
        // 将拉取任务延迟3s再放入PullMessageService的拉取任务队列中
        DefaultMQPushConsumerImpl.this.executePullRequestLater(pullRequest, pullTimeDelayMillsWhenException);
    }
};

</code></pre>
<h3 id="重平衡分配算法">重平衡分配算法</h3>
<p>重平衡过程源码可参见《RocketMQ架构与设计》文章中的 2.4.2小节 Consumer负载均衡，此处主要简单介绍重平衡的一些算法。</p>
<p>rocketmq默认使用的是平均分配：AllocateMessageQueueAveragely</p>
<ul>
<li>AllocateMachineRoomNearby：就近机房分配策略</li>
<li>AllocateMessageQueueAveragely：平均散列队列算法--》默认分配算法</li>
<li>AllocateMessageQueueAveragelyByCircle：轮询平均散列队列算法----》常用分配算法</li>
<li>AllocateMessageQueueByConfig：通过配置进行分配，相当于手动指定消费者的消息队列</li>
<li>AllocateMessageQueueByMachineRoom：通过机房分配</li>
<li>AllocateMessageQueueConsistentHash：通过一致性哈希分配</li>
</ul>
<pre><code>// 文件地址：org.apache.rocketmq.client.consumer.AllocateMessageQueueStrategy
// 重平衡消息队列分配算法的核心接口，如果默认的分配算法不满足业务，可以实现该接口自定义分配算法
public interface AllocateMessageQueueStrategy {

    /**
     * 分配算法核心函数
     *
     * @param consumerGroup current consumer group
     * @param currentCID current consumer id
     * @param mqAll message queue set in current topic
     * @param cidAll consumer set in current consumer group
     * @return The allocate result of given strategy
     */
    List&lt;MessageQueue&gt; allocate(
        final String consumerGroup,
        final String currentCID,
        final List&lt;MessageQueue&gt; mqAll,
        final List&lt;String&gt; cidAll
    );

    /**
     * Algorithm name
     *
     * @return The strategy name
     */
    String getName();
}

</code></pre>
<h4 id="allocatemachineroomnearby">AllocateMachineRoomNearby</h4>
<p>就近机房分配策略，仍需指定实际的分配策略。</p>
<p>基本逻辑：如果有任何消费者在机房中活着，部署在同一台机器上的broker的消息队列应该只分配给这些存活的消费者。否则，这些消息队列可以与所有消费者共享，因为没有活着的消费者来独享它们。</p>
<pre><code>public class AllocateMachineRoomNearby implements AllocateMessageQueueStrategy {
    private final InternalLogger log = ClientLogger.getLog();

    private final AllocateMessageQueueStrategy allocateMessageQueueStrategy;//actual allocate strategy
    private final MachineRoomResolver machineRoomResolver;
    // 指定实际的分配策略以及机房解析器 
    public AllocateMachineRoomNearby(AllocateMessageQueueStrategy allocateMessageQueueStrategy,
        MachineRoomResolver machineRoomResolver) throws NullPointerException {
        if (allocateMessageQueueStrategy == null) {
            throw new NullPointerException(&quot;allocateMessageQueueStrategy is null&quot;);
        }

        if (machineRoomResolver == null) {
            throw new NullPointerException(&quot;machineRoomResolver is null&quot;);
        }

        this.allocateMessageQueueStrategy = allocateMessageQueueStrategy;
        this.machineRoomResolver = machineRoomResolver;
    }

    @Override
    public List&lt;MessageQueue&gt; allocate(String consumerGroup, String currentCID, List&lt;MessageQueue&gt; mqAll,
        List&lt;String&gt; cidAll) {
        // 省略校验
        List&lt;MessageQueue&gt; result = new ArrayList&lt;MessageQueue&gt;();
        // 以机房名对消息队列进行分组
        Map&lt;String/*machine room */, List&lt;MessageQueue&gt;&gt; mr2Mq = new TreeMap&lt;String, List&lt;MessageQueue&gt;&gt;();
        for (MessageQueue mq : mqAll) {
            String brokerMachineRoom = machineRoomResolver.brokerDeployIn(mq);
            if (StringUtils.isNoneEmpty(brokerMachineRoom)) {
                if (mr2Mq.get(brokerMachineRoom) == null) {
                    mr2Mq.put(brokerMachineRoom, new ArrayList&lt;MessageQueue&gt;());
                }
                mr2Mq.get(brokerMachineRoom).add(mq);
            } else {
                throw new IllegalArgumentException(&quot;Machine room is null for mq &quot; + mq);
            }
        }

        // 以机房名对消费者进行分组
        Map&lt;String/*machine room */, List&lt;String/*clientId*/&gt;&gt; mr2c = new TreeMap&lt;String, List&lt;String&gt;&gt;();
        for (String cid : cidAll) {
            String consumerMachineRoom = machineRoomResolver.consumerDeployIn(cid);
            if (StringUtils.isNoneEmpty(consumerMachineRoom)) {
                if (mr2c.get(consumerMachineRoom) == null) {
                    mr2c.put(consumerMachineRoom, new ArrayList&lt;String&gt;());
                }
                mr2c.get(consumerMachineRoom).add(cid);
            } else {
                throw new IllegalArgumentException(&quot;Machine room is null for consumer id &quot; + cid);
            }
        }

        List&lt;MessageQueue&gt; allocateResults = new ArrayList&lt;MessageQueue&gt;();

        // 分配消息队列至同一机房的当前消费者
        String currentMachineRoom = machineRoomResolver.consumerDeployIn(currentCID);
        List&lt;MessageQueue&gt; mqInThisMachineRoom = mr2Mq.remove(currentMachineRoom);
        List&lt;String&gt; consumerInThisMachineRoom = mr2c.get(currentMachineRoom);
        if (mqInThisMachineRoom != null &amp;&amp; !mqInThisMachineRoom.isEmpty()) {
            // 使用实际上指定的分配策略进行再分配
            allocateResults.addAll(allocateMessageQueueStrategy.allocate(consumerGroup, currentCID, mqInThisMachineRoom, consumerInThisMachineRoom));
        }

        // 如果当前机房没有存活的消费者，那么便将剩余的消息队列分配给每个机房
        for (Entry&lt;String, List&lt;MessageQueue&gt;&gt; machineRoomEntry : mr2Mq.entrySet()) {
            if (!mr2c.containsKey(machineRoomEntry.getKey())) { 
                // 在相应的机房中没有存活的消费者，因此所有消费者都可共享这些消息队列
                allocateResults.addAll(allocateMessageQueueStrategy.allocate(consumerGroup, currentCID, machineRoomEntry.getValue(), cidAll));
            }
        }

        return allocateResults;
    }

    @Override
    public String getName() {
        return &quot;MACHINE_ROOM_NEARBY&quot; + &quot;-&quot; + allocateMessageQueueStrategy.getName();
    }

    /**
     * 一个解析器对象，用于确定消息队列或客户端部署在哪个机房。
     * AllocateMachineRoomNearby 会利用结果将消息队列和客户端按机房进行分组。
     * 注意：实现的方法返回的结果不能为空。 
     */
    public interface MachineRoomResolver {
        String brokerDeployIn(MessageQueue messageQueue);

        String consumerDeployIn(String clientID);
    }
}

</code></pre>
<h4 id="allocatemessagequeueaveragely">AllocateMessageQueueAveragely</h4>
<p>平均散列队列算法：消息队列数除以消费者总数，以确定分配给每个消费者的队列数。如果无法整除，那么额外的队列将会分配给排序靠前的消费者。</p>
<p>数学模型：设主题队列数为x，消费者数为y，设队列数除以消费者数的商为n、余数为m。那么在一个消费组中，前m个消费者将分配到n+1个队列，剩余的消费者分配n个队列。</p>
<p>例如：存在两个消费者C0，C1，两个主题t0，t1，且每个主题拥有三个队列，那么队列结果为：t0q0，t0q1，t0q2，t1q0，t1q1，t1q2。最终分配结果如下所示：</p>
<pre><code>// 依据上述数学模型，此处x=3，y=2，算出n=1，m=1，最终结果如下所示：
C0: [t0q0, t0q1, t1q0, t1q1]
C1: [t0q2, t1q2]

</code></pre>
<p>使用这种分配器的优势在于每个消费者都可以为每个主题获得一个队列，这对于某些负载均衡场景很有利。当然劣势也很明显，比如当遇到某个消费组订阅了多个Topic，那么可能会导致前几个消费者负载过重；又或者消费者数多于队列数时，将会有部分消费者处于空闲状态；且针对消费者进行扩缩容时，分配可能会发生较大的变化。</p>
<pre><code>public class AllocateMessageQueueAveragely implements AllocateMessageQueueStrategy {
    private final InternalLogger log = ClientLogger.getLog();

    @Override
    public List&lt;MessageQueue&gt; allocate(String consumerGroup, String currentCID, List&lt;MessageQueue&gt; mqAll,
        List&lt;String&gt; cidAll) {
        // 省略校验
        List&lt;MessageQueue&gt; result = new ArrayList&lt;MessageQueue&gt;();
        int index = cidAll.indexOf(currentCID);
        // 取模获取余数
        int mod = mqAll.size() % cidAll.size();
        // 获取平均大小
        int averageSize =
            mqAll.size() &lt;= cidAll.size() ? 1 : (mod &gt; 0 &amp;&amp; index &lt; mod ? mqAll.size() / cidAll.size()
                + 1 : mqAll.size() / cidAll.size());
        // 获取起始索引
        int startIndex = (mod &gt; 0 &amp;&amp; index &lt; mod) ? index * averageSize : index * averageSize + mod;
        // 获取遍历范围
        int range = Math.min(averageSize, mqAll.size() - startIndex);
        for (int i = 0; i &lt; range; i++) {
            result.add(mqAll.get((startIndex + i) % mqAll.size()));
        }
        return result;
    }

    @Override
    public String getName() {
        return &quot;AVG&quot;;
    }
}

</code></pre>
<h4 id="allocatemessagequeueaveragelybycircle">AllocateMessageQueueAveragelyByCircle</h4>
<p>轮询平均散列队列算法：直接轮询给每个消费者分配消息队列。算法实现整体上比较简单，源码如下：</p>
<pre><code>public class AllocateMessageQueueAveragelyByCircle implements AllocateMessageQueueStrategy {
    private final InternalLogger log = ClientLogger.getLog();

    @Override
    public List&lt;MessageQueue&gt; allocate(String consumerGroup, String currentCID, List&lt;MessageQueue&gt; mqAll,
        List&lt;String&gt; cidAll) {
        // 省略校验
        List&lt;MessageQueue&gt; result = new ArrayList&lt;MessageQueue&gt;();
        // 获取当前消费者在消费者列表中的索引 
        int index = cidAll.indexOf(currentCID);
        // 遍历消息队列表
        for (int i = index; i &lt; mqAll.size(); i++) {
            // 取模运算如果等于消费者索引，则进行分配
            if (i % cidAll.size() == index) {
                result.add(mqAll.get(i));
            }
        }
        return result;
    }

    @Override
    public String getName() {
        return &quot;AVG_BY_CIRCLE&quot;;
    }
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RocketMQ源码解读之消息存储(下)]]></title>
        <id>https://philosopherzb.github.io/post/rocketmq-yuan-ma-jie-du-zhi-xiao-xi-cun-chu-xia/</id>
        <link href="https://philosopherzb.github.io/post/rocketmq-yuan-ma-jie-du-zhi-xiao-xi-cun-chu-xia/">
        </link>
        <updated>2022-10-22T03:23:23.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Rocketmq源码之消息存储下篇。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/peak-190053_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="消息存储">消息存储</h2>
<h3 id="文件恢复">文件恢复</h3>
<p>RocketMQ所有消息都会先存储在commitLog中，随后再异步生成转发任务更新consumerQueue，indexFile文件。在此异步过程中，如果broker宕机，那么可能会出现数据不一致的问题。比如消息未同步至consumerQueue中，那么消费者将永远都消费不到该消息。此时便需要一种方案能够异步宕机导致的数据不一致问题。以下从源码角度分析rocketmq是如何实现数据同步操作的。</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore
// 文件加载
public boolean load() {
    boolean result = true;

    try {
        // 判断上一次退出是否正常
        boolean lastExitOK = !this.isTempFileExist();
        log.info(&quot;last shutdown {}&quot;, lastExitOK ? &quot;normally&quot; : &quot;abnormally&quot;);

        // 加载 CommitLog文件
        result = result &amp;&amp; this.commitLog.load();

        // 加载 ConsumeQueue文件---&gt;遍历消费队列根目录，获取所有主题文件，随后遍历每个主题文件，将其中的内容加载至ConsumeQueue对象中
        result = result &amp;&amp; this.loadConsumeQueue();

        if (result) {
            // 加载存储检查点，即文件对应的刷盘点
            this.storeCheckpoint =
                new StoreCheckpoint(StorePathConfigHelper.getStoreCheckpoint(this.messageStoreConfig.getStorePathRootDir()));
            // 加载索引文件---&gt;如果上次异常退出，且索引文件最后刷盘时间小于该文件最大的消息时间戳，那么该文件将被销毁
            this.indexService.load(lastExitOK);
            // 根据broker是否正常退出进行文件恢复
            this.recover(lastExitOK);

            log.info(&quot;load over, and the max phy offset = {}&quot;, this.getMaxPhyOffset());
            // 加载延迟消息相关的服务 
            if (null != scheduleMessageService) {
                result =  this.scheduleMessageService.load();
            }
        }

    } catch (Exception e) {
        log.error(&quot;load exception&quot;, e);
        result = false;
    }

    if (!result) {
        this.allocateMappedFileService.shutdown();
    }

    return result;
}

// 判断存储根目录下是否存在abort文件，存在说明为异常退出
// 正常退出时，abort文件将会被删除--&gt;可参见: org.apache.rocketmq.store.DefaultMessageStore#shutdown
private boolean isTempFileExist() {
    String fileName = StorePathConfigHelper.getAbortFile(this.messageStoreConfig.getStorePathRootDir());
    File file = new File(fileName);
    return file.exists();
}

// 加载commitLog文件
public boolean doLoad(List&lt;File&gt; files) {
    // ascending order
    // 按照文件名进行排序
    files.sort(Comparator.comparing(File::getName));

    for (File file : files) {
        // 如果文件大小与预先配置的不一致，将被忽略。
        if (file.length() != this.mappedFileSize) {
            log.warn(file + &quot;\t&quot; + file.length()
                    + &quot; length not matched message store config value, please check it manually&quot;);
            return false;
        }

        try {
            // 创建一个新的MappedFile 
            MappedFile mappedFile = new MappedFile(file.getPath(), mappedFileSize);
            // 设置三个指针为文件大小
            mappedFile.setWrotePosition(this.mappedFileSize);
            mappedFile.setFlushedPosition(this.mappedFileSize);
            mappedFile.setCommittedPosition(this.mappedFileSize);
            this.mappedFiles.add(mappedFile);
            log.info(&quot;load &quot; + file.getPath() + &quot; OK&quot;);
        } catch (IOException e) {
            log.error(&quot;load file &quot; + file + &quot; error&quot;, e);
            return false;
        }
    }
    return true;
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore
// 文件恢复
private void recover(final boolean lastExitOK) {
    long maxPhyOffsetOfConsumeQueue = this.recoverConsumeQueue();

    if (lastExitOK) {
        // 正常退出恢复
        this.commitLog.recoverNormally(maxPhyOffsetOfConsumeQueue);
    } else {
        // 异常退出恢复
        this.commitLog.recoverAbnormally(maxPhyOffsetOfConsumeQueue);
    }
    // 恢复TopicQueueTable
    this.recoverTopicQueueTable();
}

// 此函数表示消息中不仅存储了主题，队列id，还存储了消息队列的偏移量
public void recoverTopicQueueTable() {
    HashMap&lt;String/* topic-queueid */, Long/* offset */&gt; table = new HashMap&lt;String, Long&gt;(1024);
    // 获取commitLog的最小偏移量
    long minPhyOffset = this.commitLog.getMinOffset();
    for (ConcurrentMap&lt;Integer, ConsumeQueue&gt; maps : this.consumeQueueTable.values()) {
        for (ConsumeQueue logic : maps.values()) {
            String key = logic.getTopic() + &quot;-&quot; + logic.getQueueId();
            // 将队列中最大的偏移量存储table中
            table.put(key, logic.getMaxOffsetInQueue());
            logic.correctMinOffset(minPhyOffset);
        }
    }
    // 将table设置进commitLog的TopicQueueTable
    this.commitLog.setTopicQueueTable(table);
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.CommitLog
// broker正常退出，所有的文件都将恢复，内存数据也将被刷盘
public void recoverNormally(long maxPhyOffsetOfConsumeQueue) {
    // checkCRCOnRecover 表示文件恢复过程中查找消息时是否需要验证CRC
    boolean checkCRCOnRecover = this.defaultMessageStore.getMessageStoreConfig().isCheckCRCOnRecover();
    final List&lt;MappedFile&gt; mappedFiles = this.mappedFileQueue.getMappedFiles();
    if (!mappedFiles.isEmpty()) {
        // 从倒数第三个文件开始进行恢复
        int index = mappedFiles.size() - 3;
        // 如果不足三个文件，则从第一个文件开始
        if (index &lt; 0)
            index = 0;
        // 通过index获取MappedFile 
        MappedFile mappedFile = mappedFiles.get(index);
        // 获取共享ByteBuffer
        ByteBuffer byteBuffer = mappedFile.sliceByteBuffer();
        // commitLog已确认的物理偏移量
        long processOffset = mappedFile.getFileFromOffset();
        // 当前文件已校验通过offset
        long mappedFileOffset = 0;
        while (true) {
            // 检查消息是否合规，并返回消息大小
            DispatchRequest dispatchRequest = this.checkMessageAndReturnSize(byteBuffer, checkCRCOnRecover);
            int size = dispatchRequest.getMsgSize();
            // 消息合规，且消息大小大于0：mappedFileOffset 向前移动本条消息的长度
            if (dispatchRequest.isSuccess() &amp;&amp; size &gt; 0) {
                mappedFileOffset += size;
            }
            // Come the end of the file, switch to the next file Since the
            // return 0 representatives met last hole,
            // this can not be included in truncate offset
            // 消息合规，且消息大小等于0，表示已抵达文件末尾
            else if (dispatchRequest.isSuccess() &amp;&amp; size == 0) {
                index++;
                // 是否还有下一个文件，没有则直接跳出循环
                if (index &gt;= mappedFiles.size()) {
                    // Current branch can not happen
                    log.info(&quot;recover last 3 physics file over, last mapped file &quot; + mappedFile.getFileName());
                    break;
                } else {
                    // 否则重置processOffset，processOffset，并重复上述步骤
                    mappedFile = mappedFiles.get(index);
                    byteBuffer = mappedFile.sliceByteBuffer();
                    processOffset = mappedFile.getFileFromOffset();
                    processOffset = 0;
                    log.info(&quot;recover next physics file, &quot; + mappedFile.getFileName());
                }
            }
            // Intermediate file read error
            // 消息不合规，直接跳出循环
            else if (!dispatchRequest.isSuccess()) {
                log.info(&quot;recover physics file end, &quot; + mappedFile.getFileName());
                break;
            }
        }
        // processOffset 为 mappedFile.getFileFromOffset() + mappedFileOffset
        processOffset += mappedFileOffset;
        // 更新FlushedWhere，CommittedWhere指针
        this.mappedFileQueue.setFlushedWhere(processOffset);
        this.mappedFileQueue.setCommittedWhere(processOffset);
        // 删除processOffset之后的所有文件
        this.mappedFileQueue.truncateDirtyFiles(processOffset);

        // Clear ConsumeQueue redundant data
        // 清除processOffset之后的所有逻辑文件 
        if (maxPhyOffsetOfConsumeQueue &gt;= processOffset) {
            log.warn(&quot;maxPhyOffsetOfConsumeQueue({}) &gt;= processOffset({}), truncate dirty logic files&quot;, maxPhyOffsetOfConsumeQueue, processOffset);
            this.defaultMessageStore.truncateDirtyLogicFiles(processOffset);
        }
    } else {
        // Commitlog case files are deleted
        // commitlog 文件已被删除
        log.warn(&quot;The commitlog files are deleted, and delete the consume queue files&quot;);
        this.mappedFileQueue.setFlushedWhere(0);
        this.mappedFileQueue.setCommittedWhere(0);
        this.defaultMessageStore.destroyLogics();
    }
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.CommitLog
// broker异常退出，文件恢复过程与正常退出基本一致。
// 主要差别在于：异常退出会从最后一个文件开始往前遍历，找到第一个消息存储正常的文件进行恢复；
// 从此处也可看出rocketmq可能会存在重复消费的问题，消费幂等还是需要靠客户端自己保证。
// 判断MappedFile文件是否正常
private boolean isMappedFileMatchedRecover(final MappedFile mappedFile) {
    ByteBuffer byteBuffer = mappedFile.sliceByteBuffer();

     // 判断文件魔数是否等于 MESSAGE_MAGIC_CODE，如果不等于则表示该文件不符合commitLog存储格式
    int magicCode = byteBuffer.getInt(MessageDecoder.MESSAGE_MAGIC_CODE_POSTION);
    if (magicCode != MESSAGE_MAGIC_CODE) {
        return false;
    }
  
    int sysFlag = byteBuffer.getInt(MessageDecoder.SYSFLAG_POSITION);
    int bornhostLength = (sysFlag &amp; MessageSysFlag.BORNHOST_V6_FLAG) == 0 ? 8 : 20;
    int msgStoreTimePos = 4 + 4 + 4 + 4 + 4 + 8 + 8 + 4 + 8 + bornhostLength;
    // 如果文件中第一条消息的存储时间等于0，等于0表示该文件中未存储消息
    long storeTimestamp = byteBuffer.getLong(msgStoreTimePos);
    if (0 == storeTimestamp) {
        return false;
    }

    // 如果启用了消息索引，那么判断文件中第一条消息的存储时间是否小于等于存储检查点的最小索引时间戳 
    if (this.defaultMessageStore.getMessageStoreConfig().isMessageIndexEnable()
        &amp;&amp; this.defaultMessageStore.getMessageStoreConfig().isMessageIndexSafe()) {
        if (storeTimestamp &lt;= this.defaultMessageStore.getStoreCheckpoint().getMinTimestampIndex()) {
            log.info(&quot;find check timestamp, {} {}&quot;,
                storeTimestamp,
                UtilAll.timeMillisToHumanString(storeTimestamp));
            return true;
        }
    } else {
        // 否则判断文件中第一条消息的存储时间是否小于等于存储检查点的最小时间戳 
        if (storeTimestamp &lt;= this.defaultMessageStore.getStoreCheckpoint().getMinTimestamp()) {
            log.info(&quot;find check timestamp, {} {}&quot;,
                storeTimestamp,
                UtilAll.timeMillisToHumanString(storeTimestamp));
            return true;
        }
    }

    return false;
}

</code></pre>
<h3 id="文件刷盘机制">文件刷盘机制</h3>
<p>RocketMQ的读写是基于JDK NIO的内存映射机制（MappedByteBuffer），消息存储时首先将消息追加至内存，再根据配置的刷盘策略在不同时间进行刷盘。</p>
<ul>
<li>同步刷盘（SYNC_FLUSH）：消息追加至内存后，将同步调用MappedByteBuffer的force函数进行刷盘操作。</li>
<li>异步刷盘（ASYNC_FLUSH）：在消息追加至内存后将立刻返回给消息发送端，rocketmq后端会起一个独立线程按照配置的频率进行刷盘。</li>
</ul>
<h4 id="同步刷盘">同步刷盘</h4>
<p>同步刷盘主要是构建GroupCommitRequest 参数提交至GroupCommitService 进行处理，源码如下：</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.CommitLog#submitFlushRequest
if (FlushDiskType.SYNC_FLUSH == this.defaultMessageStore.getMessageStoreConfig().getFlushDiskType()) {
    // 设置GroupCommitService 
    final GroupCommitService service = (GroupCommitService) this.flushCommitLogService;
    if (messageExt.isWaitStoreMsgOK()) {
        // 构建GroupCommitRequest 对象
        GroupCommitRequest request = new GroupCommitRequest(result.getWroteOffset() + result.getWroteBytes(),
                this.defaultMessageStore.getMessageStoreConfig().getSyncFlushTimeout());
         // 将请求提交至刷盘监听器   
        flushDiskWatcher.add(request);
        service.putRequest(request);
        // 等待返回结果
        return request.future();
    } else {
        // 唤醒GroupCommitService 
        service.wakeup();
        return CompletableFuture.completedFuture(PutMessageStatus.PUT_OK);
    }
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.CommitLog#GroupCommitService

public void run() {
    CommitLog.log.info(this.getServiceName() + &quot; service started&quot;);

    // 每处理一批请求后会休眠10秒 
    while (!this.isStopped()) {
        try {
            this.waitForRunning(10);
            // 核心处理函数
            this.doCommit();
        } catch (Exception e) {
            CommitLog.log.warn(this.getServiceName() + &quot; service has exception. &quot;, e);
        }
    }

    // 正常情况下宕机，会等待请求的到来，并进行刷盘操作
    try {
        Thread.sleep(10);
    } catch (InterruptedException e) {
        CommitLog.log.warn(this.getServiceName() + &quot; Exception, &quot;, e);
    }
    synchronized (this) {
        this.swapRequests();
    }
    this.doCommit();

    CommitLog.log.info(this.getServiceName() + &quot; service end&quot;);
}

private void doCommit() {
    if (!this.requestsRead.isEmpty()) {
        // 遍历请求，依次进行刷盘操作
        for (GroupCommitRequest req : this.requestsRead) {
            // There may be a message in the next file, so a maximum of
            // two times the flush
            boolean flushOK = CommitLog.this.mappedFileQueue.getFlushedWhere() &gt;= req.getNextOffset();
            for (int i = 0; i &lt; 2 &amp;&amp; !flushOK; i++) {
                // 实际上是调用了MappedByteBuffer.force函数进行了刷盘操作
                // 
                CommitLog.this.mappedFileQueue.flush(0);
                flushOK = CommitLog.this.mappedFileQueue.getFlushedWhere() &gt;= req.getNextOffset();
            }
            // 唤醒线程通知发送客户端刷盘结果--&gt;执行flushOKFuture.complete函数
            req.wakeupCustomer(flushOK ? PutMessageStatus.PUT_OK : PutMessageStatus.FLUSH_DISK_TIMEOUT);
        }
    
        // 刷盘完成后，需要更新存储检查点的时间戳。此处仅在内存中更新了该属性，并未落地到磁盘
        long storeTimestamp = CommitLog.this.mappedFileQueue.getStoreTimestamp();
        if (storeTimestamp &gt; 0) {
            CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp);
        }

        this.requestsRead = new LinkedList&lt;&gt;();
    } else {
        // Because of individual messages is set to not sync flush, it
        // will come to this process
        CommitLog.this.mappedFileQueue.flush(0);
    }
}

</code></pre>
<h4 id="异步刷盘">异步刷盘</h4>
<p>异步刷盘根据是否开启了transientStorePoolEnable机制，刷盘实现会有些许区别。</p>
<p>如果transientStorePoolEnable为true，rocketmq会单独申请一块与目标物理文件（commitLog）同样大小的堆外内存，该堆外内存会使用内存锁定，确保不会被置换到虚拟内存中，消息首先提交至堆外内存，然后提交至物理文件对应的映射内存中，最后再flush到磁盘。</p>
<p>如果transientStorePoolEnable为false，消息是直接追加到内存映射文件中，然后flush到磁盘。</p>
<p>当transientStorePoolEnable为true时，消息刷盘流程如下所示：</p>
<ol>
<li>首先将消息追加至ByteBuffer（堆外内存DirectByteBuffer），writePosition随着消息不断向后移动。</li>
<li>CommitRealTimeService 线程默认每200ms将ByteBuffer新追加的内容数据提交至MappedFileBuffer中。</li>
<li>FlushRealTimeService 线程默认每500ms调用MappedByteBuffer#force函数将新追加的内存数据刷写到磁盘。</li>
</ol>
<pre><code>// 文件地址：org.apache.rocketmq.store.CommitLog#submitFlushRequest
// Asynchronous flush
else {
    if (!this.defaultMessageStore.getMessageStoreConfig().isTransientStorePoolEnable()) {
        // 唤醒FlushRealTimeService 线程
        flushCommitLogService.wakeup();
    } else  {
        // 唤醒CommitRealTimeService 线程
        commitLogService.wakeup();
    }
    // 异步刷盘时直接返回ok
    return CompletableFuture.completedFuture(PutMessageStatus.PUT_OK);
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.CommitLog#CommitRealTimeService
public void run() {
    CommitLog.log.info(this.getServiceName() + &quot; service started&quot;);
    while (!this.isStopped()) {
        // 获取配置的提交间隔时间，默认200ms
        int interval = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitIntervalCommitLog();
        // 一次提交任务中最少待提交页数，默认4，如果小于该值，将忽略本次提交
        int commitDataLeastPages = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitCommitLogLeastPages();
         // 两次提交任务间隔，默认200ms
        int commitDataThoroughInterval =
            CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitCommitLogThoroughInterval();

        // 如果距上次提交时间超过 commitDataThoroughInterval，那么忽略commitDataLeastPages 参数
        long begin = System.currentTimeMillis();
        if (begin &gt;= (this.lastCommitTimestamp + commitDataThoroughInterval)) {
            this.lastCommitTimestamp = begin;
            commitDataLeastPages = 0;
        }

        try {
            // 提交数据值映射内存中，提交操作返回false，不代表提交失败，而是只提交了部分数据
            boolean result = CommitLog.this.mappedFileQueue.commit(commitDataLeastPages);
            long end = System.currentTimeMillis();
            if (!result) {
                this.lastCommitTimestamp = end; // result = false means some data committed.
                //now wake up flush thread.
                // 唤醒FlushRealTimeService 线程，进行刷盘操作
                flushCommitLogService.wakeup();
            }

            if (end - begin &gt; 500) {
                log.info(&quot;Commit data to file costs {} ms&quot;, end - begin);
            }
            this.waitForRunning(interval);
        } catch (Throwable e) {
            CommitLog.log.error(this.getServiceName() + &quot; service has exception. &quot;, e);
        }
    }
    // 针对commit失败的情况进行重试提交
    boolean result = false;
    for (int i = 0; i &lt; RETRY_TIMES_OVER &amp;&amp; !result; i++) {
        result = CommitLog.this.mappedFileQueue.commit(0);
        CommitLog.log.info(this.getServiceName() + &quot; service shutdown, retry &quot; + (i + 1) + &quot; times &quot; + (result ? &quot;OK&quot; : &quot;Not OK&quot;));
    }
    CommitLog.log.info(this.getServiceName() + &quot; service end&quot;);
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.CommitLog#FlushRealTimeService
public void run() {
    CommitLog.log.info(this.getServiceName() + &quot; service started&quot;);

    while (!this.isStopped()) {
        // 是否定时刷新，默认true
        boolean flushCommitLogTimed = CommitLog.this.defaultMessageStore.getMessageStoreConfig().isFlushCommitLogTimed();
        // 刷盘间隔，默认500ms
        int interval = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushIntervalCommitLog();
        // 一次刷盘任务中最少待刷盘页数，默认4，如果小于该值，将忽略本次刷盘
        int flushPhysicQueueLeastPages = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushCommitLogLeastPages();
        // 两次刷盘任务间隔，默认10s 
        int flushPhysicQueueThoroughInterval =
            CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushCommitLogThoroughInterval();
        // 是否打印刷盘过程信息，默认false 
        boolean printFlushProgress = false;

        // 如果距上次刷盘时间超过 flushPhysicQueueThoroughInterval，那么忽略flushPhysicQueueThoroughInterval参数
        // Print flush progress
        long currentTimeMillis = System.currentTimeMillis();
        if (currentTimeMillis &gt;= (this.lastFlushTimestamp + flushPhysicQueueThoroughInterval)) {
            this.lastFlushTimestamp = currentTimeMillis;
            flushPhysicQueueLeastPages = 0;
            printFlushProgress = (printTimes++ % 10) == 0;
        }

        try {
            // 延迟刷盘
            if (flushCommitLogTimed) {
                Thread.sleep(interval);
            } else {
                this.waitForRunning(interval);
            }
            // 打印刷盘过程信息
            if (printFlushProgress) {
                this.printFlushProgress();
            }

            // 数据刷盘，刷盘操作返回false，不代表fush失败，而是只flush了部分数据
            // 刷盘操作过程还会更新flushedPosition指针
            long begin = System.currentTimeMillis();
            CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages);
            long storeTimestamp = CommitLog.this.mappedFileQueue.getStoreTimestamp();
            if (storeTimestamp &gt; 0) {
                CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp);
            }
            long past = System.currentTimeMillis() - begin;
            if (past &gt; 500) {
                log.info(&quot;Flush data to disk costs {} ms&quot;, past);
            }
        } catch (Throwable e) {
            CommitLog.log.warn(this.getServiceName() + &quot; service has exception. &quot;, e);
            this.printFlushProgress();
        }
    }

    // 针对flush失败的情况进行重试刷盘
    // Normal shutdown, to ensure that all the flush before exit
    boolean result = false;
    for (int i = 0; i &lt; RETRY_TIMES_OVER &amp;&amp; !result; i++) {
        result = CommitLog.this.mappedFileQueue.flush(0);
        CommitLog.log.info(this.getServiceName() + &quot; service shutdown, retry &quot; + (i + 1) + &quot; times &quot; + (result ? &quot;OK&quot; : &quot;Not OK&quot;));
    }

    this.printFlushProgress();

    CommitLog.log.info(this.getServiceName() + &quot; service end&quot;);
}

</code></pre>
<h3 id="过期文件删除机制">过期文件删除机制</h3>
<p>为了避免内存（内存映射机制）与磁盘资源的浪费，rocketmq中的文件不可能永远存储在消息服务器上，这时便需要一种文件过期清除策略来删除已过期的文件。</p>
<p>RocketMQ顺序写文件，也就是说所有的写操作都将落在最后一个文件，之前的文件在下一个文件创建后将不会再被更新。这样便可以引入一种过期策略：如果非当前写文件在一定时间间隔内没有再被更新，则认为该文件过期，可被删除。</p>
<p>RocketMQ不会关注文件上的消息是否已被全部消费，当抵达指定的过期时间时（默认72小时，可以通过在broker配置文件中设置fileReservedTime来更改，单位：小时）便会执行清除操作。</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore#addScheduleTask
this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
    @Override
    public void run() {
        DefaultMessageStore.this.cleanFilesPeriodically();
    }
   // 默认每隔10秒执行一次清除任务，可以设置cleanResourceInterval来控制执行频率
}, 1000 * 60, this.messageStoreConfig.getCleanResourceInterval(), TimeUnit.MILLISECONDS);

private void cleanFilesPeriodically() {
    // 清除commitLog文件
    this.cleanCommitLogService.run();
    // 清除consumerQueue文件
    this.cleanConsumeQueueService.run();
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore#CleanCommitLogService
// 清除commitLog文件
public void run() {
    try {
        this.deleteExpiredFiles();

        this.redeleteHangedFile();
    } catch (Throwable e) {
        DefaultMessageStore.log.warn(this.getServiceName() + &quot; service has exception. &quot;, e);
    }
}

private void deleteExpiredFiles() {
    int deleteCount = 0;
    // 文件保留时间，即最后一次更新到现在的时间，如果超过了该时间，则认为是过期文件，可以被删除
    // 默认72小时，可以通过在broker配置文件中设置fileReservedTime来更改，单位：小时
    long fileReservedTime = DefaultMessageStore.this.getMessageStoreConfig().getFileReservedTime();
    // 两次文件删除间隔，默认100ms；在一次清除过程中，可能需要删除的文件不止一个，该属性指定了两次删除文件的间隔
    int deletePhysicFilesInterval = DefaultMessageStore.this.getMessageStoreConfig().getDeleteCommitLogFilesInterval();
    // 在清除文件过程中，如果该文件被其他线程所占用（引用次数大于0，例如读消息），此时会阻止本次删除操作，
    // 同时在第一次试图删除该文件时记录当前时间戳，destroyMapedFileIntervalForcibly 表示第一次拒绝删除后能保留的最大时间，默认120s
    // 在此时间内，删除操作将会被拒绝，同时引用次数减1000，超过该时间后，文件会被强制删除
    int destroyMapedFileIntervalForcibly = DefaultMessageStore.this.getMessageStoreConfig().getDestroyMapedFileIntervalForcibly();
  
    // 指定删除文件的时间点，可以通过设置deleteWhen来指定在一天的哪个时间点进行删除，默认凌晨4点
    boolean timeup = this.isTimeToDelete();
    // 磁盘空间是否充足，如果不足则应该执行删除操作
    boolean spacefull = this.isSpaceToDelete();
    // 手动删除
    boolean manualDelete = this.manualDeleteFileSeveralTimes &gt; 0;

    if (timeup || spacefull || manualDelete) {

        if (manualDelete)
            this.manualDeleteFileSeveralTimes--;

        boolean cleanAtOnce = DefaultMessageStore.this.getMessageStoreConfig().isCleanFileForciblyEnable() &amp;&amp; this.cleanImmediately;

        log.info(&quot;begin to delete before {} hours file. timeup: {} spacefull: {} manualDeleteFileSeveralTimes: {} cleanAtOnce: {}&quot;,
            fileReservedTime,
            timeup,
            spacefull,
            manualDeleteFileSeveralTimes,
            cleanAtOnce);

        fileReservedTime *= 60 * 60 * 1000;
        // 删除文件
        deleteCount = DefaultMessageStore.this.commitLog.deleteExpiredFile(fileReservedTime, deletePhysicFilesInterval,
            destroyMapedFileIntervalForcibly, cleanAtOnce);
        if (deleteCount &gt; 0) {
        } else if (spacefull) {
            log.warn(&quot;disk space will be full soon, but delete file failed.&quot;);
        }
    }
}
// 针对挂起的文件，重新删除
private void redeleteHangedFile() {
    int interval = DefaultMessageStore.this.getMessageStoreConfig().getRedeleteHangedFileInterval();
    long currentTimestamp = System.currentTimeMillis();
    if ((currentTimestamp - this.lastRedeleteTimestamp) &gt; interval) {
        this.lastRedeleteTimestamp = currentTimestamp;
        int destroyMapedFileIntervalForcibly =
            DefaultMessageStore.this.getMessageStoreConfig().getDestroyMapedFileIntervalForcibly();
        if (DefaultMessageStore.this.commitLog.retryDeleteFirstFile(destroyMapedFileIntervalForcibly)) {
        }
    }
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore#CleanConsumeQueueService
// 清除consumerQueue文件
public void run() {
    try {
        this.deleteExpiredFiles();
    } catch (Throwable e) {
        DefaultMessageStore.log.warn(this.getServiceName() + &quot; service has exception. &quot;, e);
    }
}

private void deleteExpiredFiles() {
    // 两次文件删除间隔，默认100ms
    int deleteLogicsFilesInterval = DefaultMessageStore.this.getMessageStoreConfig().getDeleteConsumeQueueFilesInterval();
    // 获取最小偏移量
    long minOffset = DefaultMessageStore.this.commitLog.getMinOffset();
    // 当前最小偏移量大于上次最小偏移量才进行删除操纵，说明存在需要删除的内容
    if (minOffset &gt; this.lastPhysicalMinOffset) {
        // 重新设置lastPhysicalMinOffset 
        this.lastPhysicalMinOffset = minOffset;

        ConcurrentMap&lt;String, ConcurrentMap&lt;Integer, ConsumeQueue&gt;&gt; tables = DefaultMessageStore.this.consumeQueueTable;

        for (ConcurrentMap&lt;Integer, ConsumeQueue&gt; maps : tables.values()) {
            for (ConsumeQueue logic : maps.values()) {
                // 删除minOffset之前的所有ConsumeQueue 文件
                int deleteCount = logic.deleteExpiredFile(minOffset);

                if (deleteCount &gt; 0 &amp;&amp; deleteLogicsFilesInterval &gt; 0) {
                    try {
                        Thread.sleep(deleteLogicsFilesInterval);
                    } catch (InterruptedException ignored) {
                    }
                }
            }
        }
        // 删除minOffset之前的所有索引文件
        DefaultMessageStore.this.indexService.deleteExpiredFile(minOffset);
    }
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RocketMQ源码解读之消息存储(上)]]></title>
        <id>https://philosopherzb.github.io/post/rocketmq-yuan-ma-jie-du-zhi-xiao-xi-cun-chu-shang/</id>
        <link href="https://philosopherzb.github.io/post/rocketmq-yuan-ma-jie-du-zhi-xiao-xi-cun-chu-shang/">
        </link>
        <updated>2022-10-15T03:19:15.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述Rocketmq源码之消息存储上篇。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/bow-lake-5854210_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="消息存储">消息存储</h2>
<h3 id="简介">简介</h3>
<p>RocketMQ主要存储文件包括commitLog，consumerQueue，indexFile。发送的所有消息都将存储在commitLog文件中，以确保消息顺序写入磁盘。consumerQueue文件相当于commitLog的索引文件，主要记录对应的消息offset，方便消费者对其进行订阅操作。indexFile则是主要用来进行快速检索。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320007.png" alt="" loading="lazy"></figure>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore
// 消息存储的部分核心属性信息
// 存储配置
private final MessageStoreConfig messageStoreConfig;
// CommitLog
private final CommitLog commitLog;
// 消息队列存储缓存表，按消息主题分组
private final ConcurrentMap&lt;String/* topic */, ConcurrentMap&lt;Integer/* queueId */, ConsumeQueue&gt;&gt; consumeQueueTable;
// 消息队列文件consumerQueue的刷盘服务
private final FlushConsumeQueueService flushConsumeQueueService;
// commitLog 文件清除服务
private final CleanCommitLogService cleanCommitLogService;
// consumerQueue 文件清除服务
private final CleanConsumeQueueService cleanConsumeQueueService;
// 索引文件实现服务
private final IndexService indexService;
// mappedFile分配服务
private final AllocateMappedFileService allocateMappedFileService;
// commitLog 消息分发，可以根据commitLog文件构建对应的consumerQueue及indexFile文件
private final ReputMessageService reputMessageService;
// 存储高可用服务
private final HAService haService;
// 定时消息服务
private final ScheduleMessageService scheduleMessageService;
// 存储状态服务
private final StoreStatsService storeStatsService;
// 消息堆内存缓存
private final TransientStorePool transientStorePool;
// broker状态管理器
private final BrokerStatsManager brokerStatsManager;
// 消息拉取长轮询模式中消息到达时的监听器
private final MessageArrivingListener messageArrivingListener;
// broker配置信息
private final BrokerConfig brokerConfig;
// 文件刷盘检查点
private StoreCheckpoint storeCheckpoint;
// commitLog文件转发请求
private final LinkedList&lt;CommitLogDispatcher&gt; dispatcherList;

</code></pre>
<h3 id="消息发送时存储">消息发送时存储</h3>
<p>以消息发送时持久化操作为例。<br>
核心入口函数：org.apache.rocketmq.store.DefaultMessageStore#putMessage,通过源码可以发现同步消息存储实际上也是调用了异步存储的函数。</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore
public PutMessageResult putMessage(MessageExtBrokerInner msg) {
    return waitForPutResult(asyncPutMessage(msg));
}

// 等待异步写入操作的结果
private PutMessageResult waitForPutResult(CompletableFuture&lt;PutMessageResult&gt; putMessageResultFuture) {
    try {
        // 获取CompletableFuture等待超时时间
        int putMessageTimeout =
                Math.max(this.messageStoreConfig.getSyncFlushTimeout(),
                        this.messageStoreConfig.getSlaveTimeout()) + 5000;
        // 获取异步操作结果            
        return putMessageResultFuture.get(putMessageTimeout, TimeUnit.MILLISECONDS);
    } catch (ExecutionException | InterruptedException e) {
        return new PutMessageResult(PutMessageStatus.UNKNOWN_ERROR, null);
    } catch (TimeoutException e) {
        log.error(&quot;usually it will never timeout, putMessageTimeout is much bigger than slaveTimeout and &quot;
                + &quot;flushTimeout so the result can be got anyway, but in some situations timeout will happen like full gc &quot;
                + &quot;process hangs or other unexpected situations.&quot;);
        // 超时将返回未知错误    
        return new PutMessageResult(PutMessageStatus.UNKNOWN_ERROR, null);
    }
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.DefaultMessageStore
public CompletableFuture&lt;PutMessageResult&gt; asyncPutMessage(MessageExtBrokerInner msg) {
    // 检查存储状态是否可用，可能存在服务不可用及操作系统页缓存繁忙状态
    PutMessageStatus checkStoreStatus = this.checkStoreStatus();
    if (checkStoreStatus != PutMessageStatus.PUT_OK) {
        return CompletableFuture.completedFuture(new PutMessageResult(checkStoreStatus, null));
    }

     // 消息主题长度超过127，或消息内容长度超过2^15-1(即32767)，将不允许写入
    PutMessageStatus msgCheckStatus = this.checkMessage(msg);
    if (msgCheckStatus == PutMessageStatus.MESSAGE_ILLEGAL) {
        return CompletableFuture.completedFuture(new PutMessageResult(msgCheckStatus, null));
    }

    // 检查微消息队列数
    PutMessageStatus lmqMsgCheckStatus = this.checkLmqMessage(msg);
    if (msgCheckStatus == PutMessageStatus.LMQ_CONSUME_QUEUE_NUM_EXCEEDED) {
        return CompletableFuture.completedFuture(new PutMessageResult(lmqMsgCheckStatus, null));
    }


    long beginTime = this.getSystemClock().now();
    // 异步存储消息
    CompletableFuture&lt;PutMessageResult&gt; putResultFuture = this.commitLog.asyncPutMessage(msg);

    // 处理异步结果
    putResultFuture.thenAccept((result) -&gt; {
        long elapsedTime = this.getSystemClock().now() - beginTime;
        if (elapsedTime &gt; 500) {
            log.warn(&quot;putMessage not in lock elapsed time(ms)={}, bodyLength={}&quot;, elapsedTime, msg.getBody().length);
        }
        // 设置消息存储处理的最大时间
        this.storeStatsService.setPutMessageEntireTimeMax(elapsedTime);

        // 失败时，增加失败次数
        if (null == result || !result.isOk()) {
            this.storeStatsService.getPutMessageFailedTimes().add(1);
        }
    });

    return putResultFuture;
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.CommitLog
public CompletableFuture&lt;PutMessageResult&gt; asyncPutMessage(final MessageExtBrokerInner msg) {
    // Set the storage time
    msg.setStoreTimestamp(System.currentTimeMillis());
    // Set the message body BODY CRC (consider the most appropriate setting
    // on the client)
    msg.setBodyCRC(UtilAll.crc32(msg.getBody()));
    // Back to Results
    AppendMessageResult result = null;

    StoreStatsService storeStatsService = this.defaultMessageStore.getStoreStatsService();

    String topic = msg.getTopic();
    // int queueId msg.getQueueId();
    final int tranType = MessageSysFlag.getTransactionValue(msg.getSysFlag());
    if (tranType == MessageSysFlag.TRANSACTION_NOT_TYPE
            || tranType == MessageSysFlag.TRANSACTION_COMMIT_TYPE) {
        // Delay Delivery
        // 延迟消息将发往内部的延迟topic中
        if (msg.getDelayTimeLevel() &gt; 0) {
            if (msg.getDelayTimeLevel() &gt; this.defaultMessageStore.getScheduleMessageService().getMaxDelayLevel()) {
                msg.setDelayTimeLevel(this.defaultMessageStore.getScheduleMessageService().getMaxDelayLevel());
            }
            // 内部延迟主题
            topic = TopicValidator.RMQ_SYS_SCHEDULE_TOPIC;
            // 不同的延迟级别对应不同的consumerQueue
            int queueId = ScheduleMessageService.delayLevel2QueueId(msg.getDelayTimeLevel());

            // Backup real topic, queueId
            MessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_TOPIC, msg.getTopic());
            MessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_QUEUE_ID, String.valueOf(msg.getQueueId()));
            msg.setPropertiesString(MessageDecoder.messageProperties2String(msg.getProperties()));

            msg.setTopic(topic);
            msg.setQueueId(queueId);
        }
    }

    // 设置socket 
    InetSocketAddress bornSocketAddress = (InetSocketAddress) msg.getBornHost();
    if (bornSocketAddress.getAddress() instanceof Inet6Address) {
        msg.setBornHostV6Flag();
    }
    InetSocketAddress storeSocketAddress = (InetSocketAddress) msg.getStoreHost();
    if (storeSocketAddress.getAddress() instanceof Inet6Address) {
        msg.setStoreHostAddressV6Flag();
    }

    // 翻转buffer
    PutMessageThreadLocal putMessageThreadLocal = this.putMessageThreadLocal.get();
    PutMessageResult encodeResult = putMessageThreadLocal.getEncoder().encode(msg);
    if (encodeResult != null) {
        return CompletableFuture.completedFuture(encodeResult);
    }
    msg.setEncodedBuff(putMessageThreadLocal.getEncoder().encoderBuffer);
    PutMessageContext putMessageContext = new PutMessageContext(generateKey(putMessageThreadLocal.getKeyBuilder(), msg));

    long elapsedTimeInLock = 0;
    MappedFile unlockMappedFile = null;

    // 加锁串行写入；根据存储配置有两种锁可供选择：spin or ReentrantLock
    putMessageLock.lock(); 
    try {
        // 获取最后一个mappedFile
        MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile();
        long beginLockTimestamp = this.defaultMessageStore.getSystemClock().now();
        this.beginTimeInLock = beginLockTimestamp;

        // 设置全局存储时间，主要为了确保有序
        msg.setStoreTimestamp(beginLockTimestamp);

        // 如果mappedFile 为空或者已经满了，则新建一个mappedFile 
        if (null == mappedFile || mappedFile.isFull()) {
            mappedFile = this.mappedFileQueue.getLastMappedFile(0); // Mark: NewFile may be cause noise
        }
        // 创建失败时可能是因为磁盘空间不足或权限不够等原因
        if (null == mappedFile) {
            log.error(&quot;create mapped file1 error, topic: &quot; + msg.getTopic() + &quot; clientAddr: &quot; + msg.getBornHostString());
            return CompletableFuture.completedFuture(new PutMessageResult(PutMessageStatus.CREATE_MAPEDFILE_FAILED, null));
        }

        // 在文件末尾追加消息，如果超过了文件配置大小则抛出异常
        // 此处追加消息操作仅仅只是将其放入缓存，并未进行刷盘
        result = mappedFile.appendMessage(msg, this.appendMessageCallback, putMessageContext);
        switch (result.getStatus()) {
            case PUT_OK:
                break;
            // 抵达文件末尾，便新增一个文件进行写入操作  
            case END_OF_FILE:
                unlockMappedFile = mappedFile;
                // Create a new file, re-write the message
                mappedFile = this.mappedFileQueue.getLastMappedFile(0);
                if (null == mappedFile) {
                    // XXX: warn and notify me
                    log.error(&quot;create mapped file2 error, topic: &quot; + msg.getTopic() + &quot; clientAddr: &quot; + msg.getBornHostString());
                    return CompletableFuture.completedFuture(new PutMessageResult(PutMessageStatus.CREATE_MAPEDFILE_FAILED, result));
                }
                result = mappedFile.appendMessage(msg, this.appendMessageCallback, putMessageContext);
                break;
            case MESSAGE_SIZE_EXCEEDED:
            case PROPERTIES_SIZE_EXCEEDED:
                return CompletableFuture.completedFuture(new PutMessageResult(PutMessageStatus.MESSAGE_ILLEGAL, result));
            case UNKNOWN_ERROR:
                return CompletableFuture.completedFuture(new PutMessageResult(PutMessageStatus.UNKNOWN_ERROR, result));
            default:
                return CompletableFuture.completedFuture(new PutMessageResult(PutMessageStatus.UNKNOWN_ERROR, result));
        }

        elapsedTimeInLock = this.defaultMessageStore.getSystemClock().now() - beginLockTimestamp;
    } finally {
        beginTimeInLock = 0;
        putMessageLock.unlock();
    }

    if (elapsedTimeInLock &gt; 500) {
        log.warn(&quot;[NOTIFYME]putMessage in lock cost time(ms)={}, bodyLength={} AppendMessageResult={}&quot;, elapsedTimeInLock, msg.getBody().length, result);
    }

    if (null != unlockMappedFile &amp;&amp; this.defaultMessageStore.getMessageStoreConfig().isWarmMapedFileEnable()) {
        this.defaultMessageStore.unlockMappedFile(unlockMappedFile);
    }

    PutMessageResult putMessageResult = new PutMessageResult(PutMessageStatus.PUT_OK, result);

    // Statistics
    storeStatsService.getSinglePutMessageTopicTimesTotal(msg.getTopic()).add(1);
    storeStatsService.getSinglePutMessageTopicSizeTotal(topic).add(result.getWroteBytes());
  
    // 提交刷盘请求，同步刷盘和异步刷盘
    CompletableFuture&lt;PutMessageStatus&gt; flushResultFuture = submitFlushRequest(result, msg);
    // 数据同步至副本
    CompletableFuture&lt;PutMessageStatus&gt; replicaResultFuture = submitReplicaRequest(result, msg);
    return flushResultFuture.thenCombine(replicaResultFuture, (flushStatus, replicaStatus) -&gt; {
        if (flushStatus != PutMessageStatus.PUT_OK) {
            putMessageResult.setPutMessageStatus(flushStatus);
        }
        if (replicaStatus != PutMessageStatus.PUT_OK) {
            putMessageResult.setPutMessageStatus(replicaStatus);
        }
        return putMessageResult;
    });
}

</code></pre>
<h3 id="内存映射">内存映射</h3>
<p>RocketMQ通过使用内存映射文件来提高IO访问性能，并且使用顺序写磁盘来提高写入效率。RocketMQ中的存储文件都采用了定长设计，每当一个文件达到指定长度后便会新增一个文件进行写操作。</p>
<p>Memory Mapped Files：简称 mmap，也有叫 MMFile 的，使用 mmap 的目的是将内核中读缓冲区（read buffer）的地址与用户空间的缓冲区（user buffer）进行映射。从而实现内核缓冲区与应用程序内存的共享，省去了将数据从内核读缓冲区（read buffer）拷贝到用户缓冲区（user buffer）的过程。它的工作原理是直接利用操作系统的 Page 来实现文件到物理内存的直接映射。完成映射之后用户对物理内存的操作会被同步到硬盘上。</p>
<p>RocketMQ使用MappedFIle 及 MappedFileQueue来管理存储文件，关系如下所示：</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230320008.png" alt="img" loading="lazy"></figure>
<h4 id="mappedfilequeue">MappedFileQueue</h4>
<p>MappedFileQueue(映射文件队列)是MappedFIle 的管理器，实际上是对存储目录的管理，因为一般情况下，commitlog目录下会存在多个MappedFIle 文件。</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.MappedFileQueue
// 核心属性
// 存储目录
private final String storePath;
// 单个MappedFIle 文件的大小
protected final int mappedFileSize;
// MappedFIle 文件集合，使用写时复制容器存储
protected final CopyOnWriteArrayList&lt;MappedFile&gt; mappedFiles = new CopyOnWriteArrayList&lt;MappedFile&gt;();
// 分配MappedFIle 的服务
private final AllocateMappedFileService allocateMappedFileService;
// 当前刷盘指针，表示该指针之前的所有数据都已持久化到磁盘
protected long flushedWhere = 0;
// 当前数据提交指针，内存中ByteBuffer当前的写指针，此值大于等于flushedWhere 
private long committedWhere = 0;
// 存储时间戳
private volatile long storeTimestamp = 0;

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.MappedFileQueue
// 根据消息存储时间戳查询MappedFile
public MappedFile getMappedFileByTime(final long timestamp) {
    Object[] mfs = this.copyMappedFiles(0);

    if (null == mfs)
        return null;

     // 遍历列表，直到存在最后修改时间大于指定时间戳的MappedFile，返回该MappedFile
    for (int i = 0; i &lt; mfs.length; i++) {
        MappedFile mappedFile = (MappedFile) mfs[i];
        if (mappedFile.getLastModifiedTimestamp() &gt;= timestamp) {
            return mappedFile;
        }
    }
    // 如果不存在，那么便返回最后一个MappedFile文件 
    return (MappedFile) mfs[mfs.length - 1];
}

// 通过指定offset获取MappedFile，其中returnFirstOnNotFound参数表示如果未找到指定MappedFile便返回第一个MappedFile文件
public MappedFile findMappedFileByOffset(final long offset, final boolean returnFirstOnNotFound) {
    try {
        // 获取第一个MappedFile 
        MappedFile firstMappedFile = this.getFirstMappedFile();
        // 获取最后一个MappedFile 
        MappedFile lastMappedFile = this.getLastMappedFile();
        if (firstMappedFile != null &amp;&amp; lastMappedFile != null) {
            if (offset &lt; firstMappedFile.getFileFromOffset() || offset &gt;= lastMappedFile.getFileFromOffset() + this.mappedFileSize) {
                LOG_ERROR.warn(&quot;Offset not matched. Request offset: {}, firstOffset: {}, lastOffset: {}, mappedFileSize: {}, mappedFiles count: {}&quot;,
                    offset,
                    firstMappedFile.getFileFromOffset(),
                    lastMappedFile.getFileFromOffset() + this.mappedFileSize,
                    this.mappedFileSize,
                    this.mappedFiles.size());
            } else {
                // 此处不直接使用offset % this.mappedFileSize，是因为内存映射文件可能会被删除
                // 为了防止出现内存压力与资源浪费，rocketmq会定时删除存储文件。
                int index = (int) ((offset / this.mappedFileSize) - (firstMappedFile.getFileFromOffset() / this.mappedFileSize));
                MappedFile targetFile = null;
                try {
                    targetFile = this.mappedFiles.get(index);
                } catch (Exception ignored) {
                }

                if (targetFile != null &amp;&amp; offset &gt;= targetFile.getFileFromOffset()
                    &amp;&amp; offset &lt; targetFile.getFileFromOffset() + this.mappedFileSize) {
                    return targetFile;
                }

                // 如果通过上述算法未找到MappedFile ，则遍历MappedFile 列表，从中找出一个契合offset的MappedFile 文件
                for (MappedFile tmpMappedFile : this.mappedFiles) {
                    if (offset &gt;= tmpMappedFile.getFileFromOffset()
                        &amp;&amp; offset &lt; tmpMappedFile.getFileFromOffset() + this.mappedFileSize) {
                        return tmpMappedFile;
                    }
                }
            }
            // 通过offset未找到MappedFile，那么便返回第一个MappedFile 
            if (returnFirstOnNotFound) {
                return firstMappedFile;
            }
        }
    } catch (Exception e) {
        log.error(&quot;findMappedFileByOffset Exception&quot;, e);
    }

    return null;
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.MappedFileQueue
// 获取最小偏移量
public long getMinOffset() {

    if (!this.mappedFiles.isEmpty()) {
        try {
            // 并非直接返回0，而是返回了fileFromOffset 
            // 其中 this.fileFromOffset = Long.parseLong(this.file.getName());
            return this.mappedFiles.get(0).getFileFromOffset();
        } catch (IndexOutOfBoundsException e) {
            //continue;
        } catch (Exception e) {
            log.error(&quot;getMinOffset has exception.&quot;, e);
        }
    }
    return -1;
}
// 获取最大偏移量
public long getMaxOffset() {
    MappedFile mappedFile = getLastMappedFile();
    if (mappedFile != null) {
        // 返回最后一个文件的fileFromOffset + MappedFile的当前读指针
        return mappedFile.getFileFromOffset() + mappedFile.getReadPosition();
    }
    return 0;
}
// 获取最大写指针
public long getMaxWrotePosition() {
    MappedFile mappedFile = getLastMappedFile();
    if (mappedFile != null) {
        // 返回最后一个文件的fileFromOffset + MappedFile的当前写指针
        return mappedFile.getFileFromOffset() + mappedFile.getWrotePosition();
    }
    return 0;
}

</code></pre>
<h4 id="mappedfile">MappedFile</h4>
<p>MappedFile是rocketmq内存映射文件的具体实现。</p>
<pre><code>// 文件地址：org.apache.rocketmq.store.MappedFile
// 核心属性
// 操作系统每页的大小，默认4k
public static final int OS_PAGE_SIZE = 1024 * 4;
// 当前JVM中MappedFile虚拟内存
private static final AtomicLong TOTAL_MAPPED_VIRTUAL_MEMORY = new AtomicLong(0);
// 当前JVM中MappedFile对象个数
private static final AtomicInteger TOTAL_MAPPED_FILES = new AtomicInteger(0);
// 写指针，从0开始
protected final AtomicInteger wrotePosition = new AtomicInteger(0);
// 提交指针
protected final AtomicInteger committedPosition = new AtomicInteger(0);
// 刷盘指针，该指针之前的数据都已持久化至磁盘
private final AtomicInteger flushedPosition = new AtomicInteger(0);
// 文件大小
protected int fileSize;
// 文件通道
protected FileChannel fileChannel;
/**
 * Message will put to here first, and then reput to FileChannel if writeBuffer is not null.
 * 堆内存ByteBuffer，如果不为空，数据会先存在该缓存中
 */
protected ByteBuffer writeBuffer = null;
// 堆内存池
protected TransientStorePool transientStorePool = null;
private String fileName;
// 文件起始偏移量
private long fileFromOffset;
private File file;
// 物理文件对应的内存映射buffer
private MappedByteBuffer mappedByteBuffer;
// 存储时间戳，一般是文件最后一次写入时间
private volatile long storeTimestamp = 0;
// 是否为MappedFileQueue中的第一个文件
private boolean firstCreateInQueue = false;

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.MappedFile
// 初始化MappedFile
// 其中transientStorePool表示一个短暂的存储池，主要提供了内存锁定的功能，防止其被置换至磁盘，用于提高存储性能
public void init(final String fileName, final int fileSize,
    final TransientStorePool transientStorePool) throws IOException {
    init(fileName, fileSize);
    // 如果transientStorePoolEnable为true，那么writeBuffer 使用堆外内存。
    // 之后commit线程会将其内容提交至内存映射buffer，最后再进行刷盘操作
    this.writeBuffer = transientStorePool.borrowBuffer();
    this.transientStorePool = transientStorePool;
}

private void init(final String fileName, final int fileSize) throws IOException {
    this.fileName = fileName;
    this.fileSize = fileSize;
    this.file = new File(fileName);
    // 初始化文件名为该文件的起始偏移量
    this.fileFromOffset = Long.parseLong(this.file.getName());
    boolean ok = false;

    ensureDirOK(this.file.getParent());

    try {
        // 使用RandomAccessFile创建文件读写通道
        this.fileChannel = new RandomAccessFile(this.file, &quot;rw&quot;).getChannel();
        // 将文件内容映射至内存中---&gt;内存映射机制
        this.mappedByteBuffer = this.fileChannel.map(MapMode.READ_WRITE, 0, fileSize);
        TOTAL_MAPPED_VIRTUAL_MEMORY.addAndGet(fileSize);
        TOTAL_MAPPED_FILES.incrementAndGet();
        ok = true;
    } catch (FileNotFoundException e) {
        log.error(&quot;Failed to create file &quot; + this.fileName, e);
        throw e;
    } catch (IOException e) {
        log.error(&quot;Failed to map file &quot; + this.fileName, e);
        throw e;
    } finally {
        if (!ok &amp;&amp; this.fileChannel != null) {
            this.fileChannel.close();
        }
    }
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.MappedFile
// MappedFile 提交
// commitLeastPages为本次提交的最小页数
public int commit(final int commitLeastPages) {
    // writeBuffer 为空不会进行提交操作
    if (writeBuffer == null) {
        //no need to commit data to file channel, so just regard wrotePosition as committedPosition.
        return this.wrotePosition.get();
    }
  
    // 判断commitLeastPages是否符合提交要求
    if (this.isAbleToCommit(commitLeastPages)) {
        if (this.hold()) {
            // 提交数据
            commit0();
            this.release();
        } else {
            log.warn(&quot;in commit, hold failed, commit offset = &quot; + this.committedPosition.get());
        }
    }

    // All dirty data has been committed to FileChannel.
    if (writeBuffer != null &amp;&amp; this.transientStorePool != null &amp;&amp; this.fileSize == this.committedPosition.get()) {
        this.transientStorePool.returnBuffer(writeBuffer);
        this.writeBuffer = null;
    }
    // 返回已提交指针
    return this.committedPosition.get();
}

// 判断是否可提交
protected boolean isAbleToCommit(final int commitLeastPages) {
    // 获取已提交指针
    int flush = this.committedPosition.get();
    // 获取当前写指针
    int write = this.wrotePosition.get();
  
    // 页数已满的情况下，直接返回可提交操作
    if (this.isFull()) {
        return true;
    }
    // 如果commitLeastPages 大于0，将当前写指针与已提交指针的差值与commitLeastPages进行比较。其中除以OS_PAGE_SIZE得到的是脏页数
    if (commitLeastPages &gt; 0) {
        return ((write / OS_PAGE_SIZE) - (flush / OS_PAGE_SIZE)) &gt;= commitLeastPages;
    }
    // 当commitLeastPages小于0时，只要存在脏页便允许提交
    return write &gt; flush;
}

// 将writeBuffer中的数据写入fileChannel
protected void commit0() {
    // 获取写指针
    int writePos = this.wrotePosition.get();
    // 获取上次提交指针
    int lastCommittedPosition = this.committedPosition.get();
    // 存在可提交数据
    if (writePos - lastCommittedPosition &gt; 0) {
        try {
            // 创建writeBuffer共享区，slice()方法与原有的ByteBuffer共享内存，但维护了一套独立的指针(position，mark，limit)
            ByteBuffer byteBuffer = writeBuffer.slice();
            // 设置pos为上次提交指针
            byteBuffer.position(lastCommittedPosition);
            // 设置最大有效pos为当前写指针
            byteBuffer.limit(writePos);
            // 设置pos为上次提交指针
            this.fileChannel.position(lastCommittedPosition);
            // 将上次提交pos与本次写pos之间的数据写入fileChannel
            this.fileChannel.write(byteBuffer);
            // 更新已提交指针为当前写指针
            this.committedPosition.set(writePos);
        } catch (Throwable e) {
            log.error(&quot;Error occurred when commit data to FileChannel.&quot;, e);
        }
    }
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.MappedFile
// MappedFile 刷盘，将数据持久化到磁盘
public int flush(final int flushLeastPages) {
    // 判断flushLeastPages是否符合提交要求
    if (this.isAbleToFlush(flushLeastPages)) {
        if (this.hold()) {
            // 获取当前读指针
            int value = getReadPosition();

            try {
                //We only append data to fileChannel or mappedByteBuffer, never both.
                if (writeBuffer != null || this.fileChannel.position() != 0) {
                    // 追加数据至fileChannel，并持久化至磁盘
                    this.fileChannel.force(false);
                } else {
                    // 追加数据至mappedByteBuffer，并持久化至磁盘
                    this.mappedByteBuffer.force();
                }
            } catch (Throwable e) {
                log.error(&quot;Error occurred when force data to disk.&quot;, e);
            }
            // 设置刷盘指针为当前文件最大可读指针
            this.flushedPosition.set(value);
            this.release();
        } else {
            log.warn(&quot;in flush, hold failed, flush offset = &quot; + this.flushedPosition.get());
            this.flushedPosition.set(getReadPosition());
        }
    }
    return this.getFlushedPosition();
}

private boolean isAbleToFlush(final int flushLeastPages) {
    // 获取已刷盘指针
    int flush = this.flushedPosition.get();
    // 获取当前读指针
    int write = getReadPosition();
    // 页数已满的情况下，直接返回可刷盘操作
    if (this.isFull()) {
        return true;
    }
    // 如果flushLeastPages 大于0，将当前读指针与已刷盘指针的差值与flushLeastPages进行比较。其中除以OS_PAGE_SIZE得到的是脏页数
    if (flushLeastPages &gt; 0) {
        return ((write / OS_PAGE_SIZE) - (flush / OS_PAGE_SIZE)) &gt;= flushLeastPages;
    }
    // 当flushLeastPages 小于0时，只要存在脏页便允许刷盘
    return write &gt; flush;
}

// 获取当前文件最大可读指针
public int getReadPosition() {
    // 如果writeBuffer为空，直接返回当前写指针，否则返回上次已提交指针
    return this.writeBuffer == null ? this.wrotePosition.get() : this.committedPosition.get();
}

</code></pre>
<pre><code>// 文件地址：org.apache.rocketmq.store.MappedFile
// MappedFile 销毁
// intervalForcibly 表示拒绝销毁的最大存活时间
public boolean destroy(final long intervalForcibly) {
    // 释放资源
    this.shutdown(intervalForcibly);
    // 判断资源是否已清理完毕 
    if (this.isCleanupOver()) {
        try {
            // 关闭fileChannel
            this.fileChannel.close();
            log.info(&quot;close file channel &quot; + this.fileName + &quot; OK&quot;);

            long beginTime = System.currentTimeMillis();、
            // 删除物理文件
            boolean result = this.file.delete();
            log.info(&quot;delete file[REF:&quot; + this.getRefCount() + &quot;] &quot; + this.fileName
                + (result ? &quot; OK, &quot; : &quot; Failed, &quot;) + &quot;W:&quot; + this.getWrotePosition() + &quot; M:&quot;
                + this.getFlushedPosition() + &quot;, &quot;
                + UtilAll.computeElapsedTimeMilliseconds(beginTime));
        } catch (Exception e) {
            log.warn(&quot;close file channel &quot; + this.fileName + &quot; Failed. &quot;, e);
        }

        return true;
    } else {
        log.warn(&quot;destroy mapped file[REF:&quot; + this.getRefCount() + &quot;] &quot; + this.fileName
            + &quot; Failed. cleanupOver: &quot; + this.cleanupOver);
    }

    return false;
}

public void shutdown(final long intervalForcibly) {
    // 判断是否可用，available初始值为true
    if (this.available) {
        this.available = false;
        // 设置首次关闭时间戳
        this.firstShutdownTimestamp = System.currentTimeMillis();
        // 释放资源，只有引用次数小于1的情况下才会执行释放操作
        this.release();
    } else if (this.getRefCount() &gt; 0) {
        // 非首次销毁，判断当前时间与firstShutdownTimestamp的差值是否大于intervalForcibly
        if ((System.currentTimeMillis() - this.firstShutdownTimestamp) &gt;= intervalForcibly) {
            // 将引用次数减少1000
            this.refCount.set(-1000 - this.getRefCount());
            // 释放资源，只有引用次数小于1的情况下才会执行释放操作
            this.release();
        }
    }
}

public void release() {
    // 引用次数减1
    long value = this.refCount.decrementAndGet();
    // 大于0的情况下，不进行释放操作
    if (value &gt; 0)
        return;

    synchronized (this) {
        // 清理资源
        this.cleanupOver = this.cleanup(value);
    }
}

public boolean cleanup(final long currentRef) {
    // 如果available为true，表明当前MappedFile可用，无需清理
    if (this.isAvailable()) {
        log.error(&quot;this file[REF:&quot; + currentRef + &quot;] &quot; + this.fileName
            + &quot; have not shutdown, stop unmapping.&quot;);
        return false;
    }
    // 如果已经清理完毕，则直接返回true 
    if (this.isCleanupOver()) {
        log.error(&quot;this file[REF:&quot; + currentRef + &quot;] &quot; + this.fileName
            + &quot; have cleanup, do not do it again.&quot;);
        return true;
    }
    // 清理mappedByteBuffer
    clean(this.mappedByteBuffer);
    // 维护TOTAL_MAPPED_VIRTUAL_MEMORY及TOTAL_MAPPED_FILES值
    TOTAL_MAPPED_VIRTUAL_MEMORY.addAndGet(this.fileSize * (-1));
    TOTAL_MAPPED_FILES.decrementAndGet();
    log.info(&quot;unmap file[REF:&quot; + currentRef + &quot;] &quot; + this.fileName + &quot; OK&quot;);
    return true;
}

// 清理完成的判断：是引用次数已经归零且cleanupOver为true
public boolean isCleanupOver() {
    return this.refCount.get() &lt;= 0 &amp;&amp; this.cleanupOver;
}

</code></pre>
]]></content>
    </entry>
</feed>