<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://philosopherzb.github.io</id>
    <title>Philosopher</title>
    <updated>2023-03-16T08:18:30.049Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://philosopherzb.github.io"/>
    <link rel="self" href="https://philosopherzb.github.io/atom.xml"/>
    <subtitle>WORLD AS CODE</subtitle>
    <logo>https://philosopherzb.github.io/images/avatar.png</logo>
    <icon>https://philosopherzb.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, Philosopher</rights>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之消费者分配器]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-xiao-fei-zhe-fen-pei-qi/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-xiao-fei-zhe-fen-pei-qi/">
        </link>
        <updated>2022-07-23T08:00:58.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka 消费者，消费组以及分区分配器。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/forest-3287976_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="消费者">消费者</h2>
<h3 id="简介">简介</h3>
<p>kafka consumer 订阅感兴趣的主题，同时向其所在的主分区发送fetch请求，获取需要进行消费的消息；需要注意的是，此过程中，consumer的每个请求都需要在partition中指定offset，从而消费从offset处开始的message。因此，consumer对offset的控制便尤为重要，可以依此来进行回退重消费操作。</p>
<h3 id="消费组">消费组</h3>
<p>kafka中，每个consumer都会对应一个消费组（Consumer Group）；不同消费组可以消费同一个Topic中同一个分区的消息，但在一个消费组下，一个Topic中的一个或多个分区消息只能被一个消费者所消费。也就是说一个分区如果已跟一个消费者匹配，那么便不能再跟同一个消费组中的消费者继续匹配。</p>
<p>在kafka集群中，一般会以GroupId（字符串）区分不同的消费组。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316006.png" alt="img" loading="lazy"></figure>
<p>kafka中，消费者跟topic中的分区匹配模式，可以提供横向扩展功能，例如额外增加消费者来提高消费速率；不过需要注意的是，一味地增加消费者并不会带来高额收益。因为当分区数固定的前提下，如果消费组中的消费者大于分区数将会导致部分消费者处于空闲状态。如下图所示，C7便处于空闲状态。</p>
<p>当然，这种情况是可以通过继承kafka的AbstractPartitionAssignor类（或者直接实现ConsumerPartitionAssignor接口）来规避的，只要将其中的分配逻辑修改为同一消费组中的消费者可以任意消费topic的所有分区即可。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316007.png" alt="img" loading="lazy"></figure>
<h3 id="分区分配器">分区分配器</h3>
<p>分配器（Assignor）是ConsumerPartitionAssignor接口（也可直接继承AbstractPartitionAssignor类）的实现类。leader（消费者之一）使用分配器来根据消费者的订阅将分区分配给对应的Consumer实例。</p>
<p>分配器是消费者逻辑处理的一部分，这种设计的好处是可以动态的更改分配算法，而无需修改kafka broker。</p>
<p>assignor 获取有关消费者订阅的数据并为每个消费者返回具体的分区，具体流程如下：</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316008.png" alt="img" loading="lazy"></figure>
<p>kafka（2.4版本及之后）拥有四种可直接使用的分配器（可以直接通过属性 partition.assignment.strategy 来进行分配器配置）：</p>
<ul>
<li>RangeAssignor：<a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/RangeAssignor.html">https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/RangeAssignor.html</a></li>
<li>RoundRobinAssignor：<a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/RoundRobinAssignor.html">https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/RoundRobinAssignor.html</a></li>
<li>StickyAssignor：<a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/StickyAssignor.html">https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/StickyAssignor.html</a></li>
<li>CooperativeStickyAssignor：<a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/CooperativeStickyAssignor.html">https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/consumer/CooperativeStickyAssignor.html</a></li>
</ul>
<pre><code>// 简单示例
Properties props = new Properties();
props.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, StickyAssignor.class.getName());
KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);

</code></pre>
<p>同时，需要注意的是：属于同一ConsumerGroup的所有消费者必须声明一个共同的分配器。如果一个consumer尝试加入一个group，且其分配器与组内的其他成员不一致，那么将会出现如下异常：</p>
<pre><code>org.apache.kafka.common.errors.InconsistentGroupProtocolException: The group member’s supported protocols are incompatible with those of existing members or first group member tried to join with empty protocol type or empty protocol list.

</code></pre>
<p>此属性（partition.assignment.strategy）接受以逗号分隔的分配器列表。例如，它允许使用者通过指定新分配器来更新一组消费者，同时暂时保留前一个分配器，作为再平衡协议的一部分，broker协调者将选择所有成员都支持的协议。</p>
<p>kafka本身使用的默认分配器为：RangeAssignor及CooperativeStickyAssignor。</p>
<pre><code>// 文件地址：org.apache.kafka.clients.consumer.ConsumerConfig
.define(PARTITION_ASSIGNMENT_STRATEGY_CONFIG,
        Type.LIST,
        Arrays.asList(RangeAssignor.class, CooperativeStickyAssignor.class),
        new ConfigDef.NonNullValidator(),
        Importance.MEDIUM,
        PARTITION_ASSIGNMENT_STRATEGY_DOC)

</code></pre>
<h4 id="rangeassignor">RangeAssignor</h4>
<p>RangeAssignor 是基于每个topic进行工作的。针对每个Topic，kafka会依据数字顺序将可用分区进行排序(p0,p1,p2...)，同时按照字典顺序排序消费者(c0,c1,c2...)。之后，将分区数除以消费者总数，以确定分配给每个消费者的分区数。如果无法整除，那么额外的分区将会分配给排序靠前的消费者。</p>
<p>数学模型：设主题分区数为x，消费者数为y，设分区数除以消费者数的商为n、余数为m。那么在一个消费组中，前m个消费者将分配到n+1个分区，剩余的消费者分配n个分区。</p>
<p>例如：存在两个消费者C0，C1，两个主题t0，t1，且每个主题拥有三个分区，那么分区结果为：t0p0，t0p1，t0p2，t1p0，t1p1，t1p2。最终分配结果如下所示：</p>
<pre><code>// 依据上述数学模型，此处x=3，y=2，算出n=1，m=1，最终结果如下所示：
C0: [t0p0, t0p1, t1p0, t1p1]
C1: [t0p2, t1p2]

</code></pre>
<p>使用这种分配器的优势在于每个消费者都可以为每个主题获得一个分区，这对于某些负载均衡场景很有利。当然劣势也很明显，比如当遇到某个消费组订阅了多个Topic，那么可能会导致前几个消费者负载过重；又或者消费者数多于分区数时，将会有部分消费者处于空闲状态；且针对消费者进行扩缩容时，分配可能会发生较大的变化。</p>
<pre><code>// 源码内容如下
public class RangeAssignor extends AbstractPartitionAssignor {
    public static final String RANGE_ASSIGNOR_NAME = &quot;range&quot;;

    @Override
    public String name() {
        return RANGE_ASSIGNOR_NAME;
    }

    private Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic(Map&lt;String, Subscription&gt; consumerMetadata) {
        Map&lt;String, List&lt;MemberInfo&gt;&gt; topicToConsumers = new HashMap&lt;&gt;();
        for (Map.Entry&lt;String, Subscription&gt; subscriptionEntry : consumerMetadata.entrySet()) {
            String consumerId = subscriptionEntry.getKey();
            MemberInfo memberInfo = new MemberInfo(consumerId, subscriptionEntry.getValue().groupInstanceId());
            for (String topic : subscriptionEntry.getValue().topics()) {
                // 以topic为key，为每个topic归总所有的消费者信息
                put(topicToConsumers, topic, memberInfo);
            }
        }
        return topicToConsumers;
    }

    @Override
    public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,
                                                    Map&lt;String, Subscription&gt; subscriptions) {
        // 获取每个topic的消费者信息，形如：t0:[c0,c1], t1:[c0,c1]                                         
        Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions);

        // 将主题分区与订阅该主题的消费者绑定
        Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;();
        for (String memberId : subscriptions.keySet())
            // memberId 与消费者关联
            assignment.put(memberId, new ArrayList&lt;&gt;());

        // 对每个主题分区及消费者信息组成的map进行遍历，由此可以看出 RangeAssignor 分配器是针对每个主题进行分配操作的。
        for (Map.Entry&lt;String, List&lt;MemberInfo&gt;&gt; topicEntry : consumersPerTopic.entrySet()) {
            String topic = topicEntry.getKey();
            List&lt;MemberInfo&gt; consumersForTopic = topicEntry.getValue();

            // 获取当前topic的分区数
            Integer numPartitionsForTopic = partitionsPerTopic.get(topic);
            if (numPartitionsForTopic == null)
                continue;

            // 将消费者按字典排序 
            Collections.sort(consumersForTopic);
            // 主题分区数除以消费者数得到分配给每个消费者的分区数
            int numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();
            // 主题分区数对消费者数取余得到额外的消费者分区数
            int consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();

            // 将主题分区汇总为一个列表 
            List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);
            // 此处逻辑可参考上述数学模型
            for (int i = 0, n = consumersForTopic.size(); i &lt; n; i++) {
                // 截取主题分区列表的起始位置
                int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);
                // 截取主题分区列表的长度，如果有consumersWithExtraPartition不为0，那么排序靠前的消费者将会额外分配一个分区
                int length = numPartitionsPerConsumer + (i + 1 &gt; consumersWithExtraPartition ? 0 : 1);
                // 给消费者分配分区数
                assignment.get(consumersForTopic.get(i).memberId).addAll(partitions.subList(start, start + length));
            }
        }
        return assignment;
    }
}

</code></pre>
<h4 id="roundrobinassignor">RoundRobinAssignor</h4>
<p>RoundRobinAssignor 会将消费组内的所有消费者及其订阅的所有主题的分区按照字典序排序，然后通过轮询方式逐个将分区依次分配给每个消费者。步骤如下：</p>
<ol>
<li>消费者按照字典排序，例如C0, C1, C2... ...，并构造环形迭代器。</li>
<li>topic名称按照字典排序，并得到每个topic的所有分区，从而得到所有分区集合。</li>
<li>遍历第2步所有分区集合，同时轮询消费者。</li>
<li>如果轮询到的消费者订阅的topic不包括当前遍历的分区所属topic，则跳过；否则分配给当前消费者，并继续第3步。</li>
</ol>
<p>按照上述步骤来说，当同一个消费组内所有的消费者的订阅信息都是相同的，那么使用 RoundRobinAssignor 分配器进行分配操作时，结果会相对均匀。</p>
<p>但当同一个消费组内的消费者订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能导致分区分配得不均匀。</p>
<p>例1：存在两个消费者C0，C1，两个主题t0，t1，且每个主题拥有三个分区，那么分区结果为：t0p0，t0p1，t0p2，t1p0，t1p1，t1p2。最终分配结果如下所示：</p>
<pre><code>C0: [t0p0, t0p2, t1p1]
C1: [t0p1, t1p0, t1p2]
</code></pre>
<p>例2：存在3个消费者C0、C1 和 C2，三个主题t0、t1、t2，且t0分区数为1，t1分区数为2，t2分区数为3，那么分区结果为：t0p0, t1p0，t1p1, t2p0, t2p1, t2p2。最终分配结果将如下所示：</p>
<pre><code>C0: [t0p0]
C1: [t1p0]
C2: [t1p1, t2p0, t2p1, t2p2]
</code></pre>
<p>这种分配器最大化的利用了消费者的数量优势，某种情况下可以达到统一分配的效果；但这种分配器也有一个主要的缺点，那就是在消费者数量发生变化时（重平衡），会导致分区也重分配。这可能会由于分区更改其所有者而导致额外的处理（额外的性能影响）。</p>
<pre><code>public class RoundRobinAssignor extends AbstractPartitionAssignor {

    @Override
    public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,
                                                    Map&lt;String, Subscription&gt; subscriptions) {
        Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;();
        List&lt;MemberInfo&gt; memberInfoList = new ArrayList&lt;&gt;();
        // 取到所有的消费者信息
        for (Map.Entry&lt;String, Subscription&gt; memberSubscription : subscriptions.entrySet()) {
            assignment.put(memberSubscription.getKey(), new ArrayList&lt;&gt;());
            memberInfoList.add(new MemberInfo(memberSubscription.getKey(),
                                              memberSubscription.getValue().groupInstanceId()));
        }

        // 排序消费者列表，同时将其转化为环形迭代器（主要用于循环取值）
        CircularIterator&lt;MemberInfo&gt; assigner = new CircularIterator&lt;&gt;(Utils.sorted(memberInfoList));

        // 遍历排序后的分区列表
        for (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) {
            final String topic = partition.topic();
            // 遍历环形迭代器获取消费者信息，并判断其是否订阅相关的topic，如果没有则跳过，否则分配分区给该消费者
            while (!subscriptions.get(assigner.peek().memberId).topics().contains(topic))
                assigner.next();
            // 分配分区给该消费者
            assignment.get(assigner.next().memberId).add(partition);
        }
        return assignment;
    }

    // 将所有的分区汇总并进行排序操作
    private List&lt;TopicPartition&gt; allPartitionsSorted(Map&lt;String, Integer&gt; partitionsPerTopic,
                                                     Map&lt;String, Subscription&gt; subscriptions) {
        SortedSet&lt;String&gt; topics = new TreeSet&lt;&gt;();
        for (Subscription subscription : subscriptions.values())
            topics.addAll(subscription.topics());

        List&lt;TopicPartition&gt; allPartitions = new ArrayList&lt;&gt;();
        for (String topic : topics) {
            Integer numPartitionsForTopic = partitionsPerTopic.get(topic);
            if (numPartitionsForTopic != null)
                // 分区中包含topic信息，用于匹配对应的消费者
                allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic));
        }
        return allPartitions;
    }

    @Override
    public String name() {
        return &quot;roundrobin&quot;;
    }

}

</code></pre>
<h4 id="stickyassignor">StickyAssignor</h4>
<p>StickyAssignor 分配器除了让分区分配得更加均匀外，在重平衡期间还会尽可能的减少分区移动（如果两者发生冲突，第一种特性优先于第二种特性）。相对于RoundRobinAssignor 重平衡期间需要重分配分区的操作而言，此特性减少了资源消耗，提升了消费者性能。</p>
<p>相对于前两种分配器，StickyAssignor 实现更为复杂，简单例子如下：</p>
<p>例1：订阅的主题拥有相同分区的场景：假设有3个Consumer（C0、C1和C2），他们都订阅了4个主题（t0、t1、t2、t3），每个主题有2个分区。也就是说，整个消费者组订阅了 t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1 这 8 个 分区。最终分发结果如下：</p>
<pre><code>C0：t0p0、t1p1、t3p0
C1：t0p1、t2p0、t3p1
C2：t1p0、t2p1

</code></pre>
<p>乍一看，似乎与采用 RoundRobinAssignor 分配策略分配的结果是一样的，但真的是这样吗？我们假设消费者 C1 退出了消费者组，那么消费者组会执行再平衡操作，再分配消费区。如果是 RoundRobinAssignor 分配策略，那么分配结果如下：</p>
<pre><code>// 分区发生了移动
C0：t0p0、t1p0、t2p0、t3p0
C2：t0p1、t1p1、t2p1、t3p1
</code></pre>
<p>正如分配结果所示，RoundRobinAssignor 的分配策略将基于消费者 C0 和 C2 重新轮询分配。如果你使用的是 StickyAssignor 分配策略，那么分配结果是：</p>
<pre><code>// 上次分区结果将会被保留，以尽量减少分区移动
C0：t0p0、t1p1、t3p0、t2p0
C2：t1p0、t2p1、t0p1、t3p1
</code></pre>
<p>例2：订阅的主题拥有不同分区的场景：同样有3个Consumer（C0、C1和C2），集群中有3个主题（t0、t1和t2），这3个主题分别有1、2、3个分区。也就是说，集群中有 t0p0、t1p0、t1p1、t2p0、t2p1、t2p2 这 6 个 分区。消费者 C0 订阅了主题 t0，消费者 C1 订阅了主题 t0 和 t1，消费者 C2 订阅了主题 t0、t1 和 t2。RoundRobinAssignor 分配器结果如下：</p>
<pre><code>// 显然结果不并不是最优解
C0：t0p0
C1：t1p0
C2：t1p1、t2p0、t2p1、t2p2

</code></pre>
<p>StickyAssignor 分配器结果如下：</p>
<pre><code>// 可以看出此种分配方式更充分地利用了消费者。
C0：t0p0
C1：t1p0、t1p1
C2：t2p0、t2p1、t2p2

</code></pre>
<p>此时，如果C0从消费组中移除，那么依照 RoundRobinAssignor 分配器的重平衡结果如下：</p>
<pre><code>// 分区重新开始轮询分配
C1：t0p0、t1p1
C2：t1p0、t2p0、t2p1、t2p2

</code></pre>
<p>StickyAssignor 分配器的重平衡结果如下：</p>
<pre><code>// 原有分区结果被保留
C1：t1p0、t1p1、t0p0
C2：t2p0、t2p1、t2p2

</code></pre>
<p>这种分配器最小化了消费者之间的分区移动，节省了额外的开销处理，且分配结果会尽量平衡。不足之处在于经过多次迭代后，某个消费者可能会拥有其所订阅主题的所有分区，即使最终结果仍然平衡。这将导致某个消费者必须完成所有的消费工作（负载不再均衡）。</p>
<pre><code>public class StickyAssignor extends AbstractStickyAssignor {

    // these schemas are used for preserving consumer's previously assigned partitions
    // list and sending it as user data to the leader during a rebalance
    static final String TOPIC_PARTITIONS_KEY_NAME = &quot;previous_assignment&quot;;
    static final String TOPIC_KEY_NAME = &quot;topic&quot;;
    static final String PARTITIONS_KEY_NAME = &quot;partitions&quot;;
    private static final String GENERATION_KEY_NAME = &quot;generation&quot;;

    static final Schema TOPIC_ASSIGNMENT = new Schema(
        new Field(TOPIC_KEY_NAME, Type.STRING),
        new Field(PARTITIONS_KEY_NAME, new ArrayOf(Type.INT32)));
    static final Schema STICKY_ASSIGNOR_USER_DATA_V0 = new Schema(
        new Field(TOPIC_PARTITIONS_KEY_NAME, new ArrayOf(TOPIC_ASSIGNMENT)));
    private static final Schema STICKY_ASSIGNOR_USER_DATA_V1 = new Schema(
        new Field(TOPIC_PARTITIONS_KEY_NAME, new ArrayOf(TOPIC_ASSIGNMENT)),
        new Field(GENERATION_KEY_NAME, Type.INT32));

    private List&lt;TopicPartition&gt; memberAssignment = null;
    private int generation = DEFAULT_GENERATION; // consumer group generation

    @Override
    public String name() {
        return &quot;sticky&quot;;
    }

    @Override
    public void onAssignment(Assignment assignment, ConsumerGroupMetadata metadata) {
        memberAssignment = assignment.partitions();
        this.generation = metadata.generationId();
    }

    @Override
    public ByteBuffer subscriptionUserData(Set&lt;String&gt; topics) {
        if (memberAssignment == null)
            return null;

        return serializeTopicPartitionAssignment(new MemberData(memberAssignment, Optional.of(generation)));
    }

    @Override
    protected MemberData memberData(Subscription subscription) {
        ByteBuffer userData = subscription.userData();
        if (userData == null || !userData.hasRemaining()) {
            return new MemberData(Collections.emptyList(), Optional.empty());
        }
        return deserializeTopicPartitionAssignment(userData);
    }

    // visible for testing
    static ByteBuffer serializeTopicPartitionAssignment(MemberData memberData) {
        Struct struct = new Struct(STICKY_ASSIGNOR_USER_DATA_V1);
        List&lt;Struct&gt; topicAssignments = new ArrayList&lt;&gt;();
        // 获取每个主题的分区并分组
        for (Map.Entry&lt;String, List&lt;Integer&gt;&gt; topicEntry : CollectionUtils.groupPartitionsByTopic(memberData.partitions).entrySet()) {
            Struct topicAssignment = new Struct(TOPIC_ASSIGNMENT);
            topicAssignment.set(TOPIC_KEY_NAME, topicEntry.getKey());
            topicAssignment.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());
            topicAssignments.add(topicAssignment);
        }
        struct.set(TOPIC_PARTITIONS_KEY_NAME, topicAssignments.toArray());
        if (memberData.generation.isPresent())
            struct.set(GENERATION_KEY_NAME, memberData.generation.get());
        ByteBuffer buffer = ByteBuffer.allocate(STICKY_ASSIGNOR_USER_DATA_V1.sizeOf(struct));
        STICKY_ASSIGNOR_USER_DATA_V1.write(buffer, struct);
        buffer.flip();
        return buffer;
    }

    private static MemberData deserializeTopicPartitionAssignment(ByteBuffer buffer) {
        Struct struct;
        ByteBuffer copy = buffer.duplicate();
        try {
            struct = STICKY_ASSIGNOR_USER_DATA_V1.read(buffer);
        } catch (Exception e1) {
            try {
                // fall back to older schema
                struct = STICKY_ASSIGNOR_USER_DATA_V0.read(copy);
            } catch (Exception e2) {
                // ignore the consumer's previous assignment if it cannot be parsed
                return new MemberData(Collections.emptyList(), Optional.of(DEFAULT_GENERATION));
            }
        }

        List&lt;TopicPartition&gt; partitions = new ArrayList&lt;&gt;();
        for (Object structObj : struct.getArray(TOPIC_PARTITIONS_KEY_NAME)) {
            Struct assignment = (Struct) structObj;
            String topic = assignment.getString(TOPIC_KEY_NAME);
            for (Object partitionObj : assignment.getArray(PARTITIONS_KEY_NAME)) {
                Integer partition = (Integer) partitionObj;
                partitions.add(new TopicPartition(topic, partition));
            }
        }
        // make sure this is backward compatible
        Optional&lt;Integer&gt; generation = struct.hasField(GENERATION_KEY_NAME) ? Optional.of(struct.getInt(GENERATION_KEY_NAME)) : Optional.empty();
        return new MemberData(partitions, generation);
    }
}

</code></pre>
<h4 id="cooperativestickyassignor">CooperativeStickyAssignor</h4>
<p>CooperativeStickyAssignor 逻辑基本与 StickyAssignor 一致，不过在重平衡期间此分配器采用了一种全新的协议：RebalanceProtocol.COOPERATIVE：在这个协议中，每个消费者不会在加入重新平衡之前撤销所有分区，而是尽可能地保留它们并且仅在被告知可以撤销之后才会这样做。</p>
<p>其优劣势与StickyAssignor基本一致，但重平衡更加轻量化，并且不会阻止消费者从分区中消费。</p>
<pre><code>public class CooperativeStickyAssignor extends AbstractStickyAssignor {
    public static final String COOPERATIVE_STICKY_ASSIGNOR_NAME = &quot;cooperative-sticky&quot;;

    // these schemas are used for preserving useful metadata for the assignment, such as the last stable generation
    private static final String GENERATION_KEY_NAME = &quot;generation&quot;;
    private static final Schema COOPERATIVE_STICKY_ASSIGNOR_USER_DATA_V0 = new Schema(
        new Field(GENERATION_KEY_NAME, Type.INT32));

    private int generation = DEFAULT_GENERATION; // consumer group generation

    @Override
    public String name() {
        return COOPERATIVE_STICKY_ASSIGNOR_NAME;
    }

    // 使用 COOPERATIVE 协议进行重平衡操作
    @Override
    public List&lt;RebalanceProtocol&gt; supportedProtocols() {
        return Arrays.asList(RebalanceProtocol.COOPERATIVE, RebalanceProtocol.EAGER);
    }

    @Override
    public void onAssignment(Assignment assignment, ConsumerGroupMetadata metadata) {
        this.generation = metadata.generationId();
    }

    @Override
    public ByteBuffer subscriptionUserData(Set&lt;String&gt; topics) {
        Struct struct = new Struct(COOPERATIVE_STICKY_ASSIGNOR_USER_DATA_V0);

        struct.set(GENERATION_KEY_NAME, generation);
        ByteBuffer buffer = ByteBuffer.allocate(COOPERATIVE_STICKY_ASSIGNOR_USER_DATA_V0.sizeOf(struct));
        COOPERATIVE_STICKY_ASSIGNOR_USER_DATA_V0.write(buffer, struct);
        buffer.flip();
        return buffer;
    }

    @Override
    protected MemberData memberData(Subscription subscription) {
        ByteBuffer buffer = subscription.userData();
        Optional&lt;Integer&gt; encodedGeneration;
        if (buffer == null) {
            encodedGeneration = Optional.empty();
        } else {
            try {
                Struct struct = COOPERATIVE_STICKY_ASSIGNOR_USER_DATA_V0.read(buffer);
                encodedGeneration = Optional.of(struct.getInt(GENERATION_KEY_NAME));
            } catch (Exception e) {
                encodedGeneration = Optional.of(DEFAULT_GENERATION);
            }
        }
        return new MemberData(subscription.ownedPartitions(), encodedGeneration);
    }

    @Override
    public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,
                                                    Map&lt;String, Subscription&gt; subscriptions) {
        Map&lt;String, List&lt;TopicPartition&gt;&gt; assignments = super.assign(partitionsPerTopic, subscriptions);

        Map&lt;TopicPartition, String&gt; partitionsTransferringOwnership = super.partitionsTransferringOwnership == null ?
            computePartitionsTransferringOwnership(subscriptions, assignments) :
            super.partitionsTransferringOwnership;

        // 调整主题分区并返回
        adjustAssignment(assignments, partitionsTransferringOwnership);
        return assignments;
    }

    // Following the cooperative rebalancing protocol requires removing partitions that must first be revoked from the assignment
    // 协作重平衡协议的前提是分区已经从分配中废除。
    private void adjustAssignment(Map&lt;String, List&lt;TopicPartition&gt;&gt; assignments,
                                  Map&lt;TopicPartition, String&gt; partitionsTransferringOwnership) {
        for (Map.Entry&lt;TopicPartition, String&gt; partitionEntry : partitionsTransferringOwnership.entrySet()) {
            assignments.get(partitionEntry.getValue()).remove(partitionEntry.getKey());
        }
    }

    private Map&lt;TopicPartition, String&gt; computePartitionsTransferringOwnership(Map&lt;String, Subscription&gt; subscriptions,
                                                                               Map&lt;String, List&lt;TopicPartition&gt;&gt; assignments) {
        Map&lt;TopicPartition, String&gt; allAddedPartitions = new HashMap&lt;&gt;();
        Set&lt;TopicPartition&gt; allRevokedPartitions = new HashSet&lt;&gt;();

        for (final Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt; entry : assignments.entrySet()) {
            String consumer = entry.getKey();

            List&lt;TopicPartition&gt; ownedPartitions = subscriptions.get(consumer).ownedPartitions();
            List&lt;TopicPartition&gt; assignedPartitions = entry.getValue();

            Set&lt;TopicPartition&gt; ownedPartitionsSet = new HashSet&lt;&gt;(ownedPartitions);
            for (TopicPartition tp : assignedPartitions) {
                if (!ownedPartitionsSet.contains(tp))
                    allAddedPartitions.put(tp, consumer);
            }

            Set&lt;TopicPartition&gt; assignedPartitionsSet = new HashSet&lt;&gt;(assignedPartitions);
            for (TopicPartition tp : ownedPartitions) {
                if (!assignedPartitionsSet.contains(tp))
                    allRevokedPartitions.add(tp);
            }
        }

        allAddedPartitions.keySet().retainAll(allRevokedPartitions);
        return allAddedPartitions;
    }
}

</code></pre>
<h3 id="定制化分区分配器">定制化分区分配器</h3>
<p>除了kafka提供的几种分配器之外，还可以通过继承kafka的AbstractPartitionAssignor类或者直接实现ConsumerPartitionAssignor接口来自定义分区分配器。</p>
<pre><code>public interface ConsumerPartitionAssignor {
     // 所有消费者都将调用此函数，此函数会将订阅的主题发送给broker coordinator
    default ByteBuffer subscriptionUserData(Set&lt;String&gt; topics) {
        return null;
    }
    // 消费组leader收到消费者的订阅信息，并开始进行分区分配
    GroupAssignment assign(Cluster metadata, GroupSubscription groupSubscription);

    // 当消费者收到leader的分配消息时会执行此回调函数
    default void onAssignment(Assignment assignment, ConsumerGroupMetadata metadata) {
    }
  
    // 重平衡协议
    default List&lt;RebalanceProtocol&gt; supportedProtocols() {
        return Collections.singletonList(RebalanceProtocol.EAGER);
    }

    // 分配器版本号
    default short version() {
        return (short) 0;
    }
  
    // 分配器的唯一名称，非必传 (e.g. &quot;range&quot; or &quot;roundrobin&quot; or &quot;sticky&quot;). 
    String name();
}  

</code></pre>
<h4 id="failoverassignor">FailoverAssignor</h4>
<p>FailoverAssignor 主要针对一些需要故障转移的场景，其中的基本逻辑是：让多个消费者加入同一个组。但是，所有分区一次都分配给一个消费者。如果该消费者shutdown，则所有的分区将分配给下一个可用的消费者。通常，分区会被分配给第一个可用的消费者。此例子将会对消费者进行排序。</p>
<pre><code>import org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor;
import org.apache.kafka.common.TopicPartition;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * @author philosopherZB
 */
public class FailoverAssignor extends AbstractPartitionAssignor {

    @Override
    public String name() {
        return &quot;failover&quot;;
    }

    @Override
    public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) {
        Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions);

        Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;();
        for (String memberId : subscriptions.keySet()) {
            assignment.put(memberId, new ArrayList&lt;&gt;());
        }

        for (Map.Entry&lt;String, List&lt;MemberInfo&gt;&gt; topicEntry : consumersPerTopic.entrySet()) {
            String topic = topicEntry.getKey();
            List&lt;MemberInfo&gt; consumersForTopic = topicEntry.getValue();

            Integer numPartitionsForTopic = partitionsPerTopic.get(topic);
            if (numPartitionsForTopic == null) {
                continue;
            }

            // 按字典排序
            Collections.sort(consumersForTopic);

            List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);
            // 没有消费者则跳过
            if (consumersForTopic.isEmpty()) {
                continue;
            }
            // 取排序列表中的第一个消费者
            String consumerMemberId = consumersForTopic.stream().findFirst().get().memberId;
            assignment.get(consumerMemberId).addAll(partitions);
        }
        return assignment;
    }

    private Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic(Map&lt;String, Subscription&gt; consumerMetadata) {
        Map&lt;String, List&lt;MemberInfo&gt;&gt; topicToConsumers = new HashMap&lt;&gt;();
        for (Map.Entry&lt;String, Subscription&gt; subscriptionEntry : consumerMetadata.entrySet()) {
            String consumerId = subscriptionEntry.getKey();
            MemberInfo memberInfo = new MemberInfo(consumerId, subscriptionEntry.getValue().groupInstanceId());
            for (String topic : subscriptionEntry.getValue().topics()) {
                put(topicToConsumers, topic, memberInfo);
            }
        }
        return topicToConsumers;
    }
}

</code></pre>
<pre><code>// 使用
Properties props = new Properties();
...
props.put(
    ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG,   
    FailoverAssignor.class.getName()
);

KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);

</code></pre>
<h4 id="randomassignor">RandomAssignor</h4>
<p>RandomAssignor 主要针对的是消费者数大于分区数的场景。采用随机分配可以让所有的消费者都参与消费动作，而不是处于空闲状态。</p>
<pre><code>import org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor;
import org.apache.kafka.common.TopicPartition;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ThreadLocalRandom;

/**
 * @author philosopherZB
 */
public class RandomAssignor extends AbstractPartitionAssignor {

    @Override
    public String name() {
        return &quot;random&quot;;
    }

    @Override
    public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) {
        Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions);

        Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;();
        for (String memberId : subscriptions.keySet()) {
            assignment.put(memberId, new ArrayList&lt;&gt;());
        }

        for (Map.Entry&lt;String, List&lt;MemberInfo&gt;&gt; topicEntry : consumersPerTopic.entrySet()) {
            String topic = topicEntry.getKey();
            List&lt;MemberInfo&gt; consumersForTopic = topicEntry.getValue();
            int consumerSize = consumersForTopic.size();

            Integer numPartitionsForTopic = partitionsPerTopic.get(topic);
            if (numPartitionsForTopic == null) {
                continue;
            }

            // 可不排序
            Collections.sort(consumersForTopic);

            List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);
            partitions.forEach(partition -&gt; {
                String randomConsumer = consumersForTopic.get(ThreadLocalRandom.current().nextInt(consumerSize)).memberId;
                assignment.get(randomConsumer).add(partition);
            });
        }
        return assignment;
    }

    private Map&lt;String, List&lt;MemberInfo&gt;&gt; consumersPerTopic(Map&lt;String, Subscription&gt; consumerMetadata) {
        Map&lt;String, List&lt;MemberInfo&gt;&gt; topicToConsumers = new HashMap&lt;&gt;();
        for (Map.Entry&lt;String, Subscription&gt; subscriptionEntry : consumerMetadata.entrySet()) {
            String consumerId = subscriptionEntry.getKey();
            MemberInfo memberInfo = new MemberInfo(consumerId, subscriptionEntry.getValue().groupInstanceId());
            for (String topic : subscriptionEntry.getValue().topics()) {
                put(topicToConsumers, topic, memberInfo);
            }
        }
        return topicToConsumers;
    }
}

</code></pre>
<pre><code>// 使用
Properties props = new Properties();
...
props.put(
    ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG,   
    RandomAssignor.class.getName()
);

KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之Broker控制器]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-kong-zhi-qi/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-kong-zhi-qi/">
        </link>
        <updated>2022-07-09T07:18:09.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka broker控制器。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/neuschwanstein-701732_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="broker控制器">Broker控制器</h2>
<h3 id="简介">简介</h3>
<p>在 Kafka 集群中会有一个或多个 broker，其中有一个 broker 会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态以及执行管理任务，例如主题管理，重分配分区，broker管理，数据管理等。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316005.png" alt="img" loading="lazy"></figure>
<h3 id="控制器选举与failover">控制器选举与Failover</h3>
<p>Broker 在启动时，会尝试去 ZooKeeper 中创建 /controller 节点。第一个创建/controller节点成功的broker将会成为控制器。根据源码来看具体流程如下：</p>
<p>获取 zk 的 /cotroller 节点中的 controllerId，如果该节点不存在（比如集群刚创建时），那么controllerId 将会返回-1；</p>
<ol>
<li>
<p>如果 controller id 不为-1，即 controller 已经存在，直接结束流程；</p>
</li>
<li>
<p>如果 controller id 为-1，证明 controller 还不存在，这时候当前 broker 开始在 zk 注册 controller同时递增epoch值；</p>
</li>
<li>
<p>如果注册成功，那么当前 broker 就成为了 controller，这时候开始调用 onControllerFailover() 方法，正式初始化 controller（注意：controller 节点是临时节点，如果当前 controller 与 zk 的 session 断开，那么 controller 的临时节点会消失，会触发 controller 的重新选举）；</p>
</li>
<li>
<p>如果注册失败（刚好 controller 被其他 broker 创建了、抛出异常等），那么直接返回。</p>
</li>
<li>
<p>Zookeeper leader elector 在选举当前broker作为新控制器时调用onControllerFailover函数。</p>
<p>它在成为控制器状态更改时执行以下操作 -</p>
<ol>
<li>初始化控制器的上下文对象，该对象包含当前主题的缓存对象、存活的broker及所有已存在分区的leaders</li>
<li>启动控制器的channel管理器，建立与其他 broker 的连接的,负责与其他 broker 之间的通信;</li>
<li>启动replicaStateMachine，副本状态机,管理副本的状态</li>
<li>启动partitionStateMachine，分区状态机,管理分区的状态</li>
</ol>
<p>如果在 Controller 服务初始化的过程中，出现了任何不可预期的 异常/错误，它将会退出当前的进程，这确保了可以再次触发 controller 的选举。</p>
</li>
</ol>
<p>关于epoch值：其是存放在ZooKeeper中 /controller_epoch 节点上的一个整型值。controller_epoch 用于记录控制器发生变更的次数，即记录当前的控制器是第几代控制器，直译可以称之为“控制器的纪元”。</p>
<p>controller_epoch 的初始值为1，即集群中第一个控制器的纪元为1，当控制器发生变更时，每选出一个新的控制器就将该字段值加1。Kafka 通过 controller_epoch 来保证控制器的唯一性，进而保证相关操作的一致性。</p>
<p>每个和控制器交互的请求都会携带 controller_epoch 这个字段，如果请求的 controller_epoch 值小于内存中的 controller_epoch 值，则认为这个请求是向已经过期的控制器所发送的请求，那么这个请求会被认定为无效的请求。</p>
<p>如果请求的 controller_epoch 值大于内存中的 controller_epoch 值，那么说明已经有新的控制器当选了。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaServer.scala
override def startup(): Unit = {
    info(&quot;starting&quot;)
    if (isShuttingDown.get)
      throw new IllegalStateException(&quot;Kafka server is still shutting down, cannot re-start!&quot;)

    if (startupComplete.get)
      return

   val canStartup = isStartingUp.compareAndSet(false, true)
   if (canStartup) {
       /* start kafka controller --- 启动 kafka 控制器 */
       _kafkaController = new KafkaController(config, zkClient, time, metrics, brokerInfo, brokerEpoch, tokenManager, brokerFeatures, featureCache, threadNamePrefix)
       kafkaController.startup()
    }
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/controller/KafkaController.scala
/**
 * Invoked when the controller module of a Kafka server is started up. This does not assume that the current broker
 * is the controller. It merely registers the session expiration listener and starts the controller leader
 * elector
 * kafka服务的控制器模块启动时调用此函数。调用此函数的broker并不一定是最终的controller（取决于是否选举成功）。
 * 此函数仅仅只是注册一个会话过期监听器并开始进行controller选举。
 */
def startup() = {
  zkClient.registerStateChangeHandler(new StateChangeHandler {
    override val name: String = StateChangeHandlers.ControllerHandler
    override def afterInitializingSession(): Unit = {
      eventManager.put(RegisterBrokerAndReelect)
    }
    override def beforeInitializingSession(): Unit = {
      val queuedEvent = eventManager.clearAndPut(Expire)

      // Block initialization of the new session until the expiration event is being handled,
      // which ensures that all pending events have been processed before creating the new session
      // 在过期事件被处理掉之前，不允许初始化新会话。这确保了所有的待处理事件在新会话之前能够得到处理。
      queuedEvent.awaitProcessing()
    }
  })
  // 事件状态为启动
  // 文件地址：core/src/main/scala/kafka/controller/ControllerEventManager.scala
  // ControllerEventManager 中的 ControllerEventProcessor 会被 KafkaController 继承，同时重写了其中的process函数
  // process 便是线程具体执行的逻辑函数
  eventManager.put(Startup)
  // 启动事件线程开始进行controller选举
  eventManager.start()
}

// 重写 ControllerEventProcessor 的process函数
override def process(event: ControllerEvent): Unit = {
  try {
    event match {
      ......
      case BrokerChange =&gt;
        // broker 发生改变时，执行此函数。主要用于监听zk节点变化。
        processBrokerChange()
      case Startup =&gt;
        // 执行启动操作
        processStartup()
    }
  } catch {
    case e: ControllerMovedException =&gt;
      info(s&quot;Controller moved to another broker when processing $event.&quot;, e)
      maybeResign()
    case e: Throwable =&gt;
      error(s&quot;Error processing event $event&quot;, e)
  } finally {
    updateMetrics()
  }
}

// 执行启动函数
private def processStartup(): Unit = {
  //  注册zk节点改变处理器，与此同时也会注册一个ExistsRequest观察者
  zkClient.registerZNodeChangeHandlerAndCheckExistence(controllerChangeHandler)
  // 选举controller
  elect()
}

// 文件地址：core/src/main/scala/kafka/zk/KafkaZkClient.scala
def registerZNodeChangeHandlerAndCheckExistence(zNodeChangeHandler: ZNodeChangeHandler): Boolean = {
  zooKeeperClient.registerZNodeChangeHandler(zNodeChangeHandler)
  val existsResponse = retryRequestUntilConnected(ExistsRequest(zNodeChangeHandler.path))
  existsResponse.resultCode match {
    case Code.OK =&gt; true
    case Code.NONODE =&gt; false
    case _ =&gt; throw existsResponse.resultException.get
  }
}

// 选举函数
private def elect(): Unit = {
  // 获取活跃的controllerId  
  activeControllerId = zkClient.getControllerId.getOrElse(-1)
  /*
   * We can get here during the initial startup and the handleDeleted ZK callback. Because of the potential race condition,
   * it's possible that the controller has already been elected when we get here. This check will prevent the following
   * createEphemeralPath method from getting into an infinite loop if this broker is already the controller.
   */
  // 如果存在controller，那么直接返回，不进行选举操作 
  if (activeControllerId != -1) {
    debug(s&quot;Broker $activeControllerId has been elected as the controller, so stopping the election process.&quot;)
    return
  }

  try {
    // 注册controller节点并递增epoch值  
    val (epoch, epochZkVersion) = zkClient.registerControllerAndIncrementControllerEpoch(config.brokerId)
    controllerContext.epoch = epoch
    controllerContext.epochZkVersion = epochZkVersion
    activeControllerId = config.brokerId

    info(s&quot;${config.brokerId} successfully elected as the controller. Epoch incremented to ${controllerContext.epoch} &quot; +
      s&quot;and epoch zk version is now ${controllerContext.epochZkVersion}&quot;)
     // 执行controller快速恢复
    // Zookeeper leader elector 在选举当前broker作为新控制器时调用此callback。
    // 它在成为控制器状态更改时执行以下操作 -
    // 1. 初始化控制器的上下文对象，该对象包含当前主题的缓存对象、存活的broker及所有已存在分区的leaders
    // 2. 启动控制器的channel管理器，建立与其他 broker 的连接的,负责与其他 broker 之间的通信;
    // 3. 启动replicaStateMachine，副本状态机,管理副本的状态
    // 4. 启动partitionStateMachine，分区状态机,管理分区的状态
    // 如果在 Controller 服务初始化的过程中，出现了任何不可预期的 异常/错误，它将会退出当前的进程，这确保了可以再次触发 controller 的选举
    onControllerFailover()
  } catch {
    case e: ControllerMovedException =&gt;
      maybeResign()

      if (activeControllerId != -1)
        debug(s&quot;Broker $activeControllerId was elected as controller instead of broker ${config.brokerId}&quot;, e)
      else
        warn(&quot;A controller has been elected but just resigned, this will result in another round of election&quot;, e)
    case t: Throwable =&gt;
      error(s&quot;Error while electing or becoming controller on broker ${config.brokerId}. &quot; +
        s&quot;Trigger controller movement immediately&quot;, t)
      triggerControllerMove()
  }
}

// 快速恢复
/**
 * This callback is invoked by the zookeeper leader elector on electing the current broker as the new controller.
 * It does the following things on the become-controller state change -
 * 1. Initializes the controller's context object that holds cache objects for current topics, live brokers and
 *    leaders for all existing partitions.
 * 2. Starts the controller's channel manager
 * 3. Starts the replica state machine
 * 4. Starts the partition state machine
 * If it encounters any unexpected exception/error while becoming controller, it resigns as the current controller.
 * This ensures another controller election will be triggered and there will always be an actively serving controller
 */
private def onControllerFailover(): Unit = {
  maybeSetupFeatureVersioning()

  info(&quot;Registering handlers&quot;)

  // before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks
  val childChangeHandlers = Seq(brokerChangeHandler, topicChangeHandler, topicDeletionHandler, logDirEventNotificationHandler,
    isrChangeNotificationHandler)
  childChangeHandlers.foreach(zkClient.registerZNodeChildChangeHandler)

  val nodeChangeHandlers = Seq(preferredReplicaElectionHandler, partitionReassignmentHandler)
  nodeChangeHandlers.foreach(zkClient.registerZNodeChangeHandlerAndCheckExistence)

  info(&quot;Deleting log dir event notifications&quot;)
  zkClient.deleteLogDirEventNotifications(controllerContext.epochZkVersion)
  info(&quot;Deleting isr change notifications&quot;)
  zkClient.deleteIsrChangeNotifications(controllerContext.epochZkVersion)
  info(&quot;Initializing controller context&quot;)
  initializeControllerContext()
  info(&quot;Fetching topic deletions in progress&quot;)
  val (topicsToBeDeleted, topicsIneligibleForDeletion) = fetchTopicDeletionsInProgress()
  info(&quot;Initializing topic deletion manager&quot;)
  topicDeletionManager.init(topicsToBeDeleted, topicsIneligibleForDeletion)

  // We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines
  // are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before
  // they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and
  // partitionStateMachine.startup().
  info(&quot;Sending update metadata request&quot;)
  sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, Set.empty)

  replicaStateMachine.startup()
  partitionStateMachine.startup()

  info(s&quot;Ready to serve as the new controller with epoch $epoch&quot;)

  initializePartitionReassignments()
  topicDeletionManager.tryTopicDeletion()
  val pendingPreferredReplicaElections = fetchPendingPreferredReplicaElections()
  onReplicaElection(pendingPreferredReplicaElections, ElectionType.PREFERRED, ZkTriggered)
  info(&quot;Starting the controller scheduler&quot;)
  kafkaScheduler.startup()
  if (config.autoLeaderRebalanceEnable) {
    scheduleAutoLeaderRebalanceTask(delay = 5, unit = TimeUnit.SECONDS)
  }

  if (config.tokenAuthEnabled) {
    info(&quot;starting the token expiry check scheduler&quot;)
    tokenCleanScheduler.startup()
    tokenCleanScheduler.schedule(name = &quot;delete-expired-tokens&quot;,
      fun = () =&gt; tokenManager.expireTokens(),
      period = config.delegationTokenExpiryCheckIntervalMs,
      unit = TimeUnit.MILLISECONDS)
  }
}

</code></pre>
<h3 id="控制器之分区选举">控制器之分区选举</h3>
<p>controller负责分区的重分配。目前主要有四种选举机制。</p>
<pre><code>sealed trait PartitionLeaderElectionStrategy
// leader 掉线或新分区出现时触发
final case class OfflinePartitionLeaderElectionStrategy(allowUnclean: Boolean) extends PartitionLeaderElectionStrategy
// 分区的副本重新分配数据同步完成后触发的
final case object ReassignPartitionLeaderElectionStrategy extends PartitionLeaderElectionStrategy
// 最优 leader 选举，手动触发或自动 leader 均衡调度时触发
final case object PreferredReplicaPartitionLeaderElectionStrategy extends PartitionLeaderElectionStrategy
// controller 发送 ShutDown 请求主动关闭服务时触发
final case object ControlledShutdownPartitionLeaderElectionStrategy extends PartitionLeaderElectionStrategy

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/controller/PartitionStateMachine.scala
/**
 * Invoked on successful controller election.
 * controller 选举成功后调用此函数
 */
def startup(): Unit = {
  info(&quot;Initializing partition state&quot;)
  initializePartitionState()
  info(&quot;Triggering online partition state changes&quot;)
  // 分区函数入口
  triggerOnlinePartitionStateChange()
  debug(s&quot;Started partition state machine with initial state -&gt; ${controllerContext.partitionStates}&quot;)
}

// 选举分区leader
private def doElectLeaderForPartitions(
  partitions: Seq[TopicPartition],
  partitionLeaderElectionStrategy: PartitionLeaderElectionStrategy
): (Map[TopicPartition, Either[Exception, LeaderAndIsr]], Seq[TopicPartition]) = {
    val (partitionsWithoutLeaders, partitionsWithLeaders) = partitionLeaderElectionStrategy match {
      case OfflinePartitionLeaderElectionStrategy(allowUnclean) =&gt;
        val partitionsWithUncleanLeaderElectionState = collectUncleanLeaderElectionState(
          validLeaderAndIsrs,
          allowUnclean
        )
        // 离线/新分区选举
        leaderForOffline(controllerContext, partitionsWithUncleanLeaderElectionState).partition(_.leaderAndIsr.isEmpty)
      case ReassignPartitionLeaderElectionStrategy =&gt;
        // 重分配选举
        leaderForReassign(controllerContext, validLeaderAndIsrs).partition(_.leaderAndIsr.isEmpty)
      case PreferredReplicaPartitionLeaderElectionStrategy =&gt;
        // 最优选举
        leaderForPreferredReplica(controllerContext, validLeaderAndIsrs).partition(_.leaderAndIsr.isEmpty)
      case ControlledShutdownPartitionLeaderElectionStrategy =&gt;
        // shutdown 时选举
        leaderForControlledShutdown(controllerContext, validLeaderAndIsrs).partition(_.leaderAndIsr.isEmpty)
    }
}  

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/controller/Election.scala
// 从存活的ISR副本中选择一个作为新的leader，如果ISR为空且允许unclean election(脏选举)，那么就从存活的副本中选举一个作为新的leader。
private def leaderForOffline(partition: TopicPartition,
                             leaderAndIsrOpt: Option[LeaderAndIsr],
                             uncleanLeaderElectionEnabled: Boolean,
                             controllerContext: ControllerContext): ElectionResult = {

  val assignment = controllerContext.partitionReplicaAssignment(partition)
  val liveReplicas = assignment.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))
  leaderAndIsrOpt match {
    case Some(leaderAndIsr) =&gt;
      val isr = leaderAndIsr.isr
      // 选举操作
      // 从存活的ISR副本中选择一个作为新的leader，如果ISR为空且允许unclean election(脏选举)，那么就从存活的副本中选举一个作为新的leader。
      val leaderOpt = PartitionLeaderElectionAlgorithms.offlinePartitionLeaderElection(
        assignment, isr, liveReplicas.toSet, uncleanLeaderElectionEnabled, controllerContext)
      val newLeaderAndIsrOpt = leaderOpt.map { leader =&gt;
        val newIsr = if (isr.contains(leader)) isr.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))
        else List(leader)
        // 生成新的ISR
        leaderAndIsr.newLeaderAndIsr(leader, newIsr)
      }
      ElectionResult(partition, newLeaderAndIsrOpt, liveReplicas)

    case None =&gt;
      // 返回无可用副本
      ElectionResult(partition, None, liveReplicas)
  }
}

// 分区重分配时选举，例如新增分区时，重新选举leader
private def leaderForReassign(partition: TopicPartition,
                              leaderAndIsr: LeaderAndIsr,
                              controllerContext: ControllerContext): ElectionResult = {
  val targetReplicas = controllerContext.partitionFullReplicaAssignment(partition).targetReplicas
  // 获取所有在线的副本
  val liveReplicas = targetReplicas.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))
  val isr = leaderAndIsr.isr
  // 从存活的ISR副本中选择一个作为新的leader
  val leaderOpt = PartitionLeaderElectionAlgorithms.reassignPartitionLeaderElection(targetReplicas, isr, liveReplicas.toSet)
  // 生成新的ISR
  val newLeaderAndIsrOpt = leaderOpt.map(leader =&gt; leaderAndIsr.newLeader(leader))
  ElectionResult(partition, newLeaderAndIsrOpt, targetReplicas)
}

// 副本优选选举leader
private def leaderForPreferredReplica(partition: TopicPartition,
                                      leaderAndIsr: LeaderAndIsr,
                                      controllerContext: ControllerContext): ElectionResult = {
  val assignment = controllerContext.partitionReplicaAssignment(partition)
  // 获取所有在线的副本
  val liveReplicas = assignment.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))
  val isr = leaderAndIsr.isr
  // 从存活的ISR副本中选择第一个作为新的leader
  val leaderOpt = PartitionLeaderElectionAlgorithms.preferredReplicaPartitionLeaderElection(assignment, isr, liveReplicas.toSet)
  // 生成新的ISR
  val newLeaderAndIsrOpt = leaderOpt.map(leader =&gt; leaderAndIsr.newLeader(leader))
  ElectionResult(partition, newLeaderAndIsrOpt, assignment)
}

// controller shutdown时，选举新的leader
private def leaderForControlledShutdown(partition: TopicPartition,
                                        leaderAndIsr: LeaderAndIsr,
                                        shuttingDownBrokerIds: Set[Int],
                                        controllerContext: ControllerContext): ElectionResult = {
  val assignment = controllerContext.partitionReplicaAssignment(partition)
  // 获取所有副本，包括shutdown状态的
  val liveOrShuttingDownReplicas = assignment.filter(replica =&gt;
    controllerContext.isReplicaOnline(replica, partition, includeShuttingDownBrokers = true))
  val isr = leaderAndIsr.isr
  // 从存活的ISR副本中选择一个状态不为shutdown的作为新leader
  val leaderOpt = PartitionLeaderElectionAlgorithms.controlledShutdownPartitionLeaderElection(assignment, isr,
    liveOrShuttingDownReplicas.toSet, shuttingDownBrokerIds)
  val newIsr = isr.filter(replica =&gt; !shuttingDownBrokerIds.contains(replica))
  // 生成新的ISR
  val newLeaderAndIsrOpt = leaderOpt.map(leader =&gt; leaderAndIsr.newLeaderAndIsr(leader, newIsr))
  ElectionResult(partition, newLeaderAndIsrOpt, liveOrShuttingDownReplicas)
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/controller/PartitionStateMachine.scala
object PartitionLeaderElectionAlgorithms {
  // 从存活的ISR副本中选择一个作为新的leader，如果ISR为空且允许unclean election(脏选举)，那么就从存活的副本中选举一个作为新的leader。  
  def offlinePartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int], uncleanLeaderElectionEnabled: Boolean, controllerContext: ControllerContext): Option[Int] = {
    assignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id)).orElse {
      if (uncleanLeaderElectionEnabled) {
        val leaderOpt = assignment.find(liveReplicas.contains)
        if (leaderOpt.isDefined)
          controllerContext.stats.uncleanLeaderElectionRate.mark()
        leaderOpt
      } else {
        None
      }
    }
  }

  // 从存活的ISR副本中选择一个作为新的leader
  def reassignPartitionLeaderElection(reassignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int]): Option[Int] = {
    reassignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id))
  }

  // 从存活的ISR副本中选择第一个作为新的leader
  def preferredReplicaPartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int]): Option[Int] = {
    assignment.headOption.filter(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id))
  }

  // 从存活的ISR副本中选择一个状态不为shutdown的作为新leader
  def controlledShutdownPartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int], shuttingDownBrokers: Set[Int]): Option[Int] = {
    assignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id) &amp;&amp; !shuttingDownBrokers.contains(id))
  }
}

</code></pre>
<h3 id="控制器之管理topic">控制器之管理Topic</h3>
<p>负责topic的创建、删除，增加分区等操作。以sh命令创建topic为例，流程如下（源码）：</p>
<h4 id="启动命令">启动命令</h4>
<pre><code>bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --partitions 3 --replication-factor 3 --topic topic_test( Kafka 版本 &gt;= 2.2 支持此方式（推荐）)。
</code></pre>
<pre><code>// 文件地址：/bin/kafka-topics.sh
#!/bin/bash
exec $(dirname $0)/kafka-run-class.sh kafka.admin.TopicCommand &quot;$@&quot;
</code></pre>
<h4 id="topiccommand接受请求参数执行创建请求操作">TopicCommand接受请求参数，执行创建请求操作</h4>
<pre><code>// 文件地址：core/src/main/scala/kafka/admin/TopicCommand.scala
object TopicCommand extends Logging {

  def main(args: Array[String]): Unit = {
    val opts = new TopicCommandOptions(args)
    // 检查必填参数
    opts.checkArgs()

    // 创建topic服务
    val topicService = TopicService(opts.commandConfig, opts.bootstrapServer)

    var exitCode = 0
    try {
      if (opts.hasCreateOption)
        // 根据传入的参数判断判断是否有 --create 参数，有的话走创建主题逻辑。
        topicService.createTopic(opts)
      else if (opts.hasAlterOption)
        topicService.alterTopic(opts)
      else if (opts.hasListOption)
        topicService.listTopics(opts)
      else if (opts.hasDescribeOption)
        topicService.describeTopic(opts)
      else if (opts.hasDeleteOption)
        topicService.deleteTopic(opts)
    } catch {
      case e: ExecutionException =&gt;
        if (e.getCause != null)
          printException(e.getCause)
        else
          printException(e)
        exitCode = 1
      case e: Throwable =&gt;
        printException(e)
        exitCode = 1
    } finally {
      topicService.close()
      Exit.exit(exitCode)
    }
  }
  
    object TopicService {
    // 如果传入的参数有 --command-config，则将这个文件里的参数放到 commandConfig 一个 map 里，假如配置文件里面已经有了bootstrap.servers配置，那么会将其覆盖。  
    def createAdminClient(commandConfig: Properties, bootstrapServer: Option[String]): Admin = {
      bootstrapServer match {
        case Some(serverList) =&gt; commandConfig.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, serverList)
        case None =&gt;
      }
      // 生成一个客户端创建topic
      // Admin文件地址； clients/src/main/java/org/apache/kafka/clients/admin/Admin.java
      Admin.create(commandConfig)
    }

    def apply(commandConfig: Properties, bootstrapServer: Option[String]): TopicService =
      new TopicService(createAdminClient(commandConfig, bootstrapServer))
  }

</code></pre>
<pre><code>// 文件地址: clients/src/main/java/org/apache/kafka/clients/admin/Admin.java
@InterfaceStability.Evolving
public interface Admin extends AutoCloseable {

    /**
     * Create a new Admin with the given configuration.
     *
     * @param props The configuration.
     * @return The new KafkaAdminClient.
     */
    static Admin create(Properties props) {
        // 文件地址: clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.javas
        // 创建了一个客户端，用此客户端来管理topic
        return KafkaAdminClient.createInternal(new AdminClientConfig(props, true), null);
    }
}  

</code></pre>
<pre><code>  // 文件地址：core/src/main/scala/kafka/admin/TopicCommand.scala
  case class TopicService private (adminClient: Admin) extends AutoCloseable {

    def createTopic(opts: TopicCommandOptions): Unit = {
      val topic = new CommandTopicPartition(opts)
      if (Topic.hasCollisionChars(topic.name))
        println(&quot;WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could &quot; +
          &quot;collide. To avoid issues it is best to use either, but not both.&quot;)
      createTopic(topic)
    }

    def createTopic(topic: CommandTopicPartition): Unit = {
      //  假如配置了副本数，--replication-factor 需在1和32767之间。  
      if (topic.replicationFactor.exists(rf =&gt; rf &gt; Short.MaxValue || rf &lt; 1))
        throw new IllegalArgumentException(s&quot;The replication factor must be between 1 and ${Short.MaxValue} inclusive&quot;)
      // // 假如配置了分区数，--partitions 必须大于0。
      if (topic.partitions.exists(partitions =&gt; partitions &lt; 1))
        throw new IllegalArgumentException(s&quot;The partitions must be greater than 0&quot;)

      try {
        // 假如指定了 --replica-assignment 参数，则按照指定的方式来分配副本。  
        val newTopic = if (topic.hasReplicaAssignment)
          new NewTopic(topic.name, asJavaReplicaReassignment(topic.replicaAssignment.get))
        else {
          new NewTopic(
            topic.name,
            topic.partitions.asJava,
            topic.replicationFactor.map(_.toShort).map(Short.box).asJava)
        }

        // 将配置 --config 解析成一个配置 map
        val configsMap = topic.configsToAdd.stringPropertyNames()
          .asScala
          .map(name =&gt; name -&gt; topic.configsToAdd.getProperty(name))
          .toMap.asJava

        newTopic.configs(configsMap)
        //  调用 KafkaAdminClient.createTopics 创建 Topic
        val createResult = adminClient.createTopics(Collections.singleton(newTopic),
          new CreateTopicsOptions().retryOnQuotaViolation(false))
        // 获取创建结果  
        createResult.all().get()
        println(s&quot;Created topic ${topic.name}.&quot;)
      } catch {
        case e : ExecutionException =&gt;
          if (e.getCause == null)
            throw e
          if (!(e.getCause.isInstanceOf[TopicExistsException] &amp;&amp; topic.ifTopicDoesntExist()))
            throw e.getCause
      }
    }

</code></pre>
<pre><code>// 文件地址：clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java
@Override
public CreateTopicsResult createTopics(final Collection&lt;NewTopic&gt; newTopics,
                                       final CreateTopicsOptions options) {
    final Map&lt;String, KafkaFutureImpl&lt;TopicMetadataAndConfig&gt;&gt; topicFutures = new HashMap&lt;&gt;(newTopics.size());
    final CreatableTopicCollection topics = new CreatableTopicCollection();
    for (NewTopic newTopic : newTopics) {
        if (topicNameIsUnrepresentable(newTopic.name())) {
            KafkaFutureImpl&lt;TopicMetadataAndConfig&gt; future = new KafkaFutureImpl&lt;&gt;();
            future.completeExceptionally(new InvalidTopicException(&quot;The given topic name '&quot; +
                newTopic.name() + &quot;' cannot be represented in a request.&quot;));
            topicFutures.put(newTopic.name(), future);
        } else if (!topicFutures.containsKey(newTopic.name())) {
            topicFutures.put(newTopic.name(), new KafkaFutureImpl&lt;&gt;());
            topics.add(newTopic.convertToCreatableTopic());
        }
    }
    if (!topics.isEmpty()) {
        final long now = time.milliseconds();
        final long deadline = calcDeadlineMs(now, options.timeoutMs());
        // 远程调用执行创建操作
        final Call call = getCreateTopicsCall(options, topicFutures, topics,
            Collections.emptyMap(), now, deadline);
        runnable.call(call, now);
    }
    return new CreateTopicsResult(new HashMap&lt;&gt;(topicFutures));
}

private Call getCreateTopicsCall(final CreateTopicsOptions options,
                                 final Map&lt;String, KafkaFutureImpl&lt;TopicMetadataAndConfig&gt;&gt; futures,
                                 final CreatableTopicCollection topics,
                                 final Map&lt;String, ThrottlingQuotaExceededException&gt; quotaExceededExceptions,
                                 final long now,
                                 final long deadline) {
    //  使用ControllerNodeProvider选择controller进行远程调用，从此也可以看出，topic的创建是由controller控制                              
    return new Call(&quot;createTopics&quot;, deadline, new ControllerNodeProvider()) {
        @Override
        public CreateTopicsRequest.Builder createRequest(int timeoutMs) {
            return new CreateTopicsRequest.Builder(
                new CreateTopicsRequestData()
                    .setTopics(topics)
                    .setTimeoutMs(timeoutMs)
                    .setValidateOnly(options.shouldValidateOnly()));
        }

        @Override
        public void handleResponse(AbstractResponse abstractResponse) {
            // 处理响应结果
        }

        @Override
        void handleFailure(Throwable throwable) {
           // 处理失败
        }
    };
}

/**
 * Provides the controller node.
 */
private class ControllerNodeProvider implements NodeProvider {
    @Override
    public Node provide() {
        if (metadataManager.isReady() &amp;&amp;
                (metadataManager.controller() != null)) {
            // 返回集群中的broker控制器    
            return metadataManager.controller();
        }
        metadataManager.requestUpdate();
        return null;
    }
}  

</code></pre>
<h4 id="服务端接受请求并执行创建操作">服务端接受请求，并执行创建操作</h4>
<p>可参考上篇<a href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-qing-qiu-liu-cheng">《kafka深入理解之Broker请求流程》</a></p>
<pre><code>// 文件地址：/core/src/main/scala/kafka/server/KafkaRequestHandler.scala
// 核心函数是def run()中的 apis.handle(request, requestLocal)
// 文件地址：/core/src/main/scala/kafka/server/KafkaApis.scala
class KafkaApis(val requestChannel: RequestChannel,
                val metadataSupport: MetadataSupport,
                val replicaManager: ReplicaManager,
                val groupCoordinator: GroupCoordinator,
                val txnCoordinator: TransactionCoordinator,
                val autoTopicCreationManager: AutoTopicCreationManager,
                val brokerId: Int,
                val config: KafkaConfig,
                val configRepository: ConfigRepository,
                val metadataCache: MetadataCache,
                val metrics: Metrics,
                val authorizer: Option[Authorizer],
                val quotas: QuotaManagers,
                val fetchManager: FetchManager,
                brokerTopicStats: BrokerTopicStats,
                val clusterId: String,
                time: Time,
                val tokenManager: DelegationTokenManager,
                val apiVersionManager: ApiVersionManager) extends ApiRequestHandler with Logging {

  type FetchResponseStats = Map[TopicPartition, RecordConversionStats]
  this.logIdent = &quot;[KafkaApi-%d] &quot;.format(brokerId)
  val configHelper = new ConfigHelper(metadataCache, config, configRepository)
  val authHelper = new AuthHelper(authorizer)
  val requestHelper = new RequestHandlerHelper(requestChannel, quotas, time)
  val aclApis = new AclApis(authHelper, authorizer, requestHelper, &quot;broker&quot;, config)
  val configManager = new ConfigAdminManager(brokerId, config, configRepository)

  // 根据处理类型执行相关操作 
  private def maybeForwardToController(
    request: RequestChannel.Request,
    handler: RequestChannel.Request =&gt; Unit
  ): Unit = {
    def responseCallback(responseOpt: Option[AbstractResponse]): Unit = {
      responseOpt match {
        case Some(response) =&gt; requestHelper.sendForwardedResponse(request, response)
        case None =&gt; handleInvalidVersionsDuringForwarding(request)
      }
    }
    metadataSupport.maybeForward(request, handler, responseCallback)
  }

  /**
   * Top-level method that handles all requests and multiplexes to the right api
   */
  override def handle(request: RequestChannel.Request, requestLocal: RequestLocal): Unit = {
    try {
      trace(s&quot;Handling request:${request.requestDesc(true)} from connection ${request.context.connectionId};&quot; +
        s&quot;securityProtocol:${request.context.securityProtocol},principal:${request.context.principal}&quot;)

      if (!apiVersionManager.isApiEnabled(request.header.apiKey)) {
        // The socket server will reject APIs which are not exposed in this scope and close the connection
        // before handing them to the request handler, so this path should not be exercised in practice
        throw new IllegalStateException(s&quot;API ${request.header.apiKey} is not enabled&quot;)
      }

      // 根据api key匹配执行不同的操作
      request.header.apiKey match {
        ......  
        // 创建topic
        case ApiKeys.CREATE_TOPICS =&gt; maybeForwardToController(request, handleCreateTopicsRequest)
        ......
        case _ =&gt; throw new IllegalStateException(s&quot;No handler for request api key ${request.header.apiKey}&quot;)
      }
    } catch {
      case e: FatalExitError =&gt; throw e
      case e: Throwable =&gt;
        error(s&quot;Unexpected error handling request ${request.requestDesc(true)} &quot; +
          s&quot;with context ${request.context}&quot;, e)
        requestHelper.handleError(request, e)
    } finally {
      // try to complete delayed action. In order to avoid conflicting locking, the actions to complete delayed requests
      // are kept in a queue. We add the logic to check the ReplicaManager queue at the end of KafkaApis.handle() and the
      // expiration thread for certain delayed operations (e.g. DelayedJoin)
      replicaManager.tryCompleteActions()
      // The local completion time may be set while processing the request. Only record it if it's unset.
      if (request.apiLocalCompleteTimeNanos &lt; 0)
        request.apiLocalCompleteTimeNanos = time.nanoseconds
    }
  }

// 执行创建topic的请求  
def handleCreateTopicsRequest(request: RequestChannel.Request): Unit = {
  // 获取zookeeper支持器，用于在zookeeper上创建对应的topic节点
  val zkSupport = metadataSupport.requireZkOrThrow(KafkaApis.shouldAlwaysForward(request))
  val controllerMutationQuota = quotas.controllerMutation.newQuotaFor(request, strictSinceVersion = 6)
  
  // 响应结果回调
  def sendResponseCallback(results: CreatableTopicResultCollection): Unit = {
    def createResponse(requestThrottleMs: Int): AbstractResponse = {
      val responseData = new CreateTopicsResponseData()
        .setThrottleTimeMs(requestThrottleMs)
        .setTopics(results)
      val responseBody = new CreateTopicsResponse(responseData)
      trace(s&quot;Sending create topics response $responseData for correlation id &quot; +
        s&quot;${request.header.correlationId} to client ${request.header.clientId}.&quot;)
      responseBody
    }
    requestHelper.sendResponseMaybeThrottleWithControllerQuota(controllerMutationQuota, request, createResponse)
  }

  val createTopicsRequest = request.body[CreateTopicsRequest]
  val results = new CreatableTopicResultCollection(createTopicsRequest.data.topics.size)
  // 判断当前broker是否为controller，只有controller才能创建topic----创建topic过程中，可能出现controller变更
  if (!zkSupport.controller.isActive) {
    createTopicsRequest.data.topics.forEach { topic =&gt;
      results.add(new CreatableTopicResult().setName(topic.name)
        .setErrorCode(Errors.NOT_CONTROLLER.code))
    }
    sendResponseCallback(results)
  } else {
    createTopicsRequest.data.topics.forEach { topic =&gt;
      results.add(new CreatableTopicResult().setName(topic.name))
    }
    ......
    // 处理创建topic响应结果
    def handleCreateTopicsResults(errors: Map[String, ApiError]): Unit = {
      errors.foreach { case (topicName, error) =&gt;
        val result = results.find(topicName)
        result.setErrorCode(error.error.code)
          .setErrorMessage(error.message)
        // Reset any configs in the response if Create failed
        if (error != ApiError.NONE) {
          result.setConfigs(List.empty.asJava)
            .setNumPartitions(-1)
            .setReplicationFactor(-1)
            .setTopicConfigErrorCode(Errors.NONE.code)
        }
      }
      sendResponseCallback(results)
    }
    // 在zookeeper上创建topic节点
    zkSupport.adminManager.createTopics(
      createTopicsRequest.data.timeoutMs,
      createTopicsRequest.data.validateOnly,
      toCreate,
      authorizedForDescribeConfigs,
      controllerMutationQuota,
      handleCreateTopicsResults)
  }
}  

</code></pre>
<h4 id="写入zk的参数信息">写入zk的参数信息</h4>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/ZkAdminManager.scala
/**
  * Create topics and wait until the topics have been completely created.
  * The callback function will be triggered either when timeout, error or the topics are created.
  */
def createTopics(timeout: Int,
                 validateOnly: Boolean,
                 toCreate: Map[String, CreatableTopic],
                 includeConfigsAndMetadata: Map[String, CreatableTopicResult],
                 controllerMutationQuota: ControllerMutationQuota,
                 responseCallback: Map[String, ApiError] =&gt; Unit): Unit = {

  // 1. map over topics creating assignment and calling zookeeper
  val brokers = metadataCache.getAliveBrokers()
  val metadata = toCreate.values.map(topic =&gt;
    try {
      ......

      if (validateOnly) {
        CreatePartitionsMetadata(topic.name, assignments.keySet)
      } else {
        controllerMutationQuota.record(assignments.size)
        // 创建topic时把相关参数也写入zk中
        adminZkClient.createTopicWithAssignment(topic.name, configs, assignments, validate = false, config.usesTopicId)
        populateIds(includeConfigsAndMetadata, topic.name)
        CreatePartitionsMetadata(topic.name, assignments.keySet)
      }
    } catch {
      // Log client errors at a lower level than unexpected exceptions
      ......
    }).toBuffer
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/zk/ZkAdminManager.scala
/**
 * Create topic and optionally validate its parameters. Note that this method is used by the
 * TopicCommand as well.
 *
 * @param topic The name of the topic
 * @param config The config of the topic
 * @param partitionReplicaAssignment The assignments of the topic
 * @param validate Boolean indicating if parameters must be validated or not (true by default)
 * @param usesTopicId Boolean indicating whether the topic ID will be created
 */
def createTopicWithAssignment(topic: String,
                              config: Properties,
                              partitionReplicaAssignment: Map[Int, Seq[Int]],
                              validate: Boolean = true,
                              usesTopicId: Boolean = false): Unit = {
  if (validate)
    validateTopicCreate(topic, partitionReplicaAssignment, config)

  info(s&quot;Creating topic $topic with configuration $config and initial partition &quot; +
    s&quot;assignment $partitionReplicaAssignment&quot;)

  // write out the config if there is any, this isn't transactional with the partition assignments
  // 将topic配置信息写入zk，不包含分区参数
  zkClient.setOrCreateEntityConfigs(ConfigType.Topic, topic, config)

  // create the partition assignment
  // 写入分区参数
  writeTopicPartitionAssignment(topic, partitionReplicaAssignment.map { case (k, v) =&gt; k -&gt; ReplicaAssignment(v) },
    isUpdate = false, usesTopicId)
}

// 写入分区参数信息
// 对应的参数写入znode路径--TopicZNode.path(topic)----/brokers/topics/&lt;topic_name&gt;  
private def writeTopicPartitionAssignment(topic: String, replicaAssignment: Map[Int, ReplicaAssignment],
                                          isUpdate: Boolean, usesTopicId: Boolean = false): Unit = {
  try {
    val assignment = replicaAssignment.map { case (partitionId, replicas) =&gt; (new TopicPartition(topic,partitionId), replicas) }.toMap

    if (!isUpdate) {
      val topicIdOpt = if (usesTopicId) Some(Uuid.randomUuid()) else None
      // 不存在则创建
      zkClient.createTopicAssignment(topic, topicIdOpt, assignment.map { case (k, v) =&gt; k -&gt; v.replicas })
    } else {
      val topicIds = zkClient.getTopicIdsForTopics(Set(topic))
      // 存在就更新参数信息
      zkClient.setTopicAssignment(topic, topicIds.get(topic), assignment)
    }
    debug(&quot;Updated path %s with %s for replica assignment&quot;.format(TopicZNode.path(topic), assignment))
  } catch {
    case _: NodeExistsException =&gt; throw new TopicExistsException(s&quot;Topic '$topic' already exists.&quot;)
    case e2: Throwable =&gt; throw new AdminOperationException(e2.toString)
  }
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/zk/KafkaZkClient.scala
/**
 * Sets or creates the entity znode path with the given configs depending
 * on whether it already exists or not.
 * 根据给定的路径创建znode节点（如果不存在则创建，topic节点）---/config/topics/&lt;topic_name&gt;  
 *
 * If this is method is called concurrently, the last writer wins. In cases where we update configs and then
 * partition assignment (i.e. create topic), it's possible for one thread to set this and the other to set the
 * partition assignment. As such, the recommendation is to never call create topic for the same topic with different
 * configs/partition assignment concurrently.
 *
 * @param rootEntityType entity type
 * @param sanitizedEntityName entity name
 * @throws KeeperException if there is an error while setting or creating the znode
 */
def setOrCreateEntityConfigs(rootEntityType: String, sanitizedEntityName: String, config: Properties) = {

  def set(configData: Array[Byte]): SetDataResponse = {
    val setDataRequest = SetDataRequest(ConfigEntityZNode.path(rootEntityType, sanitizedEntityName),
      configData, ZkVersion.MatchAnyVersion)
    retryRequestUntilConnected(setDataRequest)
  }

  def createOrSet(configData: Array[Byte]): Unit = {
    // znode 节点路径名：/config/topics/&lt;topic_name&gt;  
    val path = ConfigEntityZNode.path(rootEntityType, sanitizedEntityName)
    try createRecursive(path, configData)
    catch {
      case _: NodeExistsException =&gt; set(configData).maybeThrow()
    }
  }

  val configData = ConfigEntityZNode.encode(config)

  val setDataResponse = set(configData)
  setDataResponse.resultCode match {
    case Code.NONODE =&gt; createOrSet(configData)
    case _ =&gt; setDataResponse.maybeThrow()
  }
}

</code></pre>
<h4 id="controller监听brokerstopicstopic_name-如果节点有变化则会通知controller进行处理">Controller监听/brokers/topics/&lt;topic_name&gt; ，如果节点有变化，则会通知Controller进行处理</h4>
<pre><code>// 文件地址：core/src/main/scala/kafka/controller/KafkaController.scala
override def process(event: ControllerEvent): Unit = {
  try {
    event match {
      case event: MockEvent =&gt;
        // Used only in test cases
        event.process()
      ......
      // 处理topic改变
      case TopicChange =&gt;
        processTopicChange()
      ...... 
    }
  } catch {
    case e: ControllerMovedException =&gt;
      info(s&quot;Controller moved to another broker when processing $event.&quot;, e)
      maybeResign()
    case e: Throwable =&gt;
      error(s&quot;Error processing event $event&quot;, e)
  } finally {
    updateMetrics()
  }
}

private def processTopicChange(): Unit = {
  // 非 controller 角色则直接返回，不作任何处理  
  if (!isActive) return
  // 获取zk集群中的所有topic信息---即/brokers/topics节点下的信息
  val topics = zkClient.getAllTopicsInCluster(true)
  // 获取新增状态的topic
  val newTopics = topics -- controllerContext.allTopics
  // 获取删除状态的topic
  val deletedTopics = controllerContext.allTopics.diff(topics)
  // 所有的topic都放入控制器上下文中
  controllerContext.setAllTopics(topics)

  // 注册分区变动处理器
  registerPartitionModificationsHandlers(newTopics.toSeq)
  // 对于新增topic，获取其可分配的副本及主题id
  val addedPartitionReplicaAssignment = zkClient.getReplicaAssignmentAndTopicIdForTopics(newTopics)
  // 从控制器上下文中移除已删除的topic
  deletedTopics.foreach(controllerContext.removeTopic)
  processTopicIds(addedPartitionReplicaAssignment)

  addedPartitionReplicaAssignment.foreach { case TopicIdReplicaAssignment(_, _, newAssignments) =&gt;
    newAssignments.foreach { case (topicAndPartition, newReplicaAssignment) =&gt;
      controllerContext.updatePartitionFullReplicaAssignment(topicAndPartition, newReplicaAssignment)
    }
  }
  info(s&quot;New topics: [$newTopics], deleted topics: [$deletedTopics], new partition replica assignment &quot; +
    s&quot;[$addedPartitionReplicaAssignment]&quot;)
  if (addedPartitionReplicaAssignment.nonEmpty) {
    val partitionAssignments = addedPartitionReplicaAssignment
      .map { case TopicIdReplicaAssignment(_, _, partitionsReplicas) =&gt; partitionsReplicas.keySet }
      .reduce((s1, s2) =&gt; s1.union(s2))
    // 处理新的可分配分区的状态  
    onNewPartitionCreation(partitionAssignments)
  }
}

/**
 * This callback is invoked by the topic change callback with the list of failed brokers as input.
 * It does the following -
 * 1. Move the newly created partitions to the NewPartition state--新创建的分区修改状态为NewPartition 
 * 2. Move the newly created partitions from NewPartition-&gt;OnlinePartition state--新创建的分区修改状态NewPartition为OnlinePartition 
 */
private def onNewPartitionCreation(newPartitions: Set[TopicPartition]): Unit = {
  info(s&quot;New partition creation callback for ${newPartitions.mkString(&quot;,&quot;)}&quot;)
  // 修改状态为 NewPartition
  partitionStateMachine.handleStateChanges(newPartitions.toSeq, NewPartition)
  // 修改状态为 NewReplica
  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions).toSeq, NewReplica)
  // 修改状态为 OnlinePartition
  partitionStateMachine.handleStateChanges(
    newPartitions.toSeq,
    OnlinePartition,
    Some(OfflinePartitionLeaderElectionStrategy(false))
  )
  // 修改状态为 OnlineReplica
  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions).toSeq, OnlineReplica)
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaApis.scala
def handleLeaderAndIsrRequest(request: RequestChannel.Request): Unit = {
  val zkSupport = metadataSupport.requireZkOrThrow(KafkaApis.shouldNeverReceive(request))
  // ensureTopicExists is only for client facing requests
  // We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they
  // stop serving data to clients for the topic being deleted
  val correlationId = request.header.correlationId
  val leaderAndIsrRequest = request.body[LeaderAndIsrRequest]

  authHelper.authorizeClusterOperation(request, CLUSTER_ACTION)
  if (isBrokerEpochStale(zkSupport, leaderAndIsrRequest.brokerEpoch)) {
    // When the broker restarts very quickly, it is possible for this broker to receive request intended
    // for its previous generation so the broker should skip the stale request.
    info(&quot;Received LeaderAndIsr request with broker epoch &quot; +
      s&quot;${leaderAndIsrRequest.brokerEpoch} smaller than the current broker epoch ${zkSupport.controller.brokerEpoch}&quot;)
    requestHelper.sendResponseExemptThrottle(request, leaderAndIsrRequest.getErrorResponse(0, Errors.STALE_BROKER_EPOCH.exception))
  } else {
    // 创建本地log，如果日志已经存在，只返回现有日志的副本，否则如果 isNew=true 或者如果没有离线日志目录，则为给定的主题和给定的分区创建日志，否则抛出 KafkaStorageException。
    // 源码调用链：kafka.server.ReplicaManager#becomeLeaderOrFollower -&gt; kafka.server.ReplicaManager#makeLeaders 
    // -&gt; kafka.cluster.Partition#makeLeader -&gt; kafka.cluster.Partition#createLogIfNotExists
    // -&gt; kafka.cluster.Partition#createLog-&gt; kafka.log.LogManager#getOrCreateLog
    val response = replicaManager.becomeLeaderOrFollower(correlationId, leaderAndIsrRequest,
      RequestHandlerHelper.onLeadershipChange(groupCoordinator, txnCoordinator, _, _))
    requestHelper.sendResponseExemptThrottle(request, response)
  }
}

</code></pre>
<pre><code>// 文件地址：core/src/main/scala/kafka/cluster/Partition.scala
def createLogIfNotExists(isNew: Boolean, isFutureReplica: Boolean, offsetCheckpoints: OffsetCheckpoints, topicId: Option[Uuid]): Unit = {
  def maybeCreate(logOpt: Option[UnifiedLog]): UnifiedLog = {
    logOpt match {
      case Some(log) =&gt;
        trace(s&quot;${if (isFutureReplica) &quot;Future UnifiedLog&quot; else &quot;UnifiedLog&quot;} already exists.&quot;)
        if (log.topicId.isEmpty)
          topicId.foreach(log.assignTopicId)
        log
      case None =&gt;
        // 不存在才创建
        createLog(isNew, isFutureReplica, offsetCheckpoints, topicId)
    }
  }

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之Broker请求流程]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-qing-qiu-liu-cheng/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-broker-qing-qiu-liu-cheng/">
        </link>
        <updated>2022-07-02T06:17:03.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka broker请求流程。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/nature-21474001_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="broker请求流程详解">Broker请求流程详解</h2>
<h3 id="reactor-模型">Reactor 模型</h3>
<p>在介绍kafka的broker通信前，有必要简述一下reactor(响应式)模型。Reactor核心是基于事件驱动（IO多路复用），整体架构流程图如下所示(源自Doug Lea)：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316001.png" alt="img" loading="lazy"></figure>
<ul>
<li>整个模型中所有的输入都由一个Reactor进行接受，随后通过一个dispatch loop（即acceptor）轮询将事件推送至不同的工作线程处理器进行相关逻辑处理。</li>
<li>在这个架构中，Acceptor 线程只是用来进行请求分发，所以非常轻量级，因此会有很高的吞吐量。而那些工作线程则可以根据实际系统负载情况动态的调节系统负载能力，从而达到请求处理的平衡性。</li>
<li>多核处理时，还可以针对reactor进行池化处理(pool)，来提高IO效率。在此过程中，每个Reactor都有自己的选择器，线程处理器及分发轮询器，由主acceptor将数据分发给其他reactors。如下图所示：</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316002.png" alt="img" loading="lazy"></figure>
<h3 id="broker-处理流程">Broker 处理流程</h3>
<p>在kafka中，集群中的每个broker都面临着巨量的网络请求，这个时候如果使用单线程或者多线程进行网络连接处理，那吞吐量将难以达到期望值。此时，上文所说的reactor模型便有了应用场景。</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316003.png" alt="img" loading="lazy"></figure>
<p>在Kafka的架构中，会有很多客户端向Broker端发送请求，Kafka 的 Broker 端有个 SocketServer 组件，主要负责和客户端建立连接，并将相关请求通过Acceptor转发给对应的处理器线程池；而网络连接池(RequestHandlerPool)则主要负责真实的逻辑处理。</p>
<p>SocketServer 组件是 Kafka 超高并发网络通信层中最重要的子模块。它包含 Acceptor 线程、Processor 线程和 RequestChannel 等对象，都是网络通信的重要组成部分。它主要实现了 Reactor 设计模式，用来处理外部多个 Clients（这里的 Clients 可能包含 Producer、Consumer 或其他 Broker）的并发请求，并负责将处理结果封装进 Response 中，返还给 Clients。</p>
<p>其中Acceptor 线程采用轮询的方式将入站请求公平地发到所有网络线程中，网络线程池默认大小是 3个，表示每台 Broker 启动时会创建 3 个网络线程，专门处理客户端发送的请求，可以通过Broker 端参数 num.network.threads（可适当的设置为CPU核数*2，这个值过低时可能会出现因网络空闲太低而缺失副本。）来进行修改。</p>
<p>关于网络线程处理流程如下所示：</p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230316004.png" alt="img" loading="lazy"></figure>
<p>当网络线程拿到请求后，会将请求放入到一个共享请求队列中。Broker 端还有个 IO 线程池，负责从该队列中取出请求，执行真正的处理。如果是 PRODUCE 生产请求，则将消息写入到底层的磁盘日志中；如果是 FETCH 请求，则从磁盘或页缓存中读取消息。</p>
<p>IO 线程池中的线程负责执行具体的请求逻辑，默认是8，表示每台 Broker 启动后自动创建 8 个 IO 线程处理请求，可以通过Broker 端参数 num.io.threads（可设置为broker磁盘个数*2）调整。</p>
<p>Purgatory组件是用来缓存延时请求（Delayed Request）的。比如设置了 acks=all 的 PRODUCE 请求，一旦设置了 acks=all，那么该请求就必须等待 ISR 中所有副本都接收了消息后才能返回，此时处理该请求的 IO 线程就必须等待其他 Broker 的写入结果。</p>
<h4 id="socketserver">SocketServer</h4>
<p>负责管理Acceptor 线程、Processor线程和 RequestChannel等对象。</p>
<p>源码地址：<a href="https://github.com/apache/kafka">点击此处跳转github-kafka源码地址</a></p>
<pre><code>// 文件地址：core/src/main/scala/kafka/network/SocketServer.scala
class SocketServer(val config: KafkaConfig,
                   val metrics: Metrics,
                   val time: Time,
                   val credentialProvider: CredentialProvider,
                   val apiVersionManager: ApiVersionManager)
  extends Logging with KafkaMetricsGroup with BrokerReconfigurable {
  // 共享队列长度，即网络阻塞之前可容纳的等待数。由broker端的queued.max.requests参数控制，默认值500
  private val maxQueuedRequests = config.queuedMaxRequests

  private val nodeId = config.brokerId

  private val logContext = new LogContext(s&quot;[SocketServer listenerType=${apiVersionManager.listenerType}, nodeId=$nodeId] &quot;)

  this.logIdent = logContext.logPrefix

  private val memoryPoolSensor = metrics.sensor(&quot;MemoryPoolUtilization&quot;)
  private val memoryPoolDepletedPercentMetricName = metrics.metricName(&quot;MemoryPoolAvgDepletedPercent&quot;, MetricsGroup)
  private val memoryPoolDepletedTimeMetricName = metrics.metricName(&quot;MemoryPoolDepletedTimeTotal&quot;, MetricsGroup)
  memoryPoolSensor.add(new Meter(TimeUnit.MILLISECONDS, memoryPoolDepletedPercentMetricName, memoryPoolDepletedTimeMetricName))
  private val memoryPool = if (config.queuedMaxBytes &gt; 0) new SimpleMemoryPool(config.queuedMaxBytes, config.socketRequestMaxBytes, false, memoryPoolSensor) else MemoryPool.NONE
 
   // data-plane
  // 处理数据面请求的 processor线程池 
  private val dataPlaneProcessors = new ConcurrentHashMap[Int, Processor]()
  // 处理数据面请求的 acceptor线程池，一个监听器对应一个acceptor线程
  private[network] val dataPlaneAcceptors = new ConcurrentHashMap[EndPoint, Acceptor]()
  // 处理数据面的requestChannel对象
  val dataPlaneRequestChannel = new RequestChannel(maxQueuedRequests, DataPlaneMetricPrefix, time, apiVersionManager.newRequestMetrics)
  
  // control-plane
  // 处理控制面请求的 processor，只一个
  private var controlPlaneProcessorOpt : Option[Processor] = None
  // 处理控制面请求的 acceptor，只一个
  private[network] var controlPlaneAcceptorOpt : Option[Acceptor] = None
  // 处理控制面的requestChannel对象
  val controlPlaneRequestChannelOpt: Option[RequestChannel] = config.controlPlaneListenerName.map(_ =&gt;
    new RequestChannel(20, ControlPlaneMetricPrefix, time, apiVersionManager.newRequestMetrics))

  private var nextProcessorId = 0
  val connectionQuotas = new ConnectionQuotas(config, time, metrics)
  private var startedProcessingRequests = false
  private var stoppedProcessingRequests = false

</code></pre>
<h4 id="requestchannel">RequestChannel</h4>
<p>负责管理Processor，并作为传输request和response的中转站。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/network/RequestChannel.scala
class RequestChannel(val queueSize: Int,
                     val metricNamePrefix: String,
                     time: Time,
                     val metrics: RequestChannel.Metrics) extends KafkaMetricsGroup {
  import RequestChannel._
  // 共享请求阻塞队列
  private val requestQueue = new ArrayBlockingQueue[BaseRequest](queueSize)
  // processor 线程池
  private val processors = new ConcurrentHashMap[Int, Processor]()
  val requestQueueSizeMetricName = metricNamePrefix.concat(RequestQueueSizeMetric)
  val responseQueueSizeMetricName = metricNamePrefix.concat(ResponseQueueSizeMetric)

  newGauge(requestQueueSizeMetricName, () =&gt; requestQueue.size)

  newGauge(responseQueueSizeMetricName, () =&gt; {
    processors.values.asScala.foldLeft(0) {(total, processor) =&gt;
      total + processor.responseQueueSize
    }
  })
  // 添加processor线程 
  def addProcessor(processor: Processor): Unit = {
    if (processors.putIfAbsent(processor.id, processor) != null)
      warn(s&quot;Unexpected processor with processorId ${processor.id}&quot;)

    newGauge(responseQueueSizeMetricName, () =&gt; processor.responseQueueSize,
      Map(ProcessorMetricTag -&gt; processor.id.toString))
  }
  // 删除processor线程 
  def removeProcessor(processorId: Int): Unit = {
    processors.remove(processorId)
    removeMetric(responseQueueSizeMetricName, Map(ProcessorMetricTag -&gt; processorId.toString))
  }

  /** Send a request to be handled, potentially blocking until there is room in the queue for the request */
  // 发送请求到队列中，如果空间不足将会处于阻塞状态
  def sendRequest(request: RequestChannel.Request): Unit = {
    requestQueue.put(request)
  }
  // 关闭连接
  def closeConnection(
    request: RequestChannel.Request,
    errorCounts: java.util.Map[Errors, Integer]
  ): Unit = {
    // This case is used when the request handler has encountered an error, but the client
    // does not expect a response (e.g. when produce request has acks set to 0)
    updateErrorMetrics(request.header.apiKey, errorCounts.asScala)
    sendResponse(new RequestChannel.CloseConnectionResponse(request))
  }

  def sendResponse(
    request: RequestChannel.Request,
    response: AbstractResponse,
    onComplete: Option[Send =&gt; Unit]
  ): Unit = {
    updateErrorMetrics(request.header.apiKey, response.errorCounts.asScala)
    sendResponse(new RequestChannel.SendResponse(
      request,
      request.buildResponseSend(response),
      request.responseNode(response),
      onComplete
    ))
  }

  def sendNoOpResponse(request: RequestChannel.Request): Unit = {
    sendResponse(new network.RequestChannel.NoOpResponse(request))
  }

  def startThrottling(request: RequestChannel.Request): Unit = {
    sendResponse(new RequestChannel.StartThrottlingResponse(request))
  }

  def endThrottling(request: RequestChannel.Request): Unit = {
    sendResponse(new EndThrottlingResponse(request))
  }

  /** Send a response back to the socket server to be sent over the network */
  // 将响应结果发回socket服务，并通过网络传输
  private[network] def sendResponse(response: RequestChannel.Response): Unit = {
    if (isTraceEnabled) {
      val requestHeader = response.request.headerForLoggingOrThrottling()
      val message = response match {
        case sendResponse: SendResponse =&gt;
          s&quot;Sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} of ${sendResponse.responseSend.size} bytes.&quot;
        case _: NoOpResponse =&gt;
          s&quot;Not sending ${requestHeader.apiKey} response to client ${requestHeader.clientId} as it's not required.&quot;
        case _: CloseConnectionResponse =&gt;
          s&quot;Closing connection for client ${requestHeader.clientId} due to error during ${requestHeader.apiKey}.&quot;
        case _: StartThrottlingResponse =&gt;
          s&quot;Notifying channel throttling has started for client ${requestHeader.clientId} for ${requestHeader.apiKey}&quot;
        case _: EndThrottlingResponse =&gt;
          s&quot;Notifying channel throttling has ended for client ${requestHeader.clientId} for ${requestHeader.apiKey}&quot;
      }
      trace(message)
    }

    response match {
      // We should only send one of the following per request
      case _: SendResponse | _: NoOpResponse | _: CloseConnectionResponse =&gt;
        val request = response.request
        val timeNanos = time.nanoseconds()
        request.responseCompleteTimeNanos = timeNanos
        if (request.apiLocalCompleteTimeNanos == -1L)
          request.apiLocalCompleteTimeNanos = timeNanos
      // For a given request, these may happen in addition to one in the previous section, skip updating the metrics
      case _: StartThrottlingResponse | _: EndThrottlingResponse =&gt; ()
    }

    val processor = processors.get(response.processor)
    // The processor may be null if it was shutdown. In this case, the connections
    // are closed, so the response is dropped.
    if (processor != null) {
      processor.enqueueResponse(response)
    }
  }

  /** Get the next request or block until specified time has elapsed */
  // 在过期时间之前一直阻塞获取下一个请求
  def receiveRequest(timeout: Long): RequestChannel.BaseRequest =
    requestQueue.poll(timeout, TimeUnit.MILLISECONDS)

  /** Get the next request or block until there is one */
  // 一直阻塞获取下一个请求
  def receiveRequest(): RequestChannel.BaseRequest =
    requestQueue.take()

  def updateErrorMetrics(apiKey: ApiKeys, errors: collection.Map[Errors, Integer]): Unit = {
    errors.forKeyValue { (error, count) =&gt;
      metrics(apiKey.name).markErrorMeter(error, count)
    }
  }

  def clear(): Unit = {
    requestQueue.clear()
  }

  def shutdown(): Unit = {
    clear()
    metrics.close()
  }

  def sendShutdownRequest(): Unit = requestQueue.put(ShutdownRequest)

}

</code></pre>
<h4 id="acceptor-线程">Acceptor 线程</h4>
<p>Reactor模型中，有一种重要的Dispatcher角色，主要用来接受外部请求，并进行分发操作；这个角色也被称为Acceptor。在kafka中的，每个broker端的每个SocketServer实例只会创建一个Acceptor线程，这个Acceptor线程主要用来创建连接，并分发请求给Processor线程处理。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/network/SocketServer.scala
private[kafka] class Acceptor(val endPoint: EndPoint,
                              val sendBufferSize: Int,
                              val recvBufferSize: Int,
                              nodeId: Int,
                              connectionQuotas: ConnectionQuotas,
                              metricPrefix: String,
                              time: Time,
                              logPrefix: String = &quot;&quot;) extends AbstractServerThread(connectionQuotas) with KafkaMetricsGroup {

  this.logIdent = logPrefix
  // 创建nio selector对象，用来检查一个或多个NIO Channel的状态是否处于可读、可写
  private val nioSelector = NSelector.open()
  // broker 端创建对应的socketServer channel实例，并注册到selector上。
  val serverChannel = openServerSocket(endPoint.host, endPoint.port)
  // 创建processors 线程池
  private val processors = new ArrayBuffer[Processor]()
  // processors启动标志
  private val processorsStarted = new AtomicBoolean
  // 阻塞状态记录器
  private val blockedPercentMeter = newMeter(s&quot;${metricPrefix}AcceptorBlockedPercent&quot;,
    &quot;blocked time&quot;, TimeUnit.NANOSECONDS, Map(ListenerMetricTag -&gt; endPoint.listenerName.value))
  private var currentProcessorIndex = 0
  private[network] val throttledSockets = new mutable.PriorityQueue[DelayedCloseSocket]()

  // 延迟关闭socket
  private[network] case class DelayedCloseSocket(socket: SocketChannel, endThrottleTimeMs: Long) extends Ordered[DelayedCloseSocket] {
    override def compare(that: DelayedCloseSocket): Int = endThrottleTimeMs compare that.endThrottleTimeMs
  }
  
    /**
   * Accept loop that checks for new connection attempts
   * 循环检查是否有新连接进入，如果有的话就给其分配处理器
   */
  def run(): Unit = {
    // 注册  OP_ACCEPT
    serverChannel.register(nioSelector, SelectionKey.OP_ACCEPT)
    // 等待 Acceptor启动完成
    startupComplete()
    try {
      while (isRunning) {
        try {
          // 循环检查是否有可用的新连接  
          acceptNewConnections()
          // 关闭连接
          closeThrottledConnections()
        }
        catch {
          // We catch all the throwables to prevent the acceptor thread from exiting on exceptions due
          // to a select operation on a specific channel or a bad request. We don't want
          // the broker to stop responding to requests from other clients in these scenarios.
          case e: ControlThrowable =&gt; throw e
          case e: Throwable =&gt; error(&quot;Error occurred&quot;, e)
        }
      }
    } finally {
      debug(&quot;Closing server socket, selector, and any throttled sockets.&quot;)
      CoreUtils.swallow(serverChannel.close(), this, Level.ERROR)
      CoreUtils.swallow(nioSelector.close(), this, Level.ERROR)
      throttledSockets.foreach(throttledSocket =&gt; closeSocket(throttledSocket.socket))
      throttledSockets.clear()
      shutdownComplete()
    }
  }
  
    /**
   * Listen for new connections and assign accepted connections to processors using round-robin.
   * 轮询监听并分配连接给处理器
   */
  private def acceptNewConnections(): Unit = {
    // 每500毫秒获取一次就绪的IO事件  
    val ready = nioSelector.select(500)
    // 如果存在就绪的IO事件
    if (ready &gt; 0) {
      // 获取selector中的key  
      val keys = nioSelector.selectedKeys()
      val iter = keys.iterator()
      while (iter.hasNext &amp;&amp; isRunning) {
        try {
          val key = iter.next
          iter.remove()

          // 如果key可接受数据
          if (key.isAcceptable) {
            // 针对key创建socket连接  
            accept(key).foreach { socketChannel =&gt;
              // Assign the channel to the next processor (using round-robin) to which the
              // channel can be added without blocking. If newConnections queue is full on
              // all processors, block until the last one is able to accept a connection.
              var retriesLeft = synchronized(processors.length)
              var processor: Processor = null
              do {
                retriesLeft -= 1
                // 指定处理线程processor
                processor = synchronized {
                  // adjust the index (if necessary) and retrieve the processor atomically for
                  // correct behaviour in case the number of processors is reduced dynamically
                  currentProcessorIndex = currentProcessorIndex % processors.length
                  processors(currentProcessorIndex)
                }
                // 递增当前处理器索引
                currentProcessorIndex += 1
              } while (!assignNewConnection(socketChannel, processor, retriesLeft == 0))
            }
          } else
            throw new IllegalStateException(&quot;Unrecognized key state for acceptor thread.&quot;)
        } catch {
          case e: Throwable =&gt; error(&quot;Error while accepting connection&quot;, e)
        }
      }
    }
  }

</code></pre>
<h4 id="processor-线程">Processor 线程</h4>
<p>Processor线程负责核心的逻辑处理。</p>
<p>核心参数说明：</p>
<ul>
<li>newConnections 队列: 是一个阻塞队列，主要用来保存要创建的新连接信息，也就是SocketChannel 对象，其队列长度大小为20(源码中直接硬编码了)。每当 Processor 线程接收到新的连接请求时，都会将对应的 SocketChannel 对象放入队列，等到后面创建连接时，从该队列中获取 SocketChannel，然后注册新的连接。</li>
<li>inflightResponse 队列：是一个临时的 Response 队列， 当 Processor 线程将 Repsonse 返回给 Client 之后，要将 Response 放入该队列。它存在的意义：由于有些 Response 回调逻辑要在 Response 被发送回 Request 发送方后，才能执行，因此需要暂存到临时队列。</li>
<li>ResponseQueue 队列：它主要是存放需要返回给Request 发送方的所有 Response 对象。通过源码得知：每个 Processor 线程都会维护自己的 Response 队列。</li>
</ul>
<pre><code>  // 文件地址：core/src/main/scala/kafka/network/SocketServer.scala
  private val newConnections = new ArrayBlockingQueue[SocketChannel](connectionQueueSize)
  private val inflightResponses = mutable.Map[String, RequestChannel.Response]()
  private val responseQueue = new LinkedBlockingDeque[RequestChannel.Response]()
  
  override def run(): Unit = {
    // 等待processor线程启动完成  
    startupComplete()
    try {
      while (isRunning) {
        try {
          // setup any new connections that have been queued up
          // 配置新的已就绪连接（将newConnections 里的channel取出来注册到selector中）；
          // 为了确保及时处理现有通道的流量和连接关闭的通知，每次迭代处理的连接数是有限的。
          configureNewConnections()
          // register any new responses for writing
          // selector将response通过网络发出，并将其存入inflightResponses 用作后续调用
          processNewResponses()
          // 执行nio poll，获取socketChannel上已就绪的IO事件
          poll()
          // 将接收到的请求放入request阻塞队列中
          processCompletedReceives()
          // 为了inflightResponses 中成功发送的response执行回调逻辑
          processCompletedSends()
          // 将断开连接的response从inflightResponses 中移除
          processDisconnected()
          // 关闭超过配额限制的连接
          closeExcessConnections()
        } catch {
          // We catch all the throwables here to prevent the processor thread from exiting. We do this because
          // letting a processor exit might cause a bigger impact on the broker. This behavior might need to be
          // reviewed if we see an exception that needs the entire broker to stop. Usually the exceptions thrown would
          // be either associated with a specific socket channel or a bad request. These exceptions are caught and
          // processed by the individual methods above which close the failing channel and continue processing other
          // channels. So this catch block should only ever see ControlThrowables.
          case e: Throwable =&gt; processException(&quot;Processor got uncaught exception.&quot;, e)
        }
      }
    } finally {
      debug(s&quot;Closing selector - processor $id&quot;)
      CoreUtils.swallow(closeAll(), this, Level.ERROR)
      shutdownComplete()
    }
  }

</code></pre>
<h4 id="kafkarequesthandler-核心逻辑处理器">KafkaRequestHandler 核心逻辑处理器</h4>
<p>KafkaRequestHandler 是kafka中真正的逻辑处理代码。</p>
<pre><code>// 文件地址：core/src/main/scala/kafka/server/KafkaRequestHandler.scala
// IO线程池
class KafkaRequestHandlerPool(val brokerId: Int,
                              val requestChannel: RequestChannel,
                              val apis: ApiRequestHandler,// 具体逻辑处理器
                              time: Time,
                              numThreads: Int,// 线程数
                              requestHandlerAvgIdleMetricName: String,
                              logAndThreadNamePrefix : String) extends Logging with KafkaMetricsGroup {
   // 可动态扩展
  private val threadPoolSize: AtomicInteger = new AtomicInteger(numThreads)
  /* a meter to track the average free capacity of the request handlers */
  private val aggregateIdleMeter = newMeter(requestHandlerAvgIdleMetricName, &quot;percent&quot;, TimeUnit.NANOSECONDS)

  // 线程数组
  this.logIdent = &quot;[&quot; + logAndThreadNamePrefix + &quot; Kafka Request Handler on Broker &quot; + brokerId + &quot;], &quot;
  val runnables = new mutable.ArrayBuffer[KafkaRequestHandler](numThreads)
  for (i &lt;- 0 until numThreads) {
    createHandler(i)
  }
  
  // 创建IO处理线程，即KafkaRequestHandler
  def createHandler(id: Int): Unit = synchronized {
    runnables += new KafkaRequestHandler(id, brokerId, aggregateIdleMeter, threadPoolSize, requestChannel, apis, time)
    KafkaThread.daemon(logAndThreadNamePrefix + &quot;-kafka-request-handler-&quot; + id, runnables(id)).start()
  }
  
// IO线程  
class KafkaRequestHandler(id: Int,
                          brokerId: Int,
                          val aggregateIdleMeter: Meter,
                          val totalHandlerThreads: AtomicInteger,
                          val requestChannel: RequestChannel,
                          apis: ApiRequestHandler,
                          time: Time) extends Runnable with Logging {
  this.logIdent = s&quot;[Kafka Request Handler $id on Broker $brokerId], &quot;
  private val shutdownComplete = new CountDownLatch(1)
  private val requestLocal = RequestLocal.withThreadConfinedCaching
  @volatile private var stopped = false

  def run(): Unit = {
    while (!stopped) {
      // We use a single meter for aggregate idle percentage for the thread pool.
      // Since meter is calculated as total_recorded_value / time_window and
      // time_window is independent of the number of threads, each recorded idle
      // time should be discounted by # threads.
      val startSelectTime = time.nanoseconds

      // 从requestChannel获取请求
      val req = requestChannel.receiveRequest(300)
      val endTime = time.nanoseconds
      // 统计线程空闲时间
      val idleTime = endTime - startSelectTime
      // 更新线程空闲百分比指标
      aggregateIdleMeter.mark(idleTime / totalHandlerThreads.get)

      req match {
        // 关闭请求  
        case RequestChannel.ShutdownRequest =&gt;
          debug(s&quot;Kafka request handler $id on broker $brokerId received shut down command&quot;)
          completeShutdown()
          return

        // 正常请求
        case request: RequestChannel.Request =&gt;
          try {
            request.requestDequeueTimeNanos = endTime
            trace(s&quot;Kafka request handler $id on broker $brokerId handling request $request&quot;)
            // 真正开始处理相关逻辑
            apis.handle(request, requestLocal)
          } catch {
            case e: FatalExitError =&gt;
              completeShutdown()
              Exit.exit(e.statusCode)
            case e: Throwable =&gt; error(&quot;Exception when handling request&quot;, e)
          } finally {
            request.releaseBuffer()
          }

        case null =&gt; // continue
      }
    }
    completeShutdown()
  }

</code></pre>
<h4 id="完整处理流程">完整处理流程</h4>
<ol>
<li>SocketServer 中的Acceptor 线程接受clients的请求。</li>
<li>Acceptor 线程会创建 NIO Selector 对象及 ServerSocketChannel 实例，并将其与 OP_ACCEPT 事件注册到 Selector 多路复用器上。</li>
<li>与此同时，Acceptor 线程还会创建默认大小为3的 Processor 线程池，（可通过broker端参数：num.network.threads进行修改） 。</li>
<li>Acceptor线程开始轮询监听，如果有新的请求对象 SocketChannel 进入，便会将其放入到连接队列中（newConnections），同时会触发一个processor线程进行处理。</li>
<li>Processor 线程向 SocketChannel 注册 OP_READ/OP_WRITE 事件，同时，轮询获取已就绪的IO事件。</li>
<li>Processor 线程会根据Channel中获取已经完成的 Receive 对象，构建 Request 对象，并将其存入到 Requestchannel 的 RequestQueue 请求队列中 。</li>
<li>KafkaRequestHandler 线程循环地从请求队列中获取 Request 实例，然后交由KafkaApis 的 handle 方法，执行真正的请求处理逻辑，并最终将数据存储到磁盘中。</li>
<li>待处理完请求后，KafkaRequestHandler 线程会将 Response 对象放入 Processor 线程的 Response 队列。</li>
<li>Processor 线程通过 Request 中的 ProcessorID 不停地从 Response 队列中来定位并取出 Response 对象，并返还给 Request 发送方。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka深入理解之可用性与持久性保证]]></title>
        <id>https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-ke-yong-xing-yu-chi-jiu-xing-bao-zheng/</id>
        <link href="https://philosopherzb.github.io/post/kafka-shen-ru-li-jie-zhi-ke-yong-xing-yu-chi-jiu-xing-bao-zheng/">
        </link>
        <updated>2022-06-18T08:44:35.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka可用性与持久化保证。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/lake-71208_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="kafka体系架构">kafka体系架构</h2>
<h3 id="总览">总览</h3>
<p>标准的kafka集群中一般包含多个broker，若干个producer，若干个consumer，以及一个zookeeper集群。</p>
<p>Kafka 中的message以topic为单位进行归类，producer负责将消息发送到特定的topic（发送到 Kafka 集群中的每一条消息都要指定一个topic），而consumer负责订阅topic并进行消费。</p>
<p>topic只是一个逻辑上的概念，每个主题都会被划分为多个partition，partition是实际存在的存储介质，消息在单个partition中有序。</p>
<p>架构图如下所示：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315004.png" alt="img" loading="lazy"></figure>
<h3 id="消息传递语义message-delivery-semantics">消息传递语义（Message Delivery Semantics）</h3>
<p>关于producer与consumer之间消息传递保证，kafka提供了如下三种语义（需要注意的是，此过程分为发布消息的持久化保证及消费消息的保证）：</p>
<ul>
<li>At most once：消息可能会丢失，但绝不会重传。</li>
<li>At least once：消息绝不会丢失，但可能会重传。</li>
<li>Exactly once：每条消息会传递一次且仅有一次，这正是用户真正所需要的。</li>
</ul>
<h4 id="发送消息持久化保证">发送消息持久化保证</h4>
<p>针对发送消息的持久化保证：在 kafka-0.11.0.0 之前，如果生产者没有收到表明消息已提交的响应，那么消息将会被重发，这仅提供了At least once传递语义，因为如果原始请求实际上已经成功，则在重新发送期间消息可能会再次写入日志。</p>
<p>在 kafka-0.11.0.0 之后，Kafka 生产者开始支持幂等交付选项，以保证重新发送不会导致日志中出现重复条目。 为实现这个目标，broker会为每个生产者分配一个 ID，并使用生产者与每条消息一起发送的序列号对消息进行去重操作。</p>
<p>同样从 kafka-0.11.0.0 开始，生产者支持使用类似事务(transaction-like)的语义将消息发送到多个主题分区的能力；即，所有消息都已成功写入，或者都没有（原子性）。</p>
<h4 id="消费消息的保证">消费消息的保证</h4>
<p>关于consumer读取消息时，该如何处理消息和更新位置的几个场景如下：</p>
<ol>
<li>consumer可以读取消息，然后将其位置保存在日志中，最后处理消息。在这种情况下，消费者进程可能会在保存其位置之后但在保存其消息处理的输出之前崩溃，而接管处理的进程却会从保存的位置开始，即使该位置之前的一些消息尚未处理。 这对应于“At most once”语义，因为在消费者失败的情况下，消息可能不会被处理。</li>
<li>consumer可以读取消息，处理消息，最后保存它的位置。 在这种情况下，消费者进程可能会在处理消息之后但在保存其位置之前崩溃；这样的话，当一个新进程接管它收到的前几条消息时，这些消息实际已经被处理了。这对应于消费者失败情况下的“At least once”语义。 在许多情况下，消息有一个主键，因此更新是幂等的（两次接收相同的消息只会用另一个自身的副本覆盖记录）。</li>
<li>关于如何保证“Exactly once”，当从 Kafka 主题消费并生产到另一个主题时（如在 Kafka Streams 应用程序中），可以利用上面提到的 kafka-0.11.0.0 中新的事务生产者功能。消费者的位置作为消息存储在主题中，与此同时，在和接收处理数据的输出主题相同的事务中将偏移量写入 Kafka。 如果传输中止，消费者的位置将恢复到其旧值，并且输出主题的生成数据将不会对其他消费者可见，当然，这具体取决于他们的“隔离级别”。 在默认的“read_uncommitted”隔离级别中，所有消息对消费者都是可见的，即使它们是中止事务的一部分，但在“read_committed”中，消费者将只返回来自已提交事务的消息（以及任何非事务消息）。</li>
<li>写入外部系统时，限制在于需要将消费者的位置与实际存储为输出的内容相协调。实现这一点的经典方法是在存储消费者位置和输出之间引入两阶段提交。但是通过让消费者将其偏移量与其输出存储在同一位置可以让其中的处理更简单与更通用化。这种方式是比较友好的，因为消费者可能想要写入的许多输出系统不支持两阶段提交。举个例子，Kafka Connect 连接器，它将所读取的数据和数据的 offset 一起写入到 HDFS，以保证数据和 offset 都被更新，或者两者都不被更新。</li>
</ol>
<h3 id="副本replication">副本（Replication）</h3>
<p>kafka为每个主题的分区提供了备份副本功能（可在服务端进行相关配置）；当集群中的某个服务宕机时，副本能够自动进行故障转移，以保证数据的可用性（不丢失，提供容灾能力）。</p>
<p>创建副本的单位是 topic 的 partition ，正常情况下，每个分区都有一个 leader 和零或多个 followers 。总的副本数包含 leader及followers，被称为副本因子。所有的读写操作都由 leader 处理。一般 partition 的数量都比 broker 的数量多的多，各分区的 leader 均匀的分布在 brokers 中。所有的 followers 节点都同步 leader 节点的日志，日志中的消息和偏移量都和 leader 中的一致。（当然，在任何给定时间，leader 节点的日志末尾时可能有几个消息尚未被备份完成）。</p>
<p>如下图所示，Kafka 集群中有4个 broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个 leader 副本和2个 follower 副本。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315005.png" alt="img" loading="lazy"></figure>
<h3 id="选举算法leader-election">选举算法（Leader Election）</h3>
<p>kafka分区的核心便是副本日志（replicated log），这同样也是分布式系统中最重要的基础元素之一（容灾）。</p>
<p>副本日志按照一系列有序的值（通常是编号为 0、1、2、…) 进行建模。有很多方法可以实现这一点，但最简单和最快的方法是由 leader 节点选择需要提供的有序的值，只要 leader 节点还存活，所有的 follower 只需要拷贝数据并按照 leader 节点的顺序排序。</p>
<p>理想情况中，如果leader永远存活，那么也就不需要follower了；然而实际生产环境中依旧可能出现机器宕机的场景（随着系统越大，机器越多，概率也将越高）；因此，为了应对这种情形，需要从存活的followers中选择一个新的leader，但是followers本身数据可能会落后于leader或者crash掉，在这种情况下，有必要选择数据最全的follower（up-to-date follower）作为新的leader。</p>
<p>在这个过程中，有一个基本原则：一条数据如果明确返回给客户端为已提交（committed），则当leader crash掉后出现的新leader必须拥有刚刚已提交过的所有消息。这时候便需要做出权衡：如果leader在表明一条消息已提交（committed）前等待更多的follower进行确认，那么leader宕机后便有更多的follower可以作为新的leader，但与此同时吞吐率也会有所下降。</p>
<p>一种非常常见的leader election算法便是“Majority Vote”。在这种模式下，假设拥有2f+1个副本（包含leader和follower），如果需要确保f+1个副本收到消息，且在这f+1个副本中选择一个作为新的leader，那么fail的副本不能超过f个。这是因为在f+1个副本中，任意一个都拥有最新的所有备份数据。</p>
<p>“Majority Vote”有一个非常好的特性：那就是系统的延迟只取决于最快的服务器；当然其劣势便是需要更多的副本来保证数据的完整性（所能允许fail的副本相对太少）。例如：如果要容忍1个follower宕机，那便需要3个及以上副本；如果要容忍2个follower宕机，那便需要5个及以上副本；也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降，且这种方式对于资源也过于浪费，毕竟生产资源很珍贵。一般情况下，这种算法用于Zookeeper之类的共享集群配置的系统中而很少在需要存储大量数据的系统中使用。</p>
<p>实际上，Leader Election算法非常多，比如Zookeeper的Zab, Raft和Viewstamped Replication。而Kafka所使用的Leader Election算法更像微软的PacificA算法。</p>
<p>kafka采用的选举算法与“Majority Vote”有所不同，它会在zookeeper中动态的维护一组ISR，这些副本基本与leader保持一致（未同步的副本将会被踢出ISR），一般情况下只有ISR中的副本才有机会成为新的leader。</p>
<p>在这种模式下，对于f+1个副本，一个分区能在保证不丢失已经committed的消息的前提下容忍f个副本发生故障。在大多数应用场景中，这种模式是非常有利的。而在实际中，为了冗余f个副本发生故障，Majority Vote和ISR在commit前需要等待的Replica数量是一样的（例如在一次故障恢复中，“Majority Vote”的 quorum 需要三个备份节点和一次确认，而ISR 需要两个备份节点和一次确认），但是ISR需要的总副本数几乎是Majority Vote的一半。</p>
<p>关于kafka另一个重要的特性设计便是其并不需要崩溃节点在拥有完好无损的数据下进行恢复，简而言之，kafka允许丢失部分数据（数据不一致性）。</p>
<p>关于分布式系统中可用性与一致性之间的权衡，往往取决于实际的业务场景需求（如资金类业务需要强一致性，用户活动跟踪只需保证可用性等）。</p>
<p>当kafka中的发生了all replica die时，它提供了两种恢复性方案（可用性与一致性抉择，不止kafka有此等困境，其余框架一样会遇到这种艰难的选择）：</p>
<ol>
<li>
<p>等待 ISR 中的副本恢复并选择此副本作为新的leader（希望它仍然拥有所有数据）。</p>
</li>
<li>
<p>选择第一个恢复过来的副本作为leader（不一定在 ISR 中）。</p>
<p>第一种方案中，如果等待 ISR 中副本恢复，那么只要这些副本一直处于宕机状态，kafka将始终不可用。如果ISR副本被破坏或它们的数据丢失，kafka便彻底宕机了。</p>
<p>第二种方案中，如果一个非同步副本从宕机状态中恢复并且允许它成为leader，那么它的数据将成为新的可信任来源，即使它不能保证拥有每条已提交的消息。</p>
<p>默认情况下，从 kafka-0.11.0.0 版本开始，Kafka 选择第一个策略并倾向于等待一致的副本。可以使用配置属性 unclean.leader.election.enable 更改此行为，以支持正常运行时间优于一致性的用例。</p>
</li>
</ol>
<h3 id="节点存活与ack机制可用性与持久性">节点存活与ack机制（可用性与持久性）</h3>
<p>kafka为节点定义了一个 “in sync” 状态，区别于 “alive” 和 “failed” 。Leader 会追踪所有 “in sync” 的节点。如果有节点挂掉了，或是写超时，或是心跳超时，leader 就会把它从同步副本列表（in sync replicas，缩写为ISR）中移除。同步超时和写超时的时间由 replica.lag.time.max.ms 配置确定。</p>
<p>kafka定义节点处于“in sync”状态的的条件如下：</p>
<ul>
<li>节点必须维护和 ZooKeeper 的会话连接，Zookeeper 可以通过心跳机制来检查每个节点的连接。</li>
<li>如果是 follower 节点，它必须能及时的同步 leader 的写操作，并且延时不能太久。</li>
</ul>
<p>分布式系统中，kafka只尝试处理 “fail/recover” 模式的故障，即节点突然停止工作，然后又恢复（节点可能不知道自己曾经挂掉）的状况。Kafka 没有处理所谓的 “Byzantine” 故障，即一个节点出现了随意响应和恶意响应（可能由于 bug 或 非法操作导致）。</p>
<p>当分区上的所有ISR都将消息保存至log中时，该消息可以被视为已提交，只有已提交的消息才会被消费者所消费。这意味着消费者不必担心当leader fail时，可能会看到丢失的消息。</p>
<p>另一方面，producer可以选择等待或不提交消息，这取决于他们对延迟和持久性之间的权衡，此功能由producer中的ACK机制实现。请注意，主题具有ISR的“最小数量”设置，当生产者请求确认消息已写入完整的ISR集时，会检查该设置。如果生产者请求不那么严格的确认，则可以提交和消费消息，即使同步副本的数量低于最小值（例如，它可以低至仅leader）。</p>
<p>在任何时候，只要至少有一个同步中的节点存活，kafka中的消息就不会丢失；这保证了在短暂的故障转移后，kafka仍具有可用性，不过在网络分区的场景中，可能无法保持可用。</p>
<p>关于ack机制如下：</p>
<ul>
<li>acks = 0，如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障）。此配置可以获得最高的吞吐量。</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315006.png" alt="img" loading="lazy"></figure>
<ul>
<li>acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应；一旦消息无法写入leader分区副本(比如网络原因、leader节点崩溃),生产者会收到一个错误响应，当生产者接收到该错误响应之后，为了避免数据丢失，会尝试重新发送数据（前提是配置了重试次数），这种方式的吞吐量取决于使用的是异步发送还是同步发送。在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录依旧会丢失。</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315007.png" alt="img" loading="lazy"></figure>
<ul>
<li>acks = all 这意味着leader将等待完整的同步副本（ISR）集以确认记录（此时如果ISR同步副本的个数小于min.insync.replicas的值，消息将不会被写入.），这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，相当于acks = -1的设置。此配置最安全，但延迟相对来说也最高。</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315008.png" alt="img" loading="lazy"></figure>
<p>关于最小同步副本配置：</p>
<ul>
<li>Kafka的Broker端提供了一个参数min.insync.replicas，它明确指定了数据要被同步到多少个副本才算真正的成功写入，此参数一般与acks=all配合使用，可以获得更好的持久性保证。该值默认为1，生产环境设定为一个大于1的值可以提升消息的持久性（例如复制因子为3则可以设置该参数为2）。 因为如果同步副本的数量低于该配置值，则生产者会收到错误响应，从而确保消息不丢失。</li>
<li>当min.insync.replicas=2且acks=all时，如果此时ISR列表只有[1,2],3被踢出ISR列表，只需要保证两个副本同步了，生产者就会收到成功响应。</li>
</ul>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315009.png" alt="img" loading="lazy"></figure>
<ul>
<li>当min.insync.replicas=2，如果此时ISR列表只有[1],2和3被踢出ISR列表，那么当acks=all时，则不能成功写入数；当acks=0或者acks=1可以成功写入数据。</li>
</ul>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315010.png" alt="img" loading="lazy"></figure>
<ul>
<li>如果acks=all且min.insync.replicas=2，此时ISR列表为[1,2,3]，这种情况下kafka还是会等到所有的同步副本都同步了消息，才会向生产者发送成功响应的ack。因为min.insync.replicas=2只是一个最低限制，即同步副本少于该配置值，才会抛异常，而acks=all，是需要保证所有的ISR列表的副本都同步了才可以发送成功响应。</li>
</ul>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315011.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[消息队列与kafka]]></title>
        <id>https://philosopherzb.github.io/post/xiao-xi-dui-lie-yu-kafka/</id>
        <link href="https://philosopherzb.github.io/post/xiao-xi-dui-lie-yu-kafka/">
        </link>
        <updated>2022-06-11T05:32:03.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述消息队列以及kafka整体架构。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/mountain-547363_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="消息队列message-queue">消息队列(message queue)</h2>
<h3 id="简介">简介</h3>
<p>消息队列是一种进程间通信或同一进程的不同线程间的通信方式；针对如今微服务或云架构，则指代服务间的异步通信方式。</p>
<p>在现代云架构或微服务架构中，应用程序被分解为多个规模较小且易于开发、部署和维护的构建块。消息队列可以为这些分布式应用程序提供通信与协调；同时，消息队列也可以显著的简化分离应用编码，并提高性能、可靠性和可扩展性等。</p>
<h3 id="消息投递模式">消息投递模式</h3>
<h4 id="端到端point-to-point">端到端(Point-to-point)</h4>
<p>在端到端消息模式中，一条消息将会一直存储在队列中，直到被消费者所接收；且，发送者必须要知道消费者的一些关键信息，例如：将要发送消息的队列名或者特定的队列管理器名。</p>
<p>此模式中，消息队列将会提供一个临时存储消息的轻量级缓冲区，并允许微服务连接到队列以发送和接收消息的终端节点；这种情况下，消息的规模一般较小，如请求，恢复，错误消息及明文消息等；同时一条消息只能由一个接收者处理一次。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314008.png" alt="img" loading="lazy"></figure>
<h4 id="发布订阅publishsubscribe">发布订阅(Publish/Subscribe)</h4>
<p>发布订阅模式中，一条消息由发送者推送至特定的主题(topic)，随后所有订阅了该topic的消费者都将收到同一条广播消息。</p>
<p>消息主题的订阅者通常会执行不同的功能，并可以同时对消息执行不同的操作。发布者无需知道谁在使用广播的信息，而订阅者也无需知道消息来自哪里。这种消息收发模式与端到端稍有不同，在端到端中，发送消息的组件通常知道发送的目的地。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314009.png" alt="img" loading="lazy"></figure>
<h3 id="消息队列优势">消息队列优势</h3>
<ul>
<li>解耦：在分布式系统中，同一份数据可能要分发给不同的程序进行处理；例如订单生成后，要扣库存，加积分；此时如果由订单服务直接调用库存服务及积分服务的接口，将会让订单服务变得十分臃肿，且不利于后续扩展（比如再加一个支付服务）。这时便可以加入消息队列，采用发布订阅模式，订单服务只需将消息发送至指定topic即可，其他服务选择性订阅。这样便将系统业务进行了解耦操作，增加了系统的可扩展性。</li>
<li>异步通信：消息队列天然支持异步操作（所有的消息都会存储在队列中，消费者可以延迟处理），针对分布式系统中的异步操作，可采用消息队列进行处理。</li>
<li>削峰填谷：针对高并发大数据的场景，消息队列可以有效地做到削峰填谷。数据推送高峰时，将由消息队列暂存所有的数据，等到数据推送低谷时，由消费者逐步消费所有的数据。必要时，也可以额外的增加消费者进行数据处理。</li>
<li>缓冲：消息队列通过一个缓冲层来帮助任务最高效率的执行；写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。</li>
<li>跨平台：消息队列支持跨平台消息处理，只需要发送者及消费者约定数据格式即可。例如，由Java发送消息，go消费消息。</li>
<li>灵活性与可扩展性：在高流量期间，可以通过动态的扩展机器来接受更多的消息，防止高并发冲击服务，导致服务挂掉。</li>
</ul>
<h2 id="kafka">kafka</h2>
<h3 id="简介-2">简介</h3>
<p>Kafka 是由 Linkedin 公司开发的，它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的消息流平台，它同时也是一款开源的基于发布订阅模式的消息引擎系统；</p>
<p>Kafka由服务端及客户端组成，且服务端和客户端可以通过高性能的TCP网络协议进行通讯。关于kafka的部署环境，无论本地还是云环境中的裸机，虚拟机或者容器都可以支持。</p>
<h3 id="基本术语">基本术语</h3>
<ul>
<li>
<p>生产者(Producer)：向kafka中的主题(topic)发布消息事件的客户端应用程序被称为生产者。</p>
</li>
<li>
<p>消费者(Consumer)：订阅了kafka消息事件所在的主题(topic)的客户端应用程序称为消费者。</p>
</li>
<li>
<p>主题(Topic)：用于存储同一类消息事件；打个简单的比喻：主题类似于文件系统中的文件夹，而消息事件则类似于文件夹中的文件。</p>
</li>
<li>
<p>消息(Message)：kafka中的数据单元被称为消息，也可以叫记录(record)。</p>
</li>
<li>
<p>偏移量(Offset)：是一种元数据，且是一个不断递增的整数值；当消费者处理完消息后，将会提交offset+1到broker中。</p>
</li>
<li>
<p>broker： 一个独立的 Kafka 服务器就被称为 broker，broker 接收来自生产者的消息，并为消息设置偏移量，同时持久化消息到磁盘。</p>
</li>
<li>
<p>分区(Partition)：在一个主题中可以存在一个或多个分区，每个分区中消息有序；同一主题中的多个分区可以位于不同机器上，方便后续扩容操作。</p>
</li>
<li>
<p>副本(Replica)：消息的备份称为副本，在创建主题时可以指定副本数量。</p>
</li>
<li>
<p>重平衡(Rebalance)：当消费组(多个消费者将会形成一个消费组)中的某个消费者实例挂掉或者新增一个消费者实例时，其他消费者实例可以自动地重新分配订阅主题，这个过程叫重平衡。</p>
</li>
<li>
<p>AR与ISR：AR即可分配副本 Assigned Replicas；而所有与leader副本保持一定同步状态的副本（包含leader副本）构成ISR（In-Sync Replicas）；ISR集合是AR集合中的一个子集。</p>
</li>
<li>
<p>ISR的伸缩：leader 副本负责维护和跟踪 ISR 集合中所有 follower 副本的滞后状态，当 follower 副本落后太多或失效时，leader 副本会把它从 ISR 集合中剔除。如果 OSR（Out-Sync Replicas） 集合中有 follower 副本“追上”了 leader 副本，那么 leader 副本会把它从 OSR 集合转移至 ISR 集合。默认情况下，当 leader 副本发生故障时，只有在 ISR 集合中的副本才有资格被选举为新的 leader，而在 OSR 集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变）。</p>
<p>replica.lag.time.max.ms ： 这个参数的含义是 Follower 副本能够落后 Leader 副本的最长时间间隔，默认10s。</p>
<p>unclean.leader.election.enable：是否允许 Unclean 领导者选举。开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。</p>
</li>
<li>
<p>HW与LW：HW（High Watermark）俗称高水位，它标识了一个特定的偏移量(offset)，消费者只能拉取此偏移量之前的数据；LW（Low Watermark）俗称低水位，它同样标识了一个特定的偏移量(offset)，一般为AR集合中最小的LSO值；LW值的增长与副本的拉取或删除请求息息相关。</p>
</li>
<li>
<p>LSO：LSO 是LogStartOffset的缩写，一般情况下，日志文件的起始偏移量 logStartOffset 等于第一个日志分段的 baseOffset，但这并不是绝对的，logStartOffset 的值可以通过 DeleteRecordsRequest 请求(比如使用 KafkaAdminClient 的 deleteRecords()方法、使用 kafka-delete-records.sh 脚本、日志的清理和截断等操作）进行修改。</p>
</li>
<li>
<p>LEO：LEO是 LogEndOffset 的缩写，它标识当前日志文件中下一条待写入消息的 offset，如下图中 offset 为9的位置即为当前日志文件的 LEO，LEO 的大小相当于当前日志分区中最后一条消息的 offset 值加1。分区 ISR 集合中的每个副本都会维护自身的 LEO，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息</p>
</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314010.png" alt="img" loading="lazy"></figure>
<ul>
<li>发送者，broker集群，消费者基本协作结构图</li>
</ul>
<p><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314011.png" alt="img" loading="lazy"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314011.png" alt="" loading="lazy"></p>
<ul>
<li>主题剖析结构图</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315001.png" alt="img" loading="lazy"></figure>
<ul>
<li>一个两节点的 kafka 集群支持的 2 个消费组的四个分区 (P0-P3)。消费者 A 有两个消费者实例，消费者 B 有四个消费者实例。</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315002.png" alt="img" loading="lazy"></figure>
<h3 id="基本特性">基本特性</h3>
<ul>
<li>高吞吐、低延迟：收发消息快是kafka最重要的特性之一，即使在非常廉价的机器上，kafka也可以轻松达到每秒几十万条消息的传输速率，且此过程中最低延迟仅有几毫秒；一般可以用来做实时日志聚合。</li>
<li>持久化：kafka中的所有消息都会被其持久化存储在文件系统中；在这个过程中，kafka采用顺序写磁盘及直接写页缓存的机制提高IO效率（顺序写磁盘的速度接近于随机写内存的速度：<a href="https://queue.acm.org/detail.cfm?id=1563874">点击此处跳转文章页面</a>）。</li>
<li>水平扩容(Scale out)：kafka提供分区机制以达到动态扩容的效果，即增加分区数（不支持减少分区操作）；</li>
<li>分区容错(Partition-tolerance)：即使集群中的某个节点挂掉，kafka仍然可以提供可用性（可用性与一致性间的冲突，kafka目前采用的是ISR机制进行了部分妥协）。</li>
<li>核心API：Producer API，Consumer API，Streams API，Connect API，Admin API</li>
</ul>
<h3 id="生产者producer">生产者(Producer)</h3>
<h4 id="简介-3">简介</h4>
<p>生产者无需经过路由便可将消息发送至主分区所在的服务器上；为了实现这个功能，所有的kafka服务器节点都能够响应这样的元数据请求：哪些服务器存活，主题的主分区位于哪台服务器上。</p>
<p>对于生产者而言，只需要执行发送请求，消息将会自动根据路由规则分配到不同的分区上。</p>
<p>消息路由规则：如果指定了 partition，则直接使用；如果未指定 partition 但指定了 key，则通过对 key 的 value 进行hash 选出一个 partition（形如：Math.abs(key.hashCode()) % partitions.size()）； 如果partition 和 key 都未指定，便会轮询选出一个 partition。</p>
<h4 id="整体流程架构">整体流程架构</h4>
<p>整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。</p>
<p>在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。</p>
<p>Sender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。</p>
<p>RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230315003.png" alt="img" loading="lazy"></figure>
<h5 id="序列化器">序列化器</h5>
<p>生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka；在消费者侧同样需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换成相应的对象。网络传输过程中，序列化是必须的操作。自定义时需要实现org.apache.kafka.common.serialization.Serializer</p>
<pre><code>package org.apache.kafka.common.serialization;

import org.apache.kafka.common.header.Headers;

import java.io.Closeable;
import java.util.Map;

/**
 * 一个用于转换对象为字节的接口
 *
 * 实现此接口的类应具有不带参数的构造函数
 * 
 * @param &lt;T&gt; Type to be serialized from.
 */
public interface Serializer&lt;T&gt; extends Closeable {

    /**
     * 配置当前类，此方法一般是在创建 KafkaProducer 实例的时候调用的，主要用来确定编码类型。
     * @param configs configs in key/value pairs
     * @param isKey whether is for key or value
     */
    default void configure(Map&lt;String, ?&gt; configs, boolean isKey) {
        // intentionally left blank
    }

    /**
     * 转换数据为字节数组（执行序列化操作）
     * 可以进行编解码，如果 Kafka 客户端提供的几种序列化器都无法满足应用需求，
     * 则可以选择使用如 Avro、JSON、Thrift、ProtoBuf 和 Protostuff 等通用的序列化工具来实现，
     * 或者使用自定义类型的序列化器来实现。  
     *
     * @param topic topic associated with data
     * @param data typed data
     * @return serialized bytes
     */
    byte[] serialize(String topic, T data);

    /**
     * 转换数据为字节数组（执行序列化操作）
     * 可以进行编解码，如果 Kafka 客户端提供的几种序列化器都无法满足应用需求，
     * 则可以选择使用如 Avro、JSON、Thrift、ProtoBuf 和 Protostuff 等通用的序列化工具来实现，
     * 或者使用自定义类型的序列化器来实现。  
     *
     * @param topic topic associated with data
     * @param headers headers associated with the record
     * @param data typed data
     * @return serialized bytes
     */
    default byte[] serialize(String topic, Headers headers, T data) {
        return serialize(topic, data);
    }

    /**
     * 关闭序列化器
     * 注意：此方法必须是幂等的，因为它可能会被多次调用。
     */
    @Override
    default void close() {
        // intentionally left blank
    }
}

</code></pre>
<h5 id="分区器">分区器</h5>
<p>分区器的作用就是为消息分配partition。如果消息 ProducerRecord 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值；或者依赖轮询分配partition（可参考消息路由规则）。Kafka 中提供的默认分区器是 org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了 org.apache.kafka.clients.producer.Partitioner 接口。</p>
<pre><code>package org.apache.kafka.clients.producer.internals;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.utils.Utils;

import java.util.Map;

/**
 * 默认的分区策略器
 * &lt;ul&gt;
 * &lt;li&gt;如果记录中指定了分区，则直接使用
 * &lt;li&gt;如果未制定分区，但存在key，则基于key的hash值选择一个分区（拥有相同 key 的消息会被写入同一个分区，前提是分区数不会变更）
 * &lt;li&gt;如果分区和key都不存在，则选择在批处理已满时更改的粘性分区（轮询操作）。
 * 
 * See KIP-480 for details about sticky partitioning.
 */
public class DefaultPartitioner implements Partitioner {

    private final StickyPartitionCache stickyPartitionCache = new StickyPartitionCache();

    public void configure(Map&lt;String, ?&gt; configs) {}

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param key The key to partition on (or null if no key)
     * @param keyBytes serialized key to partition on (or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        return partition(topic, key, keyBytes, value, valueBytes, cluster, cluster.partitionsForTopic(topic).size());
    }

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param numPartitions The number of partitions of the given {@code topic}
     * @param key The key to partition on (or null if no key)
     * @param keyBytes serialized key to partition on (or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster,
                         int numPartitions) {
        if (keyBytes == null) {
            return stickyPartitionCache.partition(topic, cluster);
        }
        // 基于key 的hash值选择一个翻去
        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
    }

    public void close() {}
  
    /**
     * 轮询选择分区
     */
    public void onNewBatch(String topic, Cluster cluster, int prevPartition) {
        stickyPartitionCache.nextPartition(topic, cluster, prevPartition);
    }
}

</code></pre>
<pre><code>package org.apache.kafka.clients.producer;

import org.apache.kafka.common.Configurable;
import org.apache.kafka.common.Cluster;

import java.io.Closeable;

/**
 * 分区器接口，自定义分区器时，需要实现此接口。
 */
public interface Partitioner extends Configurable, Closeable {

    /**
     * 计算将要发送消息的分区
     *
     * @param topic The topic name
     * @param key The key to partition on (or null if no key)
     * @param keyBytes The serialized key to partition on( or null if no key)
     * @param value The value to partition on or null
     * @param valueBytes The serialized value to partition on or null
     * @param cluster The current cluster metadata
     */
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);

    /**
     * 关闭分区器时执行相关操作
     */
    public void close();


    /**
     * 通知分区器有一个已创建的新批处理数据。当时用粘性分区器时，次方法会自动为新批处理数据选择一个粘性分区。
     * @param topic The topic name
     * @param cluster The current cluster metadata
     * @param prevPartition The partition previously selected for the record that triggered a new batch
     */
    default public void onNewBatch(String topic, Cluster cluster, int prevPartition) {
    }
}

</code></pre>
<h5 id="拦截器">拦截器</h5>
<p>生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。消费者拦截器主要在消费到消息或在提交消费位移时进行一些定制化的操作。自定义时需要实现org.apache.kafka.clients.producer. ProducerInterceptor</p>
<pre><code>package org.apache.kafka.clients.producer;

import org.apache.kafka.common.Configurable;

/**
 * 一个插件接口，允许拦截（并可能改变）生产者发布到kafka集群中的记记录
*/
public interface ProducerInterceptor&lt;K, V&gt; extends Configurable {
    /**
     * 将消息序列化及分区（如果分区未指定）前对消息进行相关定制化处理。
     * @param record the record from client or the record returned by the previous interceptor in the chain of interceptors.
     * @return producer record to send to topic/partition
     */
    public ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; record);

    /**
     * 当发送到服务器的记录已被确认，或者在发送到服务器之前发送记录失败时调用此方法。
     * 此方法通常在调用用户callback之前调用
     * 此方法通常运行在 Producer 的 background I/O线程中，因此这个方法中的实现代码逻辑越简单越好，否则会影响消息的发送速度。
     * 
     * This method will generally execute in the background I/O thread, so the implementation should be reasonably fast.
     * Otherwise, sending of messages from other threads could be delayed.
     *
     * @param metadata The metadata for the record that was sent (i.e. the partition and offset).
     *                 If an error occurred, metadata will contain only valid topic and maybe
     *                 partition. If partition is not given in ProducerRecord and an error occurs
     *                 before partition gets assigned, then partition will be set to RecordMetadata.NO_PARTITION.
     *                 The metadata may be null if the client passed null record to
     *                 {@link org.apache.kafka.clients.producer.KafkaProducer#send(ProducerRecord)}.
     * @param exception The exception thrown during processing of this record. Null if no error occurred.
     */
    public void onAcknowledgement(RecordMetadata metadata, Exception exception);

    /**
     * 用于关闭拦截器时执行逻辑，例如清理相关资源等
     */
    public void close();
}

</code></pre>
<h5 id="处理顺序">处理顺序</h5>
<p>拦截器-&gt;序列化器-&gt;分区器；KafkaProducer 在将消息序列化和计算分区之前会调用生产者拦截器的 onSend() 方法来对消息进行相应的定制化操作。然后生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka。最后可能会被发往分区器为消息分配分区。</p>
<h3 id="消费者consumer">消费者(Consumer)</h3>
<h4 id="简介-4">简介</h4>
<p>kafka consumer 订阅感兴趣的主题，同时向其所在的主分区发送fetch请求，获取需要进行消费的消息；需要注意的是，此过程中，consumer的每个请求都需要在partition中指定offset，从而消费从offset处开始的message。因此，consumer对offset的控制便尤为重要，可以依此来进行回退重消费操作。</p>
<h4 id="关于offset">关于offset</h4>
<p>大多数消息系统都在 broker 上保存被消费消息的元数据。也就是说，当消息被传递给 consumer，broker 要么立即在本地记录该事件，要么等待 consumer 的确认后再记录。这是一种相当直接的选择，而且事实上对于单机服务器来说，也没其它地方能够存储这些状态信息。</p>
<p>由于大多数消息系统用于存储的数据结构规模都很小，所以这也是一个很实用的选择，因为只要 broker 知道哪些消息被消费了，就可以在本地立即进行删除，一直保持较小的数据量。</p>
<p>然而，此过程中要一直保持broker与consumer的数据一致性不是一件容易的事；例如consumer已消费了数据，但是回执给broker时出现网络错误导致broker没有删除本地记录，这时消息便有可能会重复消费；并且这种记录操作对于broker而言也是一个不小的性能负担（首先对其加锁，确保该消息只被发送一次，然后将其永久的标记为 consumed，以便将其移除）。</p>
<p>为了应对上述问题，Kafka 使用了完全不同的方式来解决消息丢失问题。</p>
<p>首先Kafka 的 topic 被分割成了一组完全有序的 partition，其中每一个 partition 在任意给定的时间内只能被每个订阅了这个 topic 的 consumer group中的一个 consumer 消费。这意味着 partition 中 每一个 consumer 的位置仅仅是一个数字，即下一条要消费的消息的 offset。这使得被消费的消息的状态信息相当少，每个 partition 只需要一个数字。这个状态信息还可以作为周期性的 checkpoint。这以非常低的代价实现了和消息确认机制等同的效果。</p>
<p>这种方式还有一个附加的好处，那就是consumer 可以回退到之前的 offset 来再次消费之前的数据，这个操作违反了队列的基本原则，但事实证明对大多数 consumer 来说这是一个必不可少的特性。 例如，如果 consumer 的代码有 bug，并且在 bug 被发现前已经有一部分数据被消费了，那么 consumer 可以在 bug 修复后通过回退到之前的 offset 来再次消费这些数据。需要注意的是，对于重复消费的消息要保持幂等操作。</p>
<h4 id="push-vs-pull">Push vs Pull</h4>
<p>关于消息的推送拉取，kafka采用的是：Producer push数据到broker，Consumer从broker pull数据。针对消费者而言，无论是pull-based还是push-based都有各自的优缺点。</p>
<p>pull-based：数据传输速率由Consumer控制，可防止服务因大量数据冲击而宕机，同时也简化了broker的设计；且，此模式中，Producer可以达到最大化量产消息。当然，其缺点就是如果 broker 中没有数据，consumer 可能会在一个紧密的循环中结束轮询，实际上却是在忙于等待数据的到达。为了避免 busy-waiting，kafka在 pull 请求中加入参数，使得 consumer 在一个“long pull”中阻塞等待，直到数据到来（还可以选择等待给定字节长度的数据来确保传输长度）。</p>
<p>push-based：消息能最快的被指定消费者所消费，但是相应的broker设计将会更复杂，且数据传输速率过大时，容易冲击消费者服务，导致其宕机无法提供服务。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka安装&使用&配置]]></title>
        <id>https://philosopherzb.github.io/post/kafka-an-zhuang-andshi-yong-andpei-zhi/</id>
        <link href="https://philosopherzb.github.io/post/kafka-an-zhuang-andshi-yong-andpei-zhi/">
        </link>
        <updated>2022-06-04T08:42:31.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述kafka安装，使用以及spring-kafka配置信息。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/lake-6278825_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="win10下部署kafka">Win10下部署kafka</h2>
<p>主要涉及Java，zookeeper，kafka，步骤如下。</p>
<h3 id="java安装">Java安装</h3>
<p>JDK下载：<a href="https://www.oracle.com/java/technologies/javase-downloads.html">点此此处跳转官网下载页面</a></p>
<p>安装过程比较简单，基本都是下一步即可，唯一需要注意的是环境变量的设置；安装完后效果如下图所示（注意：Java安装路径不要太深（目录过多），同时文件夹命名不要存在空格，否则将导致kafka部署失败）</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314001.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314002.png" alt="img" loading="lazy"></figure>
<h3 id="zookeeper安装">zookeeper安装</h3>
<p>下载地址：<a href="https://zookeeper.apache.org/releases.html">点击此处跳转官网下载页面</a></p>
<p>找一个稳定的版本下载即可：</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314003.png" alt="img" loading="lazy"></figure>
<ol>
<li>下载后解压到一个目录：eg: D:\Program\zookeeper\zookeeper-3.4.14</li>
<li>在zookeeper-3.4.14目录下，新建两个文件夹，并命名(eg: data,log)，(路径：D:\Program\zookeeper\zookeeper-3.4.14\data,D:\Program\zookeeper\zookeeper-3.4.14\log)</li>
<li>进入Zookeeper设置目录，eg: D:\Program\zookeeper\zookeeper-3.4.14\conf；复制“zoo_sample.cfg”副本并将副本重命名为“zoo.cfg”；在任意文本编辑器（eg：记事本）中打开zoo.cfg；找到并编辑dataDir=D:\Program\zookeeper\zookeeper-3.4.14\data；dataLogDir=D:\Program\zookeeper\zookeeper-3.4.14\log</li>
<li>进入D:\Program\zookeeper\zookeeper-3.4.14\bin目录，双击zkServer.cmd即可运行zk。</li>
</ol>
<h3 id="kafka安装">kafka安装</h3>
<p>下载地址：<a href="http://kafka.apache.org/downloads.html">点击此处跳转官网下载页面</a></p>
<p>2.8之后kafka支持不依赖zookeeper启动：<a href="https://kafka.apache.org/quickstart">点击此处跳转详细说明页面</a></p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314004.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>下载后解压缩。eg: D:\Program\kafka_2.13-2.7.0</p>
</li>
<li>
<p>建立一个空文件夹 logs. eg: D:\Program\kafka_2.13-2.7.0\logs</p>
</li>
<li>
<p>进入config目录，编辑 server.properties文件(eg: 用“写字板”打开)。；找到并编辑log.dirs=D:\Program\kafka_2.13-2.7.0\logs；找到并编辑zookeeper.connect=localhost:2181。表示本地运行。(Kafka会按照默认，在9092端口上运行，并连接zookeeper的默认端口：2181)</p>
</li>
<li>
<p>运行：命令行下切入D:\Program\kafka_2.13-2.7.0目录，随后执行如下命令即可（注意先开启zookeeper）：.\bin\windows\kafka-server-start.bat .\config\server.properties</p>
</li>
<li>
<p>可能存在的报错：</p>
<p>输入行太长。 命令语法不正确：目录树太深了，减少几个目录即可。</p>
<p>找不到或无法加载主类 Files\java\jdk8\lib;D:\Program：此错误由目录存在空格所导致。解决：找到D:\Program\kafka_2.13-2.7.0\bin\windows，用编辑器（eg：记事本）打开kafka-run-class.bat，查看配置是否有加上双引号</p>
</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314005.png" alt="img" loading="lazy"></figure>
<p>如果加了双引号仍然无法启动，那可以去查看java安装路径中的文件夹是否存在空格，如果存在，将空格去掉即可。</p>
<p>启动图如下：</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314006.png" alt="img" loading="lazy"></figure>
<h2 id="简单kafka命令">简单kafka命令</h2>
<p>部分kafka命令，基于win10（如果非win系统，使用bin目录下的.sh即可）</p>
<h3 id="创建topic两个副本-四个分区">创建topic（两个副本。四个分区）</h3>
<pre><code>kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic testTopic  
$ bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --partitions 3 --replication-factor 3 --topic topic_test( Kafka 版本 &gt;= 2.2 支持此方式（推荐）)  
$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
</code></pre>
<h3 id="查看topic">查看topic</h3>
<p>查看topic列表：kafka-topics.bat --list --zookeeper localhost:2181</p>
<p>查看topic详情：bin/kafka-topics.sh --zookeeper host12:2181  --describe --topic ltopicName</p>
<pre><code>$ bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092
Topic:quickstart-events  PartitionCount:1    ReplicationFactor:1 Configs:
Topic: quickstart-events Partition: 0    Leader: 0   Replicas: 0 Isr: 0
</code></pre>
<p>清除Kafka topic下所有消息：kafka-topics.sh --zookeeper zookeeper地址:端口 --delete --topic topic_name</p>
<h3 id="删除topic">删除topic</h3>
<p>linux：./bin/kafka-topics  --delete --zookeeper 【zookeeper server】  --topic 【topic name】</p>
<p>win：kafka-topics.bat --delete --zookeeper localhost:2181 --topic testTopic</p>
<p>如果kafaka启动时加载的配置文件中server.properties没有配置delete.topic.enable=true，那么此时的删除并不是真正的删除，而是把topic标记为：marked for deletion</p>
<p>彻底删除topic，可以如下操作：</p>
<ol>
<li>删除kafka存储目录（server.properties文件log.dirs配置，默认为&quot;/tmp/kafka-logs&quot;）相关topic目录</li>
<li>登录zookeeper客户端：命令：./bin/zookeeper-client</li>
<li>找到topic所在的目录：ls /brokers/topics</li>
<li>找到要删除的topic，执行命令：rmr /brokers/topics/【topic name】即可，此时topic被彻底删除。</li>
</ol>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230314007.png" alt="img" loading="lazy"></figure>
<h3 id="发送消息至topic">发送消息至topic</h3>
<pre><code>$ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
This is my first event
This is my second event
</code></pre>
<h3 id="消费topic消息">消费topic消息</h3>
<p>从头开始查看kafka topic下的数据：kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic_name --from-beginning</p>
<p>按照偏移量查看topic下数据：kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic_name --offset latest --partition 0</p>
<p>--offset设置偏移量 latest代表最后 ，可以设置区间，不设置结尾的话默认为查询到latest(最后)</p>
<p>--partition 设置分区 使用偏移量查询时一定要设置分区才能查询</p>
<pre><code>$ bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
This is my first event
This is my second event
</code></pre>
<h2 id="spring-kafka属性配置">spring-kafka属性配置</h2>
<p>spring已经封装了kafka，所以在springboot项目中可以非常便捷的集成kafka，引入其依赖，如下：</p>
<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;
  &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;
  &lt;version&gt;${last.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>文档地址：<a href="https://docs.spring.io/spring-kafka/docs/current/reference/html/">点击此处跳转官网页面</a></p>
<p>属性配置：<a href="https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html#spring.kafka.admin.client-id">点击此处跳转说明页面</a></p>
<h3 id="producer的配置参数">producer的配置参数</h3>
<pre><code>#procedure要求leader在考虑完成请求之前收到的确认数，用于控制发送记录在服务端的持久化，其值可以为如下：
#acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为-1。
#acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。
#acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1的设置。
#可以设置的值为：all(-1), 0, 1
spring.kafka.producer.acks=1
 
#每当多个记录被发送到同一分区时，生产者将尝试将记录一起批量处理为更少的请求， 
#这有助于提升客户端和服务器上的性能，此配置控制默认批量大小（以字节为单位），默认值为16384
spring.kafka.producer.batch-size=16384
 
#以逗号分隔的主机：端口对列表，用于建立与Kafka集群的初始连接
spring.kafka.producer.bootstrap-servers
 
#生产者可用于缓冲等待发送到服务器的记录的内存总字节数，默认值为33554432
spring.kafka.producer.buffer-memory=33554432
 
#ID在发出请求时传递给服务器，用于服务器端日志记录
spring.kafka.producer.client-id
 
#生产者生成的所有数据的压缩类型，此配置接受标准压缩编解码器（'gzip'，'snappy'，'lz4'），
#它还接受'uncompressed'以及'producer'，分别表示没有压缩以及保留生产者设置的原始压缩编解码器，
#默认值为producer
spring.kafka.producer.compression-type=producer
 
#key的Serializer类，实现类实现了接口org.apache.kafka.common.serialization.Serializer
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
 
#值的Serializer类，实现类实现了接口org.apache.kafka.common.serialization.Serializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
 
#如果该值大于零时，表示启用重试失败的发送次数
spring.kafka.producer.retries
</code></pre>
<h3 id="consumer的配置参数">consumer的配置参数</h3>
<pre><code>#当'enable.auto.commit'为true时，该配置表示消费者offset自动提交给Kafka的频率（以毫秒为单位），默认值为5000。
spring.kafka.consumer.auto-commit-interval;
 
#当Kafka中没有初始偏移量或者服务器上不再存在当前偏移量时该怎么办，默认值为latest，表示自动将偏移重置为最新的偏移量
#可选的值为latest, earliest, none
spring.kafka.consumer.auto-offset-reset=latest;
 
#以逗号分隔的主机：端口对列表，用于建立与Kafka群集的初始连接。
spring.kafka.consumer.bootstrap-servers;
 
#ID在发出请求时传递给服务器;用于服务器端日志记录。
spring.kafka.consumer.client-id;
 
#如果为true，则消费者的偏移量将在后台定期提交，默认值为true(2.3后默认为false)
spring.kafka.consumer.enable-auto-commit=true;
 
#如果没有足够的数据立即满足“fetch.min.bytes”给出的要求，服务器在回答获取请求之前将阻塞的最长时间（以毫秒为单位）
#默认值为500
spring.kafka.consumer.fetch-max-wait;
 
#服务器应以字节为单位返回获取请求的最小数据量，默认值为1，对应的kafka的参数为fetch.min.bytes。
spring.kafka.consumer.fetch-min-size;
 
#用于标识此使用者所属的使用者组的唯一字符串。
spring.kafka.consumer.group-id;
 
#心跳与消费者协调员之间的预期时间（以毫秒为单位），默认值为3000
spring.kafka.consumer.heartbeat-interval;
 
#密钥的反序列化器类，实现类实现了接口org.apache.kafka.common.serialization.Deserializer
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
 
#值的反序列化器类，实现类实现了接口org.apache.kafka.common.serialization.Deserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
 
#一次调用poll()操作时返回的最大记录数，默认值为500
spring.kafka.consumer.max-poll-records;
</code></pre>
<h3 id="listener的配置参数">listener的配置参数</h3>
<pre><code>#侦听器的AckMode
#当enable.auto.commit的值设置为false时，该值会生效；为true时不会生效
spring.kafka.listener.ack-mode;
 
#在侦听器容器中运行的线程数
spring.kafka.listener.concurrency;
 
#轮询消费者时使用的超时（以毫秒为单位）
spring.kafka.listener.poll-timeout;
 
#当ackMode为“COUNT”或“COUNT_TIME”时，偏移提交之间的记录数
spring.kafka.listener.ack-count;
 
#当ackMode为“TIME”或“COUNT_TIME”时，偏移提交之间的时间（以毫秒为单位）
spring.kafka.listener.ack-time;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[敏感数据加密方案]]></title>
        <id>https://philosopherzb.github.io/post/min-gan-shu-ju-jia-mi-fang-an/</id>
        <link href="https://philosopherzb.github.io/post/min-gan-shu-ju-jia-mi-fang-an/">
        </link>
        <updated>2022-05-31T07:43:18.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述一种基于云服务的敏感数据加密设计方案。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/sunset-1117008_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="引言">引言</h2>
<h3 id="需求描述">需求描述</h3>
<p>为了防止敏感数据泄露，造成资产损失，故需要对相关数据进行加密处理；同时，允许部分加密数据支持模糊搜索。</p>
<h2 id="详细设计">详细设计</h2>
<h3 id="源码地址">源码地址</h3>
<p><a href="https://github.com/philosopherZB/demo-all/tree/master/secret">点击此处跳转源码地址</a></p>
<h3 id="总体设计">总体设计</h3>
<p>加密服务提供针对敏感数据存储的加密能力，用于防止因外部或内部安全威胁所导致的数据泄露问题，从而提高数据安全的防护水平。</p>
<p>基本功能：</p>
<ul>
<li>基础安全保障：加解密从根本上夯实了数据安全性。对敏感字段加密后，可以有效防止数据库内容被直接盗取。且密钥以租户维度隔离，有效解决应用的水平权限隔离问题。</li>
<li>数据库内容和密钥存储管理分离：接入方只存储加密数据，不保存密钥。只需接入本产品提供的SDK即可（实现细节将由SDK处理）。这在增强安全系数的同时，也简化了开发者管理、存储密钥的成本。</li>
<li>安全与服务平滑性兼得：SDK提供智能的、丰富的api可以自动识别数据库中存量密文的版本、自动加密、解密。接入方引入SDK后，可以做到在不停服务的条件下进行密钥升级。</li>
<li>接入简单便捷：加解密方案不依赖硬件，只需要引入加解密SDK即可，对数据库无特殊要求，使用成本、改造成本低。</li>
<li>保证数据完整性传输：所有接口都将使用非对称加密RSA进行签名校验，保证了数据的完整性。</li>
</ul>
<h3 id="整体业务流程">整体业务流程</h3>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310009.png" alt="img" loading="lazy"></figure>
<h4 id="接入方使用">接入方使用</h4>
<p>引入加解密SDK，初始化client后，直接调用对应的函数即可。</p>
<h4 id="加解密云服务">加解密云服务</h4>
<p>1、允许用户输入应用级别的RSA公私钥，应用公私钥功能：应用使用私钥加签数据，云服务使用公钥验签数据。此值支持变更。</p>
<p>2、自动生成云服务RSA公私钥，云服务公私钥功能：云服务使用私钥加签数据，应用使用公钥验签数据。此值不支持变更。</p>
<p>3、自动生成AES密钥（外部获取此值时，将会被RSA加密），此密钥不展示。此值支持变更。</p>
<p>4、自动生成伪随机码（日期+雪花算法生成），用于验伪操作，客户端需持有此参数并传递。此值支持变更。</p>
<p>5、自动生成appid，用于唯一标识一个应用（同一个客户允许存在多个appid），云服务前缀+雪花算法生成。</p>
<p>6、appId加version为组合唯一索引；其中version默认为0，每次变更AES密钥时，version+1。</p>
<p>7、允许设置过期时间（默认90天），最大有效期（默认120天，必须大于过期时间）；单位：天。主要用于客户端本地缓存的有效时长。</p>
<p>8、允许设置滑动窗口（默认4）大小及压缩长度（默认3）；用于加密search类型的数据。</p>
<h3 id="整体技术方案">整体技术方案</h3>
<h4 id="关于算法">关于算法</h4>
<p>对称加密与非对称加密混合使用：</p>
<ol>
<li>云服务自动生成出一对秘钥pub/pri。将私钥保密，将公钥公开。</li>
<li>接入方请求云服务时，拿到云服务的公钥pub。</li>
<li>云服务通过AES计算出一个对称加密的秘钥X。 然后使用应用公钥将X进行加密。</li>
<li>接入方发送加签请求到云服务获取加密后的秘钥X，云服务对请求进行验签，通过了方才返回秘钥X。</li>
<li>接入方得到加密后的秘钥X，便可以使用应用私钥解密，得到AES秘钥。</li>
<li>之后便可以使用AES秘钥进行敏感数据的加解密操作。</li>
</ol>
<p>注意：接入方不需要额外的开发，只需引入云服务SDK，并调用对应的接口即可。</p>
<h5 id="rsa">RSA</h5>
<p>将用于签名（保证数据完整性）及加密（保证数据安全性）</p>
<table>
<thead>
<tr>
<th>开放平台签名算法名称</th>
<th>标准签名算法名称</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>RSA2</td>
<td>SHA256WithRSA</td>
<td>强制要求 RSA 密钥的长度至少为 2048</td>
</tr>
<tr>
<td>RSA</td>
<td>SHA1WithRSA</td>
<td>对 RSA 密钥的长度不限制，推荐使用 2048 位以上</td>
</tr>
</tbody>
</table>
<p>由于计算能力的飞速发展，从安全性角度考虑，云服务推荐使用 SHA256WithRSA 的签名算法。该算法在摘要算法上比 SHA1WithRSA 有更强的安全能力。</p>
<p>注意避免公私钥混用：不同签名算法的签名密钥是隔离的。由于同时提供了两套签名算法，若选择了特定的签名算法，请保证使用对应的私钥签名，同时使用对应的云服务公钥进行验签。</p>
<h5 id="aes">AES</h5>
<p>将用于加解密敏感数据，此数据传输时，会被RSA加密保护，算法特性如下：</p>
<ol>
<li>加密算法使用“AES/CBC/PKCS5Padding”</li>
<li>加密用的初始向量会直接编码到加密数据中, 因此相同数据内容多次加密的结果不同</li>
<li>不同应用使用不同的秘钥，不同应用间加密数据无法解密（以appId作为区分）</li>
<li>支持秘钥升级, 密文中会包含加密密文用的秘钥版本，更新后支持老版本密文解密（appId + version 获取唯一AES秘钥）</li>
<li>传输过程AES秘钥将会被应用RSA公钥加密，返回时需要使用应用RSA私钥解密。</li>
</ol>
<h4 id="加密类型">加密类型</h4>
<table>
<thead>
<tr>
<th>类型</th>
<th>phone</th>
<th>id</th>
<th>simple</th>
<th>search</th>
</tr>
</thead>
<tbody>
<tr>
<td>描述</td>
<td>手机号</td>
<td>有规律的数字，例如身份证</td>
<td>普通文本类型</td>
<td>支持模糊搜索的文本类型</td>
</tr>
</tbody>
</table>
<p>1、phone，id为有规律的数字, 其加密算法只支持使用尾号后4位搜索。如13912345678的手机号加密后可用5678搜索。</p>
<p>2、接入方如果存在phone，id类的加密类型，建议增加一列检索串（创建索引），用于匹配搜索（List<code>&lt;DO&gt;</code> objects =  SELECT * FROM table WHERE phone=‘encryptedData’）；如果不想增加额外的检索串，也可以在原有的加密手机号字段上建立前缀索引，可缩短一定的模糊匹配查询时间；当然也可以在加密手机号字段上直接建立普通索引，因为加密后的索引串会放在整个加密字符的最前方（List<code>&lt;DO&gt;</code> objects =  SELECT * FROM table WHERE phone like ‘encryptedData%’）。关于如何截取检索串可在附录中查看。</p>
<p>3、simple类型只进行加密，不支持模糊搜索。</p>
<p>4、search类型支持模糊搜索，基本实现原理是根据4位英文字符（半角），2个中文字符（全角）为一个检索条件。将一个字段拆分为多个。</p>
<p>比如：test123，使用4个字符为一组的加密方式。切割结果为：[test, est1, st12, t123]，第一组 test，第二组est1，第三组st12，第四组 t123… 依次类推，如果需要检索 所有包含 检索条件4个字符的数据 比如：test，加密字符后通过key like “%partial%” 查库。</p>
<p>因为密文检索开启后 密文长度会膨胀几倍以上，如果没有强需求建议不开启。</p>
<p>使用这种方式存在一定的代价：</p>
<ul>
<li>支持模糊查询加密方式，产出的密文比较长；</li>
<li>支持的模糊查询子句长度必须大于等于4个英文/数字，或者2个汉字。不支持过短的查询(出于安全考虑)；</li>
<li>如果数据本身长度不够4个数字或2个汉字, 则此时输入全文即可搜索.  如: 原始数据为&quot;安&quot;的, 使用&quot;安&quot;调用平台搜索接口即可获取搜索用文本；</li>
<li>返回的结果列表中有可能有多余的结果，需要增加筛选的逻辑：对记录先解密，再筛选；</li>
</ul>
<h4 id="加密格式">加密格式</h4>
<p>注意：其中 Index 是检索信息，同一份数据多次加密后的结果都不会变。</p>
<p>手机号(phone)和身份证(id)格式，其中手机号(phone)分隔符：SEP=$；身份证(id)分隔符：SEP=#</p>
<table>
<thead>
<tr>
<th>SEP</th>
<th>Index</th>
<th>SEP</th>
<th>EncryptedData</th>
<th>SEP</th>
<th>Version</th>
<th>SEP</th>
<th>SEP</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>search类文本，SEP=~</p>
<table>
<thead>
<tr>
<th>SEP</th>
<th>EncryptedData</th>
<th>SEP</th>
<th>Index</th>
<th>SEP</th>
<th>Version</th>
<th>SEP</th>
<th>SEP</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>simple类文本，SEP=~</p>
<table>
<thead>
<tr>
<th>SEP</th>
<th>EncryptedData</th>
<th>SEP</th>
<th>Version</th>
<th>SEP</th>
</tr>
</thead>
<tbody></tbody>
</table>
<h4 id="加密样例">加密样例</h4>
<p>加密算法支持模糊搜索(可选)，模糊加密的数据比一般加密更长一些，大概长度扩大 10-15 倍。</p>
<p>加密数据的示例如下：</p>
<p>phone(手机号)加密方式，加密类型：phone</p>
<table>
<thead>
<tr>
<th>手机号</th>
<th>13012345678</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>$mMHmK1rfa+OZ23eYhO1AUQ==$q6cFTUyW6SuJD2NReTUVBQ==$1$$</td>
</tr>
<tr>
<td>后四位手机号</td>
<td>5678</td>
</tr>
<tr>
<td>5678检索串</td>
<td>mMHmK1rfa+OZ23eYhO1AUQ==</td>
</tr>
</tbody>
</table>
<p>身份证加密方式，加密类型：id</p>
<table>
<thead>
<tr>
<th>身份证</th>
<th>200300190002039898</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>#uJfxXOF0EJ+6V8H51L9rdg==#ao/gDujB17PTObfSjeAOvl/YxmScgYuxQPl2wokwlec=#1##</td>
</tr>
<tr>
<td>后四位身份证</td>
<td>9898</td>
</tr>
<tr>
<td>9898检索串</td>
<td>uJfxXOF0EJ+6V8H51L9rdg==</td>
</tr>
</tbody>
</table>
<p>普通文本加密方式，加密类型：simple</p>
<table>
<thead>
<tr>
<th>地址</th>
<th>浙江省杭州市滨江区星耀城</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>~thzORLtQtSKYQZW2UFvM64bMN5NhDPb8PSRZ5KNLtl3ZJ+sIYfzMN5gxPy7+ba4o~1~</td>
</tr>
</tbody>
</table>
<p>支持搜索的文本加密方式，加密类型：search</p>
<table>
<thead>
<tr>
<th>域名</th>
<th>www.test.hostname.com</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持搜索的密文</td>
<td>~J/7ro/3scxjvrPXFB5+OxxOZdv45iqXbkP4doNfcEAo=~Q9bpt3Na0G8oOwOe1i9y+Eanjg7Tfr5U3vcYuMyC2iZi7ciYfvC+bNP6pS29uUPpo4L3spYx~1~~</td>
</tr>
<tr>
<td>需要查询的字符</td>
<td>test.hostname</td>
</tr>
<tr>
<td>test.hostname检索串</td>
<td>1i9y+Eanjg7Tfr5U3vcYuMyC2iZi7ciYfvC+bNP6</td>
</tr>
</tbody>
</table>
<h4 id="加密长度">加密长度</h4>
<p>加密后由于增加了密文信息, 将比原文要长, 各种常见字段加密后的数据长度参考表如下:</p>
<p>注: 当发生加密秘钥version字段变更时, 密文长度可能会发生变化. 建议设计字段时在表中字段上加10个左右字符。</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>密文长度（支持搜索）</th>
<th>密文长度（不支持搜索）（使用simple类型加密）</th>
</tr>
</thead>
<tbody>
<tr>
<td>phone（手机号）</td>
<td>54</td>
<td>28</td>
</tr>
<tr>
<td>id（身份证为例）</td>
<td>74</td>
<td>48</td>
</tr>
<tr>
<td>文本数据（5字符，全汉字）</td>
<td>46</td>
<td>28</td>
</tr>
<tr>
<td>文本数据（10字符，全汉字）</td>
<td>86</td>
<td>48</td>
</tr>
<tr>
<td>文本数据（20字符，全汉字）</td>
<td>170</td>
<td>92</td>
</tr>
<tr>
<td>文本数据（40字符，全汉字）</td>
<td>334</td>
<td>176</td>
</tr>
</tbody>
</table>
<h2 id="数据库设计">数据库设计</h2>
<h3 id="数据库ddl">数据库DDL</h3>
<pre><code>-- DROP TABLE IF EXISTS `tb_app_config_aes`;
CREATE TABLE `tb_app_config_aes`
(
  `id`             bigint(20)  NOT NULL AUTO_INCREMENT COMMENT '自增主键',
  `app_id`         varchar(64) NOT NULL COMMENT '应用id，用于唯一标识应用；生成规则：云服务前缀+雪花算法生成',
  `aes_key`        char(24)    NOT NULL COMMENT 'aes秘钥,固定长度',
  `secret_version` bigint(16)  NOT NULL COMMENT '版本号，用于升级aesKey',
  `is_delete`      tinyint(1)  NOT NULL DEFAULT 0 COMMENT '删除标识，0-未删除，1-已删除',
  `create_time`    datetime    NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `modify_time`    datetime    NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`),
  unique key `unique_index_app_id_secret_version` (`app_id`, `secret_version`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8 COMMENT ='AES秘钥配置表';
  
-- DROP TABLE IF EXISTS `tb_app_config_rsa`;
CREATE TABLE `tb_app_config_rsa`
(
  `id`                         bigint(20)    NOT NULL AUTO_INCREMENT COMMENT '自增主键',
  `app_id`                     varchar(64)   NOT NULL COMMENT '应用id，用于唯一标识应用；生成规则：云服务前缀+雪花算法生成',
  `tenant_id`                  varchar(128)  NOT NULL COMMENT '租户id',
  `tenant_name`                varchar(255)  NOT NULL COMMENT '租户名称',
  `app_pub_key`                varchar(512)  NOT NULL COMMENT '应用公钥，用于验签接口，同时加密AES秘钥',
  `das_pub_key`                varchar(512)  NOT NULL COMMENT '云服务公钥，用于接入方验签',
  `das_pri_key`                varchar(2048) NOT NULL COMMENT '云服务私钥，云服务使用此私钥进行加签',
  `encrypt_slide_size`         int(3)        NOT NULL DEFAULT 4 COMMENT '滑动窗口大小',
  `encrypt_index_compress_len` int(3)        NOT NULL DEFAULT 3 COMMENT '密文滑窗压缩长度',
  `random_num`                 varchar(64)   NOT NULL COMMENT '伪随机码，生成规则：日期+雪花算法生成',
  `invalid_time`               int(4)        NOT NULL DEFAULT 90 COMMENT '过期时间，单位：天，默认90天',
  `max_invalid_time`           int(4)        NOT NULL DEFAULT 120 COMMENT '最大有效期，单位：天，默认120天；必须大于invalidTime',
  `is_delete`                  tinyint(1)    NOT NULL DEFAULT 0 COMMENT '删除标识，0-未删除，1-已删除',
  `create_time`                datetime      NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `modify_time`                datetime      NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`),
  unique key `unique_index_app_id` (`app_id`),
  unique key `unique_index_tenant_id` (`tenant_id`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8 COMMENT ='RSA秘钥配置表';

</code></pre>
<h2 id="附录">附录</h2>
<h3 id="rsa公私钥生成代码-java版">RSA公私钥生成代码-java版</h3>
<pre><code>import java.security.KeyPair;
import java.security.KeyPairGenerator;
import java.security.NoSuchAlgorithmException;
import java.security.PrivateKey;
import java.security.PublicKey;
import java.security.SecureRandom;
import java.util.Base64;
 
/**
 * @author philosopherZB
 * @date 2022/1/13
 */
public class GenerateKeyUtils {
    private static KeyPair KEY_PAIR_INSTANCE = null;
    private static String ALGORITHM = &quot;RSA&quot;;
    private static Integer KEY_LENGTH = 2048;
 
    /**
     * 生成默认公钥--algorithm=RSA; keyLength=2048
     *
     * @return publicKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genDefaultPublicKey() throws NoSuchAlgorithmException {
        return genPublicKey(ALGORITHM, KEY_LENGTH);
 
    }
 
    /**
     * 生成默认私钥--algorithm=RSA; keyLength=2048
     *
     * @return privateKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genDefaultPrivateKey() throws NoSuchAlgorithmException {
        return genPrivateKey(ALGORITHM, KEY_LENGTH);
    }
 
    /**
     * @param algorithm 标准的算法名: https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html
     * @param keyLength 指定长度
     * @return publicKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genPublicKey(String algorithm, int keyLength) throws NoSuchAlgorithmException {
        PublicKey publicKey = getKeyPairInstance(algorithm, keyLength).getPublic();
        return Base64.getEncoder().encodeToString(publicKey.getEncoded());
 
    }
 
    /**
     * @param algorithm 标准的算法名: https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html
     * @param keyLength 指定长度
     * @return privateKey
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    public static String genPrivateKey(String algorithm, int keyLength) throws NoSuchAlgorithmException {
        PrivateKey privateKey = getKeyPairInstance(algorithm, keyLength).getPrivate();
        return Base64.getEncoder().encodeToString(privateKey.getEncoded());
    }
 
    /**
     * getKeyPairInstance
     *
     * @param algorithm 标准的算法名: https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html
     * @param keyLength 指定长度
     * @return KeyPair
     * @throws NoSuchAlgorithmException 算法不存在异常
     */
    private static KeyPair getKeyPairInstance(String algorithm, int keyLength) throws NoSuchAlgorithmException {
        if (KEY_PAIR_INSTANCE == null) {
            KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(algorithm);
            SecureRandom secureRandom = new SecureRandom();
            keyPairGenerator.initialize(keyLength, secureRandom);
            KEY_PAIR_INSTANCE = keyPairGenerator.generateKeyPair();
        }
 
        return KEY_PAIR_INSTANCE;
    }
 
    public static void main(String[] args) throws NoSuchAlgorithmException {
        System.out.println(&quot;publicKey: &quot; + genPublicKey(&quot;RSA&quot;, 2048));
        System.out.println(&quot;privateKey: &quot; + genPrivateKey(&quot;RSA&quot;, 2048));
 
        System.out.println(&quot;default publicKey: &quot; + genDefaultPublicKey());
        System.out.println(&quot;default privateKey: &quot; + genDefaultPrivateKey());
    }
}

</code></pre>
<h3 id="检索串提取代码示例-java版">检索串提取代码示例-java版</h3>
<pre><code>public static String extractIndex(String encryptedData) {
    if (encryptedData == null || encryptedData.length() &lt; 4) {
        return null;
    }
    char sepInData = encryptedData.charAt(0);
    if (encryptedData.charAt(encryptedData.length() - 2) != sepInData) {
        return null;
    }
    String[] parts = StringUtils.split(encryptedData, sepInData);
    if (sepInData == '$' || sepInData == '#') {
        return parts[0];
    } else {
        return parts[1];
    }
}

</code></pre>
<h3 id="sdk使用样例-java版">SDK使用样例-java版</h3>
<pre><code>public static void main(String[] args) {
    String privateKey = &quot;MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDGbO1R5OQ0Ff27V+k1FuPBnKMqwC3c0HFnAFu0ZUWH5FTyfkMahhsRntg7/uLk2QM1v0rx7kEdd/ExhOs5z68nO8xbyX/YbC+DX/NH3IsvNRrDU+xfpZdnuMT3bjtlGsasTIOK8S3DlHfiGpO4HPgeY7mBfduBaPStmJFLMM4Xpg3piD4r8mmq+2dAIhq6vX8GpwVqap0XLk8TcYY5h8WQ0FTbTSbRUNN/+YHmlwbEJVa+NR8qaUoo75/WHeVgjNlZ8SAfdjMt1oVWmibSiKYDr1JBVZrPD4CjBy6UR9jDrTxTrwmGsvHUzE3ZThPJZvJEWVXHbMCTs4KReUds/UcPAgMBAAECggEAXSsAM5e53wsEXFbm1VquDlax9nzODASDes3cQZPblfcMO+A1OdsGErv25BTGDJYo/6+WTQqF4IRU5991Y2u03kMhrWdrc/84QANpg7B2WfAhZN2e+zoRYU5MjbFgihSMfJJgoXik+FRaBfxcp/JSPlKs47RowNa7LFeawSdlXYxy+eXCouYomGe0aCPACKfHRyBWDRCf+eK/UwXRcmiyHi8hXOa/IfsaBsZNLtHR3rFod4x+hZUoIJAuXpupc2KW4qsK8ImW9BDcx2p2EO6iggw6MZMZkIo7NRHf3aaaLfpVsebjHm5lBF95ptVUP4CFpfoNevn3y09D4nhGjBP7AQKBgQDnZIcX+XJDykYES10jClxKndibhLYN9zNp7FOiin1Sfd5tUux+VXNEocWSMIqz8wuuI6kyrigwa0E2Kis3QANLizsaOu39jCW5kAHFm19txG3IgntPix95qADl9fQAO1Y3Nbh+zL43FH8f0s066L3V9bTvmZ6rN9vf8GF/xHrfLwKBgQDbhuUop370RhFbgN1CIFM6m6gio7pft7NW6YLyn4yXa/vbpw7c7B+c5TA1ru9maaXlF+Sff5WtLVK10czghDOdI0B5qrm62+TEhOEk7C6dgNsCdaHuDzqA2MYEmq4HJhoXrwM+xvzrlCwNZQ1AIRQWKdceKK9YgEnZQXQU9TEeIQKBgQCfPELrcLH9jLlaQzK45mxUvQNPIqjWO4OaJRP5CyzrE8t5mFM/LTbByEHaNKV+6IblM41AXzExAN5DlAlhYB/kYNAvYNZeYY+kf0F4509ojoCuN3z8ZFUot0DG/9cGQc829zUbrXJJHUXOdJbfL0NUdl4pdKIIWcxp81ZlQqT76QKBgHMlCzfKux1XTy1mpydTGzSnhoY8yLoB+dBBhQzLwQt/eUhaFMKuG1rJIANYcXuPOJO0d5dtbU27cyGpHMQ6s3PdlKj8cpTfV9v4MruSIlU8zCM7Hidm13HTwfGSTGu1gYQgqRwZdXn/ayfPdCbJ8uY5JftMrcRG7fVFjqSbgxrhAoGAVPNPW/QOVarHZDCVJi0XtOhl1g6oR51FGkOZ13SEuu8271v0PxcP55Hib6WvCPDOFB75oPaa1HmQs/WL41RCFMlELH/DhwEEkd2hLFmou4AiiB+M2DQODxagyINOx0RGd7FZuhiIWQTocqXj2PV71gkn5q4pIGKLsk1aklLTKXY=&quot;;
    String dasPubKey = &quot;MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAiPNXFw6tQyfpNi63OoR5E+VR+KS4Sy1+1EFNiHTIc/f5AXXZtmo8CVTgAM8X7a2GHLMwRqjH1JZ/2Da9HHn8zHdEtSAbObxMtTKPrnQOG5NrTuA3hfRb/4N00iZZ2KZMr5fTXJ4824VMr2/fQZySwDd0bOPTmrNnlHLu6ErFvfJwjQqbhWVC1VhRkGzvT81O2SM+ALuTnbgoFGqFyaUE9YUP57COA/Hw4Yz+GmQkHxs9ELPvikFSGdBdptDvHQ2dTprskRW8UU/v0XjVED8jeayiqKxJFn2Yejq43eqkH+SV6c9R1jE39qhMyEX7hVvzSMcyvONSo5Za4R5zLqap4wIDAQAZ&quot;;
    // 初始化一次即可（单例模式）
    SecurityClient securityClient =
            new DefaultSecurityClient(&quot;http://127.0.0.1:8999&quot;, &quot;cloud20220209651316091208540160&quot;, privateKey, &quot;20220209651322386368110592&quot;, dasPubKey);
    try {
        // 加密类型: TYPE_PHONE-手机号; TYPE_ID-id; TYPE_SIMPLE-普通加密；TYPE_SEARCH-支持搜索的加密
        // version 为AES版本号，可从云服务中查看
        String encryptData = securityClient.encrypt(&quot;www.test.hostname.com&quot;, SecurityConstants.TYPE_SEARCH, 1L);
        System.out.println(&quot;encryptData: &quot; + encryptData);

        boolean isEncryptData = securityClient.isEncryptData(encryptData, SecurityConstants.TYPE_SEARCH);
        System.out.println(&quot;isEncryptData: &quot; + isEncryptData);

        String decryptData = securityClient.decrypt(encryptData, SecurityConstants.TYPE_SEARCH, 1L);
        System.out.println(&quot;decryptData: &quot; + decryptData);

        String searchData = securityClient.search(&quot;test.hostname&quot;, SecurityConstants.TYPE_SEARCH, 1L);
        System.out.println(&quot;searchData: &quot; + searchData);
    } catch (DasSecurityException e) {
        e.printStackTrace();
    }
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[分布式事务详解]]></title>
        <id>https://philosopherzb.github.io/post/fen-bu-shi-shi-wu-xiang-jie/</id>
        <link href="https://philosopherzb.github.io/post/fen-bu-shi-shi-wu-xiang-jie/">
        </link>
        <updated>2022-05-21T06:09:37.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述单事务到分布式事务的转变以及对应分布式事务解决方案。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/iceland-1768744_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="事务演变">事务演变</h2>
<h3 id="概要">概要</h3>
<p>随着业务的不断发展以及业务复杂度的提升，传统单体服务逐渐暴露出了一些问题，如开发效率低、可维护性差、架构扩展难、部署不灵活等。因此，走上分布式微服务是一条必然的道路。</p>
<p>在分布式中，每个服务便是一个独立的进程，各个服务间可单独迭代，互不影响，甚至可以用不同的语言开发，只需事先定义好对接规范即可。</p>
<p>分布式很好地解决了单体服务的一些问题，但随之而来的便是一系列分布式难题。本篇将要讲述的便是分布式事务这个问题。</p>
<h3 id="本地事务">本地事务</h3>
<p>本地事务，一般也被称之为数据库事务。它主要功能是将多条SQL语句当做一个整体来处理，保证这一个整体要么执行成功，要么执行失败。</p>
<p>比较典型的例子就是转账服务了：A转账100给B，那么对应有A-100和B+100两条语句，事务必须保证这两条语句的整体性，否则就会出现A-100，但B没有增加100的场景。</p>
<p>上述例子体现了事务的一个重要特性：原子性。事实上，事务具备四个基本特性：原子性，一致性，隔离性，永久性；通常也简称为ACID特性。</p>
<ul>
<li>Atomicity(原子性)：一个事务便是一个整体，是不可再被分隔的最小独立单元；在一个事务中的操作，要么同时成功，要么同时失败，不会存在部分成功，部分失败的场景。</li>
<li>Consistency(一致性)：在事务开始前以及结束后，数据库的完整性没有被破坏。完整性包括约束、级联、触发器及其任意组合。</li>
<li>Isolation(隔离性)：数据库允许事务并发执行读写，为了防止出现多个事务并发写导致数据不一致，便有了隔离性。</li>
<li>Durability(持久性)：事务提交后，数据将会被永久地保存，即使系统故障也不会丢失。</li>
</ul>
<p>隔离级别：</p>
<table>
<thead>
<tr>
<th>Isolation Level</th>
<th>Dirty Reads</th>
<th>Non-Repeatable Reads</th>
<th>Phantom Reads</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read uncommitted</td>
<td>允许</td>
<td>允许</td>
<td>允许</td>
</tr>
<tr>
<td>Read committed(Sql server, Oracle)</td>
<td>不允许</td>
<td>允许</td>
<td>允许</td>
</tr>
<tr>
<td>Repeatable reads(Mysql)</td>
<td>不允许</td>
<td>不允许</td>
<td>允许</td>
</tr>
<tr>
<td>Serializable</td>
<td>不允许</td>
<td>不允许</td>
<td>不允许</td>
</tr>
</tbody>
</table>
<ul>
<li>Dirty reads：A事务可以读到B事务还未提交的数据。</li>
<li>Non-repeatable read：A事务读取一行数据，B事务后续修改了这行数据，A事务再次读取这行数据，结果得到的数据不同。</li>
<li>Phantom reads：A事务通过SELECT ... WHERE得到一些行，B事务插入新行或者删除已有的行使得这些行满足A事务的WHERE条件，A事务再次SELECT ... WHERE结果比上一次多/少了一些行。</li>
</ul>
<p>注意1：mysql默认使用RR隔离级别，这是由于binlog的格式问题（statement-记录修改的SQL语句,row-记录每行实际数据的变更,mixed-前面两种混合）所导致的，在5.0之前binlog只有statement一种格式，而主从复制时，这会导致数据的不一致。</p>
<p>注意2：其他数据库选用RC隔离级别，是由于RR隔离级别增加了间隙锁，会增加发生死锁的概率；同时，条件列未命中索引时，会锁全表，RC只会锁行。</p>
<p>注意3：RC隔离级别下，主从复制需要采用binlog的row格式，基于行的复制，这样不会出现主从不一致问题。</p>
<h3 id="分布式事务">分布式事务</h3>
<p>随着业务量的增加，单体服务所能承载的数据量越来越多，系统的运行速度逐渐下降，这时候便需要进行服务独立化（微服务）。</p>
<p>比如跨行转账，或者本行跨服务转账等都是比较典型的分布式场景；这里的每个服务都有自己独立的数据库，为了避免服务不可用，网络连接异常等情造成的数据不一致，分布式事务便应运而生了。</p>
<p>分布式事务的本质是为了多服务之间事务的正确执行，是对本地事务的一个扩展，但它只遵循部分ACID规范。所以，有必要介绍下分布式事务中的CAP定理与BASE理论。</p>
<h4 id="cap定理">CAP定理</h4>
<p>在理论计算机科学中，CAP定理(CAP theorem)，又被称为<a href="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/">布鲁尔定理(Brewer's theorem)</a>。</p>
<ul>
<li>Consistency(一致性)：此处指的是强一致性；它要求分布式系统中一个服务的写操作成功后，其他服务的读操作都是刚刚写入的数据（最新数据）。</li>
<li>Availability(可用性)：非故障服务节点收到请求后必须给予响应（非最新数据）。</li>
<li>Partition-tolerance(分区容错性)：分布式系统在遇到任何网络分区故障时，仍然可以对外提供服务，除非整个分布式系统宕机。（ Gilbert and Lync describe partitions: the network will be allowed to lose arbitrarily many messages sent from one node to another）</li>
</ul>
<p>网络分区：在分布式系统中，不同的节点分布在不同的子网络中，由于一些特殊的原因，这些子节点之间出现了网络不通的状态，但他们的内部子网络是正常的。从而导致了整个系统的环境被切分成了若干个孤立的区域，这就是分区。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310001.png" alt="img" loading="lazy"></figure>
<h4 id="cap定理验证">CAP定理验证</h4>
<p>事实上，在分布式系统中，CAP三者是无法同时存在的，且由于分布式的缘故，分区容灾是必须存在的。</p>
<p>为了验证CAP定理，假设网络中存在两个节点N1和N2，它们之间网络互通，其中N1上存在服务A和独立数据库D1，N2上存在服务B和独立数据库D2。此时，A和B是分布式系统中的一部分，同时对外提供相关服务。</p>
<ul>
<li>一致性要求A，B服务中的数据是一致的，即D1=D2。</li>
<li>可用性要求不管是请求A还是B都可以得到确认的响应。</li>
<li>分区容灾则是指出现网络故障或其他异常场景时，都不会影响A，B之间的正常运行，除非服务本身宕机。</li>
</ul>
<p>在分布式系统中，网络通信时最常见的，就比如上述N1与N2之间；现在假设N1与N2因网络故障而无法通信，为了支持这种网络异常，即要满足分区容灾，那么如果还要同时满足一致性与可用性是否可行了？</p>
<p>可以简单模拟一下：当N1与N2断开网络时，一个请求抵达服务A，更新了D1上的一条数据；此时为了一致性要求，需要将该条更新数据同步至服务B上的数据库D2，但由于网络故障，两个服务无法通信，此时便面临两种选择。</p>
<ul>
<li>牺牲一致性，保证可用性；当下次请求服务B时响应旧的数据。</li>
<li>牺牲可用性，保证一致性；阻塞等待网络恢复，将数据更新至服务B。</li>
</ul>
<h4 id="cap抉择">CAP抉择</h4>
<p>CAP三者究竟该如何选择，才能满足业务需求，这是每个分布式系统架构中都需要考虑。</p>
<ul>
<li>CA(一致性与可用性)：如果选择CA，那么就需要一种非常严格的强一致性协议来进行保障；且，不允许出现网络故障或节点错误，否则整个系统将会不可用。显然，CA不是很适合分布式系统。</li>
<li>CP(一致性与分区容灾)：CP保证了数据的一致性，但由于P的存在，某些时候一个请求将会被无限延长。</li>
<li>AP(可用性与分区容灾)：AP保证了可用性，但却丢失了数据的一致性，在某些场景（银行转账服务）中是难以忍受的。</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310002.png" alt="img" loading="lazy"></figure>
<h4 id="base理论">BASE理论</h4>
<p>由上述CAP定理中可以知道，分布式系统是无法同时满足三者的，只能从中取二。由于分布式系统中网络环境的不可信，分区容灾是必选的；其次可用性保证了用户体验，除开一些需要强一致性的场景(支付，转账等)，应该先选；但如果没有一致性，分布式系统也就失去了存在的意义。</p>
<p>针对此种场景，可以对CAP中的强一致性做一定的让步，有些时候，我们只关注数据的最终一致性即可。于是，BASE理论便诞生了。</p>
<ul>
<li>Basically Available(基本可用)：相对高可用而言，基本可用要求系统即使出现了重大故障，仍然能够提供一些基本型的服务。例如响应时间上的损失以及功能上的损失(熔断，降级)。</li>
<li>Soft state(软状态)：软状态是相对原子性(硬状态，即多个节点间的数据完全一致)而言的。该状态允许数据存在中间状态，并认为该状态并不影响系统的整体可用性，即允许系统在多个不同节点的数据同步存在一定的延时。（注意：对于软状态,我们允许中间状态存在，但不可能一直是中间状态，必须要有个期限，系统保证在没有后续更新的前提下,在这个期限后,系统最终返回上一次更新操作的值,从而达到数据的最终一致性,这个容忍期限（不一致窗口的时间）取决于通信延迟，系统负载，数据复制方案设计，复制副本个数等，DNS是一个典型的最终一致性系统。）</li>
<li>Eventually consisten(最终一致性)：相对强一致性而言，最终一致性仅仅要求数据在经过一段合理的延时后（软状态），最终抵达一致即可。</li>
</ul>
<p>关于最终一致性的变种模型，一般在实践中，五种模式会组合使用：</p>
<ul>
<li>因果一致性(Causal consistency)：如果节点A在更新完某个数据后通知了节点B，那么节点B之后对该数据的访问和修改都是基于A更新后的值。与此同时，和节点A没有因果关系的节点C则没有这样的限制。</li>
<li>读己所写(Read your writes)：一个节点总能访问自己更新过的最新值，是因果一致性的特定形式。</li>
<li>会话一致性(session consistency)：系统保证在一个有效的会话中实现读己所写(Read your writes)。</li>
<li>单调读一致性(Monotonic read consistency)：节点A从系统中读到一个数据项D的某个值V后，节点A后续对数据项D的访问都不会返回比值V更旧的值。</li>
<li>单调写一致性(Monotonic write consistency)：系统需保证来自同一个节点的写操作被顺序执行。</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310003.png" alt="img" loading="lazy"></figure>
<h2 id="解决方案">解决方案</h2>
<h3 id="概要-2">概要</h3>
<p>因为业务的多样性与复杂性，再加上分布式事务解决方案也并未银弹，所以，在实际开发中，应该以本身业务作为作出发点，选择最适合自己的方案。</p>
<h3 id="二阶段提交协议">二阶段提交协议</h3>
<p>在分布式系统中，为了保证多节点间事务的正确执行，便需要一个协调者来管理参与事务的所有节点，确保操作结果符合执行预期。其中，XA便是一个典型的分布式事务处理协议。</p>
<p>XA协议是由X/Open组织提出的分布式事务处理规范，它主要定义了(全局)事务管理器TM和(局部)资源管理器RM之间的接口；是通过二阶段提交协议来保证强一致性的。</p>
<ul>
<li>第一阶段(prepare)：事务管理者TM(协调者)向资源管理者RM(参与者)发送prepare请求，RM(参与者)进行预提交并将结果响应给TM。</li>
<li>第二阶段(commit/rollback)：如果所有RM(参与者)预提交结果都为成功，则TM(协调者)向所有RM(参与者)发送commit请求，否则发送rollback请求。</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310004.png" alt="img" loading="lazy"></figure>
<p>二阶段提交协议虽然为强一致性提出了一套解决方案，但需要注意的是，其中也有几个不可忽略的缺点。</p>
<ul>
<li>同步阻塞：TM(协调者)控制着所有RM(参与者)的操作(准备与实际提交)，这个过程是同步的，TM(协调者)必须等待所有的RM(参与者)返回操作结果才能进行下一步操作；如果在这个过程中有其他请求进来，将会被阻塞。</li>
<li>单点故障：无论是TM(协调者)还是RM(参与者)发生故障，整个流程都将会陷入无限期等待中。</li>
<li>数据不一致：极端情况下，prepare成功，但commit出现部分提交，部分宕机，便会导致数据不一致。</li>
</ul>
<h3 id="三阶段提交协议">三阶段提交协议</h3>
<p>三阶段提交协议是二阶段提交协议的改良版本，它增加了超时机制用来解决同步阻塞问题；实际上是将第一阶段prepare拆分为canCommit和preCommit两个阶段。</p>
<p>preCommit阶段，TM(协调者)在发送真正的commit请求之前，会再次检查各个RM(参与者)的状态，以确保它们的状态一致。</p>
<p>当然，某些极端场景下，同样会出现数据不一致；如：第三阶段的commit出现机器宕机。</p>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310005.png" alt="img" loading="lazy"></figure>
<h3 id="tcc">TCC</h3>
<p>在二阶段提交协议，资源管理者RM负责准备、提交与回滚，而事务管理者TM则负责协调所有RM具体进行哪些操作；资源管理者RM有很多种实现方式，其中TCC(Try-Confirm-Cancel)便是资源管理者的一种服务化的实现。</p>
<ul>
<li>Try阶段：尝试执行，完成业务检查（一致性），预留业务资源（隔离性）。</li>
<li>Confirm阶段：真正的执行具体业务操作，保证幂等。</li>
<li>Cancel阶段：回滚操作，释放try阶段预留的业务资源，保证幂等。</li>
</ul>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310006.png" alt="img" loading="lazy"></figure>
<p>关于TCC理论及设计可参考：<a href="https://www.sofastack.tech/blog/seata-tcc-theory-design-realization/">点击此处跳转文章页面</a></p>
<p>关于TCC使用场景可参考：<a href="https://www.sofastack.tech/blog/seata-tcc-applicable-models-scenarios/">点击此处跳转文章页面</a></p>
<h3 id="saga">Saga</h3>
<p>saga会将一个长事务(long lived transaction)拆分为多个短事务，然后saga自己进行调度管理。saga有点类似于tcc，但是没有try阶段。</p>
<p>当流程出现异常导致部分事务执行失败时，saga会进行补偿操作。此时saga有两种选择：backward recovery 和 forward recovery。</p>
<ul>
<li>逆向恢复(backward recovery)：即回滚操作，将之前所有成功的节点进行回滚，以达到数据一致的目的。</li>
<li>正向恢复(forward recovery)：根据save-point尽最大努力不断重试，以达到数据一致的目的。</li>
</ul>
<p>sagas论文地址：<a href="https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf">点击此处跳转论文页面</a></p>
<h3 id="重试补偿模式">重试补偿模式</h3>
<p>在实际业务开发中，可以针对某些特定的业务添加额外的消息表记录失败信息，后续定时地进行补偿操作。例如京东订单成功后会发送邮件信息，此时可简单的拆分为订单服务与邮件服务，时序图如下：</p>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310007.png" alt="img" loading="lazy"></figure>
<h2 id="开源分布式事务框架">开源分布式事务框架</h2>
<h3 id="rocketmq">RocketMQ</h3>
<p>从4.3版本开始提供基于2PC(二阶段提交协议)加补偿机制的分布式事务消息，其中补偿机制主要针对2PC过程超时或失败的场景。可参考：<a href="http://rocketmq.apache.org/rocketmq/the-design-of-transactional-message/">点击此处跳转页面</a></p>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230310008.png" alt="img" loading="lazy"></figure>
<ol>
<li>生产者发送半消息(half message)至MQ服务端。</li>
<li>如果发送半消息成功，则执行本地事务。</li>
<li>基于本地事务的执行结果发送commit或rollback消息到MQ服务端。</li>
<li>如果commit/rollback消息丢失或者生产者在执行本地事务时处于pended状态，MQ服务端将会发送检查消息到同组(same group)的每个生产者获取事务消息的状态(CommitTransaction,RollbackTransaction,Unknown)。</li>
<li>生产者基于本地事务状态恢复commit/rollback消息。</li>
<li>已提交的消息会被MQ服务端分发给消费者，而回滚类消息则会被丢弃。</li>
</ol>
<p>注意1：半消息(half message)是无法被消费的，因为它位于一个独立的内部half-topic中，对消费者是不可见的；commit过程相当于从half-topic中取出消息进行重新投递。</p>
<p>注意2：使用事务消息功能将会降低rocketMQ的执行效率，因为写过程被放大了。</p>
<h3 id="tcc-transaction">TCC-Transaction</h3>
<p>基于TCC模式开发的分布式事务框架，具体可参考：<a href="https://github.com/changmingxie/tcc-transaction">点击此处跳转官网地址</a></p>
<h3 id="servicecomb-saga">ServiceComb-Saga</h3>
<p>servicecomb-saga是servicecomb中的一部分，比之tcc而言少了一个try操作，更加轻量化，具体可参考：<a href="https://github.com/apache/servicecomb-saga-actuator">点击此处跳转官网地址</a></p>
<h3 id="seata">Seata</h3>
<p>阿里开源的分布式事务解决方案，致力于提供高性能和简单易用，包括AT，TCC，SAGA以及XA事务模式。具体可参考：<a href="http://seata.io/zh-cn/">点击此处跳转官网地址</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bash条件判断详解]]></title>
        <id>https://philosopherzb.github.io/post/bash-tiao-jian-pan-duan-xiang-jie/</id>
        <link href="https://philosopherzb.github.io/post/bash-tiao-jian-pan-duan-xiang-jie/">
        </link>
        <updated>2022-05-13T09:20:29.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述bash条件判断规则语法。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/rocks-1757593_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="条件判断">条件判断</h2>
<h3 id="if结构">if结构</h3>
<p>if是最常用的条件判断结构，只有符合给定条件时，才会执行指定的命令。它的语法如下。</p>
<p>注意：如果写在同一行，关键字之间需要使用分号隔离。分号是 Bash 的命令分隔符</p>
<pre><code>if commands; then
  commands
[elif commands; then
  commands...]
[else
  commands]
fi

# 例子 判断环境变量$USER是否等于root
if test $USER = &quot;root&quot;; then
  echo &quot;Hello root.&quot;
else
  echo &quot;You are not root.&quot;
fi

# 单行书写例子
if true; then echo 'hello world'; fi

</code></pre>
<p>if后面可以跟任意数量的命令。这时，所有命令都会执行，但是判断真伪只看最后一个命令，即使前面所有命令都失败，只要最后一个命令返回0，就会执行then的部分。</p>
<pre><code>if false; i=3; true; then echo 'hello world'; fi
echo ${i}

</code></pre>
<h3 id="test命令">test命令</h3>
<p>if结构的判断条件，一般使用test命令，有三种形式。</p>
<pre><code># 写法一
test expression
# 写法二
[ expression ]
# 写法三
[[ expression ]]

</code></pre>
<p>三种形式是等价的，但是第三种形式还支持正则判断，前两种不支持。</p>
<p>expression是一个表达式。这个表达式为真，test命令执行成功（返回值为0）；表达式为伪，test命令执行失败（返回值为1）。</p>
<p>注意：第二种和第三种写法，[ ]与内部的表达式之间必须有空格。</p>
<pre><code>read i
# 写法一
if test ${i} == 1 ; then
  echo &quot;input param: ${i}&quot;
fi
# 写法二
if [ ${i} == 1 ] ; then
   echo &quot;input param: ${i}&quot;
fi
# 写法三
if [[ ${i} == 1 ]] ; then
   echo &quot;input param: ${i}&quot;
fi

</code></pre>
<h2 id="判断表达式">判断表达式</h2>
<p>[[ ]]使用在条件判断中，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt; 和 &gt; 操作符能够正常存在于 [[ ]] 条件判断结构中，但是如果出现在 [ ] 结构中的话，会报错。</p>
<p>执行的时候，需要用bash test.sh；因为[[]]是bash脚本中的命令（bash是sh的增强版本）。</p>
<p>注意：[[  ]]中操作符与变量之间需要有空格，否则会被当做一个变量处理。</p>
<h3 id="文件判断">文件判断</h3>
<ul>
<li>[ -a file ]：如果 file 存在，则为true。</li>
<li>[ -b file ]：如果 file 存在并且是一个块（设备）文件，则为true。</li>
<li>[ -c file ]：如果 file 存在并且是一个字符（设备）文件，则为true。</li>
<li>[ -d file ]：如果 file 存在并且是一个目录，则为true。</li>
<li>[ -e file ]：如果 file 存在，则为true。</li>
<li>[ -f file ]：如果 file 存在并且是一个普通文件，则为true。</li>
<li>[ -g file ]：如果 file 存在并且设置了组 ID，则为true。</li>
<li>[ -G file ]：如果 file 存在并且属于有效的组 ID，则为true。</li>
<li>[ -h file ]：如果 file 存在并且是符号链接，则为true。</li>
<li>[ -k file ]：如果 file 存在并且设置了它的“sticky bit”，则为true。</li>
<li>[ -L file ]：如果 file 存在并且是一个符号链接，则为true。</li>
<li>[ -N file ]：如果 file 存在并且自上次读取后已被修改，则为true。</li>
<li>[ -O file ]：如果 file 存在并且属于有效的用户 ID，则为true。</li>
<li>[ -p file ]：如果 file 存在并且是一个命名管道，则为true。</li>
<li>[ -r file ]：如果 file 存在并且可读（当前用户有可读权限），则为true。</li>
<li>[ -s file ]：如果 file 存在且其长度大于零，则为true。</li>
<li>[ -S file ]：如果 file 存在且是一个网络 socket，则为true。</li>
<li>[ -t fd ]：如果 fd 是一个文件描述符，并且重定向到终端，则为true。 这可以用来判断是否重定向了标准输入／输出错误。</li>
<li>[ -u file ]：如果 file 存在并且设置了 setuid 位，则为true。</li>
<li>[ -w file ]：如果 file 存在并且可写（当前用户拥有可写权限），则为true。</li>
<li>[ -x file ]：如果 file 存在并且可执行（有效用户有执行／搜索权限），则为true。</li>
<li>[ file1 -nt file2 ]：如果 FILE1 比 FILE2 的更新时间最近，或者 FILE1 存在而 FILE2 不存在，则为true。</li>
<li>[ file1 -ot file2 ]：如果 FILE1 比 FILE2 的更新时间更旧，或者 FILE2 存在而 FILE1 不存在，则为true。</li>
<li>[ FILE1 -ef FILE2 ]：如果 FILE1 和 FILE2 引用相同的设备和 inode 编号，则为true。</li>
</ul>
<pre><code>file=/opt/shellScriptDir/test1.sh
if [[ -e ${file} &amp;&amp; -a ${file} ]]; then
    echo &quot;${file} exist&quot;
    if [[ -f ${file} ]]; then
        echo &quot;${file} is normal file&quot;
    fi
    if [[ -d ${file} ]]; then
        echo &quot;${file} is directory&quot;
    fi
    if [[ -r ${file} ]]; then
        echo &quot;${file} is readable&quot;
    fi
    if [[ -w ${file} ]]; then
        echo &quot;${file} is writable&quot;
    fi
    if [[ -x ${file} ]]; then
        echo &quot;${file} is executable/searchable&quot;
    fi
else
    echo &quot;${file} not exist&quot;
fi

</code></pre>
<p>注意：上述判断中，如果使用的是单个中括号[]时，$file需要用双引号括起来，否则判断将会失误。因为当$file为空时，-e会判断为真，如果放在双引号中，返回的是空字符串，[ -e &quot;&quot; ]会判断为伪。</p>
<pre><code># 下面的例子会输出 not exist
file=
if [ -e &quot;${file}&quot; ]; then
    echo &quot;${file} exist&quot;
else
    echo &quot;${file} not exist&quot;
fi

# 下面的例子会输出 exist
file=
if [ -e ${file} ]; then
    echo &quot;${file} exist&quot;
else
    echo &quot;${file} not exist&quot;
fi

</code></pre>
<h3 id="字符串判断">字符串判断</h3>
<ul>
<li>[ string ]：如果string不为空（长度大于0），则判断为真。</li>
<li>[ -n string ]：如果字符串string的长度大于零，则判断为真。</li>
<li>[ -z string ]：如果字符串string的长度为零，则判断为真。</li>
<li>[ string1 = string2 ]：如果string1和string2相同，则判断为真。</li>
<li>[ string1 == string2 ] 等同于[ string1 = string2 ]。</li>
<li>[ string1 != string2 ]：如果string1和string2不相同，则判断为真。</li>
<li>[ string1 '&gt;' string2 ]：如果按照字典顺序string1排列在string2之后，则判断为真。</li>
<li>[ string1 '&lt;' string2 ]：如果按照字典顺序string1排列在string2之前，则判断为真。</li>
</ul>
<p>注意，test命令内部的&gt;和&lt;，必须用引号引起来（或者是用反斜杠转义，或者使用双中括号）。否则，它们会被 shell 解释为重定向操作符。</p>
<p>字符串判断时，变量要放在双引号之中，比如[ -n &quot;$COUNT&quot; ]，否则变量替换成字符串以后，test命令可能会报错，提示参数过多。另外，如果不放在双引号之中，变量为空时，命令会变成[ -n ]，这时会判断为真。如果放在双引号之中，[ -n &quot;&quot; ]就判断为伪。</p>
<p>如果不想使用双引号，也可以使用双括号。</p>
<pre><code>str=fwfw
if [[ -z ${str} ]]; then
    echo &quot;${str} length =0&quot;
elif [[ -n ${str} ]]; then
    echo &quot;${str} length &gt;0&quot;
    if [[ ${str} = &quot;fwfw&quot; ]]; then
        echo &quot;${str} exist&quot;
    fi
fi

</code></pre>
<h3 id="整数判断">整数判断</h3>
<ul>
<li>[ integer1 -eq integer2 ]：如果integer1等于integer2，则为true。</li>
<li>[ integer1 -ne integer2 ]：如果integer1不等于integer2，则为true。</li>
<li>[ integer1 -le integer2 ]：如果integer1小于或等于integer2，则为true。</li>
<li>[ integer1 -lt integer2 ]：如果integer1小于integer2，则为true。</li>
<li>[ integer1 -ge integer2 ]：如果integer1大于或等于integer2，则为true。</li>
<li>[ integer1 -gt integer2 ]：如果integer1大于integer2，则为true。</li>
</ul>
<pre><code>a=10
b=20
if [ ${a} -lt ${b} ]
then
    echo &quot;${a} &lt; ${b}&quot;
else
    echo &quot;${a} &gt;= ${b}&quot;
fi

# 使用双中括号，效果一致
if [[ ${a} &lt; ${b} ]]
then
    echo &quot;${a} &lt; ${b}&quot;
else
    echo &quot;${a} &gt;= ${b}&quot;
fi

</code></pre>
<h3 id="正则判断">正则判断</h3>
<p>[[ expression ]]这种判断形式，支持正则表达式。</p>
<pre><code># regex是一个正则表示式，=~是正则比较运算符。
[[ string =~ regex ]]
</code></pre>
<pre><code>read input
if [[ ${input} =~ [0-9] ]]; then
    echo &quot;match num value = ${input}&quot;
fi

</code></pre>
<h3 id="逻辑运算">逻辑运算</h3>
<ul>
<li>AND运算：符号&amp;&amp;，也可使用参数-a。</li>
<li>OR运算：符号||，也可使用参数-o。</li>
<li>NOT运算：符号!。</li>
</ul>
<pre><code>## 使用双中括号，可以直接在命令内容拼接逻辑运算符
read input
if [[ ${input} =~ [0-9] &amp;&amp; ${input} == 3 ]]; then
    echo &quot;match num value = ${input}&quot;
fi

</code></pre>
<p>&amp;&amp; 和 || 也被称作命令控制操作符，可以用来聚合多个逻辑运算命令。</p>
<pre><code>file=/opt/shellScriptDir/temp
[[ -d  ${file} ]] || echo &quot;${file} not exist&quot;
</code></pre>
<h3 id="算术判断">算术判断</h3>
<p>bash提供了(( ... ))作为算术条件，用于进行算术运算的判断。</p>
<p>注意，算术判断不需要使用test命令，而是直接使用((...))结构。这个结构的返回值，决定了判断的真伪。</p>
<p>如果算术计算的结果是非零值，则表示判断成立。这一点跟命令的返回值正好相反，需要小心。</p>
<pre><code># 输出match num value true
if [[ 0 ]]; then
    echo &quot;match num value true&quot;
else
    echo &quot;value false&quot;
fi

# 输出value false
if (( 0 )); then
    echo &quot;match num value true&quot;
else
    echo &quot;value false&quot;
fi

</code></pre>
<p>(( ... ))可以用作变量赋值，赋值完成后将会返回变量的值。</p>
<pre><code># 输出match num value: 1
if (( var=1 )); then
    echo &quot;match num value: ${var}&quot;
else
    echo &quot;value false&quot;
fi

</code></pre>
<h3 id="case结构判断">case结构判断</h3>
<p>case结构用于多值判断，可以为每个值指定对应的命令，跟包含多个elif的if结构等价，但是语义更好。</p>
<pre><code># 语法格式
# expression是一个表达式，pattern是表达式的值或者一个模式，可以有多条，
# 用来匹配多个值，每条以两个分号（;）结尾。
case expression in
  pattern )
    commands ;;
  pattern )
    commands ;;
  ...
esac

</code></pre>
<pre><code># 简单实例
echo &quot;input value[1-3]: &quot;
read input
case ${input} in
    1) echo &quot;read value is 1&quot;;;
    2) echo &quot;read value is 2&quot;;;
    3) echo &quot;read value is 3&quot;;;
    *) echo &quot;read value is not match: ${input}&quot;;;
esac

</code></pre>
<p>case的匹配模式可以使用各种通配符，类似于下方所示：</p>
<ul>
<li>a)：匹配a。</li>
<li>a|b)：匹配a或b。</li>
<li>[[:alpha:]])：匹配单个字母。</li>
<li>???)：匹配3个字符的单词。</li>
<li>*.txt)：匹配.txt结尾。</li>
<li>*)：匹配任意输入，通过作为case结构的最后一个模式。</li>
</ul>
<pre><code># 匹配数值，单个字符，两个字符
echo &quot;input value[number or character]: &quot;
read input
case ${input} in
    [0-9]) echo &quot;read number value is: ${input}&quot;;;
    [[:lower:]] | [[:upper:]]) echo &quot;read character value is: ${input}&quot;;;
    ??) echo &quot;read double input value is: ${input}&quot;;;
    *) echo &quot;read value is not match: ${input}&quot;;;
esac

</code></pre>
<p>Bash 4.0之前，case结构只能匹配一个条件，然后就会退出case结构。Bash 4.0之后，允许匹配多个条件，这时可以用;;&amp;终止每个条件块。</p>
<pre><code>echo &quot;input character value: &quot;
read input
case ${input} in
  [[:upper:]])    echo &quot;'${input}' is upper case.&quot; ;;&amp;
  [[:lower:]])    echo &quot;'${input}' is lower case.&quot; ;;&amp;
  [[:alpha:]])    echo &quot;'${input}' is alphabetic.&quot; ;;&amp;
  [[:digit:]])    echo &quot;'${input}' is a digit.&quot; ;;&amp;
  [[:graph:]])    echo &quot;'${input}' is a visible character.&quot; ;;&amp;
  [[:punct:]])    echo &quot;'${input}' is a punctuation symbol.&quot; ;;&amp;
  [[:space:]])    echo &quot;'${input}' is a whitespace character.&quot; ;;&amp;
  [[:xdigit:]])   echo &quot;'${input}' is a hexadecimal digit.&quot; ;;&amp;
esac

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shell基本知识]]></title>
        <id>https://philosopherzb.github.io/post/shell-ji-ben-zhi-shi/</id>
        <link href="https://philosopherzb.github.io/post/shell-ji-ben-zhi-shi/">
        </link>
        <updated>2022-04-30T09:03:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>本章主要简述shell基本知识。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/nature-2147400_1280.jpg" alt="" loading="lazy"></figure>
<h2 id="简介">简介</h2>
<h3 id="简介-2">简介</h3>
<p>Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。</p>
<p>Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。</p>
<p>Shell 脚本（shell script），是一种为 shell 编写的脚本程序。业界所说的 shell 通常都是指 shell 脚本（故此处也沿用该说明），需要注意的是：shell 和 shell script 是两个不同的概念。</p>
<p>查看安装的shell信息，两种查看方式任选一种键入回车即可得到相关信息。</p>
<pre><code>ls -l /bin/*sh
cat /etc/shells

</code></pre>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/philosopherZB/blog-images/master/blogImg/clipboard20230307001.png" alt="img" loading="lazy"></figure>
<h2 id="基本语法bash">基本语法（Bash）</h2>
<h3 id="变量">变量</h3>
<h4 id="基础变量">基础变量</h4>
<p>创建变量时，需遵循如下规则：</p>
<ul>
<li>命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。</li>
<li>不允许出现空格及标点符号。</li>
<li>不能使用bash里的关键字（可用help命令查看保留关键字）。</li>
</ul>
<p>变量声明的语法如下（注意：等号两边不能存在空格，读取时使用$符）：</p>
<pre><code>variable=vlaue
</code></pre>
<pre><code># 简单例子
# 变量 a 赋值为字符串 hello
a=hello
# $a 等效于 ${a}，加花括号是为了帮助解释器识别变量的边界
# 如：echo $a_world 将不会输出内容，因为变量a_world不存在
# 但是可以使用echo ${a}_world，将会输出hello_world
echo ${a}

# 变量值包含空格，就必须放在引号里面        
b=&quot;world shell&quot;
echo ${b}

# 变量值可以引用其他变量的值
c=&quot;${a} ${b} !&quot;
echo ${c}

# 变量值可以使用转义字符
d=&quot;\t content \n&quot;
echo ${d}

# 变量值可以是命令的执行结果
e=$(ls -l /bin/*sh)
echo ${e}

# 变量值可以是数学运算的结果
f=$((4 * 5))
echo ${f}

# 变量重复赋值，后面的赋值将会覆盖前面的赋值
foo=1
foo=2
echo ${foo}

# 变量的值是变量，如果需要输出，可以使用${!varname}愈发
myvar=USER
echo ${!myvar}

</code></pre>
<h4 id="特殊变量">特殊变量</h4>
<p>Bash（是shell的一个增强版本）提供了一些特殊变量，特殊变量的值由shell预定义，用户不用进行赋值。</p>
<ul>
<li>$?: 表示上一个命令的退出码，用来判断上一个命令是否执行成功，0表示成功，非0表示失败。</li>
<li>$$: 表示当前shell的进程id，也可以用来命名临时文件</li>
<li>$_: 表示上个命令的最后一个参数</li>
<li>$!: 表示最近一个后台执行的异步命令的进程id</li>
<li>$0: 表示当前shell的名称（在命令直接执行时）或者脚本名（在脚本中执行时）</li>
<li>$-: 表示当前shell的启动参数</li>
<li>$@ $#: 表示脚本中的参数数量（两者不同之处：在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 &quot; * &quot; 等价于 &quot;1 2 3&quot;（传递了一个参数），而 &quot;@&quot; 等价于 &quot;1&quot; &quot;2&quot; &quot;3&quot;（传递了三个参数）。）</li>
</ul>
<pre><code># $? 例子
ls -l /bin/bash
echo $?
ls -l notexistfile
echo $?
# 输出结果
-rwxr-xr-x 1 root root 1183448 Jun 18  2020 /bin/bash
0
ls: cannot access 'notexistfile': No such file or directory
2

</code></pre>
<h4 id="变量默认值">变量默认值</h4>
<p>Bash 提供四个特殊语法，跟变量的默认值有关，目的是保证变量不为空。</p>
<ul>
<li>${varname:-word} 语法含义：如果变量varname存在且不为空，则返回它的值，否则返回word。它的目的是返回一个默认值，比如${count:-0}表示变量count不存在时返回0。</li>
<li>${varname:=word} 语法含义：如果变量varname存在且不为空，则返回它的值，否则将它设为word，并且返回word。它的目的是设置变量的默认值，比如${count:=0}表示变量count不存在时返回0，且将count设为0。</li>
<li>${varname:+word} 语法含义：如果变量名存在且不为空，则返回word，否则返回空值。它的目的是测试变量是否存在，比如${count:+1}表示变量count存在时返回1（表示true），否则返回空值。</li>
<li>${varname:?message} 语法含义：如果变量varname存在且不为空，则返回它的值，否则打印出varname: message，并中断脚本的执行。如果省略了message，则输出默认的信息“parameter null or not set.”。它的目的是防止变量未定义，比如${count:?&quot;undefined!&quot;}表示变量count未定义时就中断执行，抛出错误，返回给定的报错信息undefined!。</li>
</ul>
<pre><code>echo ${a:-0}
echo ${a:=word}
echo ${a:+1}
echo ${b:?&quot;undefined!&quot;}
# 输出结果
0
word
1
./test1.sh: line 5: b: undefined!

</code></pre>
<h4 id="变量命令">变量命令</h4>
<p>declare命令可以声明一些特殊类型的变量，为变量设置一些限制，比如声明只读类型的变量和整数类型的变量。</p>
<p>declare语法格式：declare OPTION VARIABLE=value，命令的主要参数（OPTION）如下：</p>
<ul>
<li>-a：声明数组变量。</li>
<li>-f：输出所有函数定义。</li>
<li>-F：输出所有函数名。</li>
<li>-i：声明整数变量。</li>
<li>-l：声明变量为小写字母。</li>
<li>-p：查看变量信息。</li>
<li>-r：声明只读变量。</li>
<li>-u：声明变量为大写字母。</li>
<li>-x：该变量输出为环境变量。</li>
</ul>
<p>readonly命令等同于declare -r，用来声明只读变量，不能改变变量值，也不能unset变量。</p>
<p>let命令声明变量时，可以直接执行算术表达式。</p>
<h4 id="数组变量">数组变量</h4>
<p>数组中可以存放多个值。Bash Shell 只支持一维数组（不支持多维数组），初始化时不需要定义数组大小。</p>
<p>与大部分编程语言类似，数组元素的下标由 0 开始。</p>
<p>Shell 数组用括号来表示，元素用&quot;空格&quot;符号分割开，语法格式如下：</p>
<pre><code>array_name=(value1 value2 ... valuen)
</code></pre>
<pre><code># 数组例子
array_test=(&quot;hello&quot; &quot;world&quot; &quot;shell&quot;)
echo ${array_test[2]}
# 获取所有元素
echo ${array_test[@]}
echo ${array_test[*]}

</code></pre>
<h4 id="字符串变量进阶例子">字符串变量&lt;进阶例子&gt;</h4>
<pre><code>var=&quot;opt/temp/test.sh&quot;
# 字符串长度
echo ${#var}

# 截取字符串 ${varname:offset:length}: 从位置offset开始（从0开始计算），长度为length
echo ${var:0:3}
# 如果省略length，则从位置offset开始，一直返回到字符串的结尾。
echo ${var:0}
# 如果offset为负值，表示从字符串的末尾开始算起。
# 注意，负数前面必须有一个空格（如果不想写空格，可以填0），以防止与${variable:-word}的变量的设置默认值语法混淆。
# 这时，如果还指定length，则length不能小于零。
echo ${var: -16:3}
echo ${var: -13}
echo ${var:0-16:3}
echo ${var:0-13}

# 输出大写
echo ${var^^}
# 输出小写
echo ${var,,}

# 匹配删除字符串
# 匹配模式pattern可以使用*、?、[]等通配符。

# ${variable#pattern}
# 如果 pattern 匹配变量 variable 的开头，删除最短匹配（非贪婪匹配）的部分，返回剩余部分
# #*/ 表示从左边开始删除第一个 / 号及左边的所有字符，即删除opt/，结果为：temp/test.sh
echo ${var#*/}

# ${variable##pattern}
# 如果 pattern 匹配变量 variable 的开头，删除最长匹配（贪婪匹配）的部分，返回剩余部分
# ##*/ 表示从左边开始删除最后（最右边）一个 / 号及左边的所有字符，即删除opt/temp/，结果为：test.sh
echo ${var##*/}

# ${variable%pattern}
# 如果 pattern 匹配变量 variable 的结尾，删除最短匹配（非贪婪匹配）的部分，返回剩余部分
# %/* 表示从右边开始，删除第一个 / 号及右边的字符，即删除/test.sh，结果：opt/temp
echo ${var%/*}

# ${variable%%pattern}
# 如果 pattern 匹配变量 variable 的结尾，删除最长匹配（贪婪匹配）的部分，返回剩余部分
# %%/* 表示从右边开始，删除最后（最左边）一个 / 号及右边的字符，即删除/temp/test.sh，结果：opt
echo ${var%%/*}

</code></pre>
<h3 id="算术运算符">算术运算符</h3>
<h4 id="算术表达式">算术表达式</h4>
<p>((...))语法可以进行整数的算术运算。该表达式可以忽略内部的空格，在其内部可以使用()改变运算顺序。输出结果时，需要在前面加上$符。</p>
<pre><code>a=2
echo $(( ${a} + 2 ))
# 可以不使用$或者${}（花括号是为了确定边界）引用变量
echo $(( + 2))

# 赋值
i=$((a + 2))
echo &quot;i= ${i}&quot;

</code></pre>
<p>expr命令同样支持算是运算，其可以使用变量替换。</p>
<p>注意：表达式与运算符之间需要有空格，否则会被当做字符串输出。</p>
<pre><code>a=2
expr ${a} + 2

# 赋值，两种方式都可进行赋值，``符号位于esc键下方，并非单引号
b=$(expr ${a} + 2)
c=`expr ${a} + 2`
echo &quot;b= ${b}&quot;
echo &quot;c= ${c}&quot;

</code></pre>
<p>((...))语法支持的算术运算符如下。</p>
<ul>
<li>+：加法</li>
<li>-：减法</li>
<li>*：乘法</li>
<li>/：除法（整除）</li>
<li>%：余数</li>
<li>**：指数</li>
<li>++：自增运算（作为前缀是先运算后返回值，作为后缀是先返回值后运算）</li>
<li>--：自减运算（作为前缀是先运算后返回值，作为后缀是先返回值后运算）</li>
</ul>
<h4 id="进制数值">进制数值</h4>
<p>Bash 的数值默认都是十进制，但是在算术表达式中，也可以使用其他进制。</p>
<ul>
<li>number：没有任何特殊表示法的数字是十进制数（以10为底）。</li>
<li>0number：八进制数。</li>
<li>0xnumber：十六进制数。</li>
<li>base#number：base进制的数。</li>
</ul>
<pre><code>echo $((016))
echo $((0xee))
echo $((2#00000011))

</code></pre>
<h4 id="位运算">位运算</h4>
<p>Bash 支持二进制位运算符</p>
<ul>
<li>&lt;&lt;：位左移运算，把一个数字的所有位向左移动指定的位。</li>
<li>&gt;&gt;：位右移运算，把一个数字的所有位向右移动指定的位。</li>
<li>&amp;：位的“与”运算，对两个数字的所有位执行一个AND操作。</li>
<li>|：位的“或”运算，对两个数字的所有位执行一个OR操作。</li>
<li>~：位的“否”运算，对一个数字的所有位取反。</li>
<li>!：逻辑“否”运算</li>
<li>^：位的异或运算（exclusive or），对两个数字的所有位执行一个异或操作。</li>
</ul>
<h4 id="逻辑运算">逻辑运算</h4>
<p>Bash 支持逻辑运算符</p>
<ul>
<li>&lt;/-lt：小于</li>
<li>&gt;/-gt：大于</li>
<li>&lt;=/-le：小于或相等</li>
<li>&gt;=/ge：大于或相等</li>
<li>==/-eq：相等</li>
<li>!=/-ne：不相等</li>
<li>&amp;&amp;：逻辑与</li>
<li>||：逻辑或</li>
<li>expr1?expr2:expr3：三元条件运算符。若表达式expr1的计算结果为非零值（算术真），则执行表达式expr2，否则执行表达式expr3。</li>
</ul>
<h4 id="赋值与求值运算">赋值与求值运算</h4>
<p>逗号,在$((...))内部是求值运算符，执行前后两个表达式，并返回后一个表达式的值。</p>
<pre><code>echo $((foo = 1 + 2, 3 * 4))
echo ${foo}
# 输出
12
3

</code></pre>
<p>赋值运算如下：</p>
<ul>
<li>parameter = value：简单赋值。</li>
<li>parameter += value：等价于parameter = parameter + value。</li>
<li>parameter -= value：等价于parameter = parameter – value。</li>
<li>parameter *= value：等价于parameter = parameter * value。</li>
<li>parameter /= value：等价于parameter = parameter / value。</li>
<li>parameter %= value：等价于parameter = parameter % value。</li>
<li>parameter &lt;&lt;= value：等价于parameter = parameter &lt;&lt; value。</li>
<li>parameter &gt;&gt;= value：等价于parameter = parameter &gt;&gt; value。</li>
<li>parameter &amp;= value：等价于parameter = parameter &amp; value。</li>
<li>parameter |= value：等价于parameter = parameter | value。</li>
<li>parameter ^= value：等价于parameter = parameter ^ value。</li>
</ul>
<pre><code>echo $((foo = 3))
echo $((foo *= 2))
# 输出
3
6

</code></pre>
<h2 id="流程控制">流程控制</h2>
<h3 id="条件判断">条件判断</h3>
<p>1、简单if语句</p>
<pre><code># 简单if语句
if condition
then
    command
fi

</code></pre>
<p>2、if else语句</p>
<pre><code># if else语句
if condition
then
    command
else
    command
fi

</code></pre>
<p>3、if else-if else语句</p>
<pre><code># if else-if else语句
if condition1
then
    command1
elif condition2 
then 
    command2
else
    commandN
fi

</code></pre>
<pre><code># 例子
a=10
b=20
# [[ ]]使用在条件判断中，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt; 和 &gt; 操作符能够正常存在于 [[ ]] 条件判断结构中，但是如果出现在 [ ] 结构中的话，会报错。
# 执行的时候，需要用bash test.sh；因为[[]]是bash脚本中的命令（bash是sh的增强版本）。
if [[ ${a} &lt; ${b} ]]
then
    echo &quot;a &lt; b&quot;
else
    echo &quot;a &gt;= b&quot;
fi

</code></pre>
<h3 id="循环语句">循环语句</h3>
<p>Bash 提供三种循环语法for、while和until。</p>
<p>while循环有一个判断条件，只要符合条件，就不断循环执行指定的语句。关键字do可以跟while不在同一行，这时两者之间不需要使用分号分隔。</p>
<pre><code>while condition; do
  commands
done

</code></pre>
<p>until循环与while循环恰好相反，只要不符合判断条件（判断条件失败），就不断循环执行指定的语句。一旦符合判断条件，就退出循环。关键字do可以与until不写在同一行，这时两者之间不需要分号分隔。</p>
<pre><code>until condition; do
  commands
done

</code></pre>
<p>for...in循环用于遍历列表中的每一项。关键词do可以跟for写在同一行，两者使用分号分隔。</p>
<pre><code>for variable in list
do
  commands
done

</code></pre>
<p>for循环支持C语言的循环语法。</p>
<pre><code># expression1用来初始化循环条件，expression2用来决定循环结束的条件，
# expression3在每次循环迭代的末尾执行，用于更新值。
# 注意，循环条件放在双重圆括号之中。另外，圆括号之中使用变量，不必加上美元符号$。
for (( expression1; expression2; expression3 )); do
  commands
done

# 等同于下述表达式
(( expression1 ))
while (( expression2 )); do
  commands
  (( expression3 ))
done

# 例子
for (( i=0; i&lt;5; i=i+1 )); do
  echo $i
done

</code></pre>
<p>Bash 提供了两个内部命令break和continue，用来在循环内部跳出循环。</p>
<p>break命令立即终止循环，程序继续执行循环块之后的语句，即不再执行剩下的循环。</p>
<p>continue命令立即终止本轮循环，开始执行下一轮循环。</p>
<pre><code># break
a=(first second third)
for i in ${a[*]}; do
    echo ${i}
    if [[ ${i} == &quot;first&quot; ]]; then break; fi
done

# continue
a=(first second third)
for i in ${a[*]}; do
    if [[ ${i} == &quot;second&quot; ]]; then continue; fi
    echo ${i}
done

</code></pre>
<h2 id="函数">函数</h2>
<p>函数（function）是可以重复使用的代码片段，有利于代码的复用。它与别名（alias）的区别是，别名只适合封装简单的单个命令，函数则可以封装复杂的多行命令。</p>
<p>函数总是在当前 Shell 执行，这是跟脚本的一个重大区别，Bash 会新建一个子 Shell 执行脚本。如果函数与脚本同名，函数会优先执行。但是，函数的优先级不如别名，即如果函数与别名同名，那么别名优先执行。</p>
<pre><code># 第一种
fn() {
  # codes
}
# 第二种
function fn() {
  # codes
}

</code></pre>
<p>$1~$9：函数的第一个到第9个的参数。如果函数的参数多于9个，那么第10个参数可以用${10}的形式引用，以此类推</p>
<pre><code>printParam(){
    echo &quot;first param is $1&quot;
}
printParam wqrwq

function log_msg(){
    echo &quot;[$(date '+%F %T')]: $@&quot;
}
log_msg this is sample log message

</code></pre>
<p>函数体内支持局部变量的声明。同时，也支持修改全局变量。</p>
<pre><code>function fn(){
    local str=paramValue
    echo &quot;local: str=${str}&quot;
}
fn
echo &quot;global: str=${str}&quot;

</code></pre>
]]></content>
    </entry>
</feed>